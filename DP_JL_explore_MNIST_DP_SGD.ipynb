{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z6KT53o6TLb",
        "outputId": "d7ae5a7d-e0a2-459e-c1fb-6a67ac525926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299824\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.771508\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.950519\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.658959\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.303416\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.323190\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.152533\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.531138\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.269302\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.146324\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.285488\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.317067\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.419974\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.161970\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.217496\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.103642\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.184252\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.365709\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.214398\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.130990\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.233532\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.099975\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.150190\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.110024\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.404166\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.354492\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.148039\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.143722\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.255179\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.246712\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.205331\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.193911\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.171557\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.051330\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.336340\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.116467\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.148265\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.189479\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.045207\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.113705\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.133244\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.132966\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.143654\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.009309\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.104657\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.016252\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.047749\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.259936\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.191085\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.186835\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.176420\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.100352\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.112744\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.078696\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.088314\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.119024\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.140753\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.118348\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.122668\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.058561\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.041925\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.085722\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.371308\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.132388\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.173287\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.153335\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.163121\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.033593\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.072021\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.116581\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.058807\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.315160\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.044505\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.045954\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.141491\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.096609\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.370217\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.069809\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.093105\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.064116\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.130187\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.215357\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.066343\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.047455\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.025290\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.089707\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.196634\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.042813\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.136623\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.120246\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.067335\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.067731\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.049679\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.026997\n",
            "\n",
            "Test set: Average loss: 0.0489, Accuracy: 9837/10000 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.093328\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.041831\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.015188\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.103468\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.042753\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.056078\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.119283\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.145885\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.141373\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.117543\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.043897\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.038269\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.042705\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.043463\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.026021\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.092006\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.049314\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.023284\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.147822\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.060211\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.040804\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.019090\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.244936\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.023324\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.136013\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.008762\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.160578\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.025343\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.051981\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.117537\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.025738\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.047277\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.082217\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.059822\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.020508\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.026630\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.074736\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.076880\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.004120\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.196599\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.071679\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.062316\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.061114\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.072515\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.022375\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.217066\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.065245\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.196444\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.064163\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.015564\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.021928\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.055149\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.032220\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.026473\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.098551\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.024963\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.005268\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.047827\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.128633\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.077053\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.019479\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.084197\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.017439\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.155927\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.035029\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.056228\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.099668\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.049870\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.024138\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.030798\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.094216\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.015737\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.032993\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.040016\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.131229\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.103594\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.151487\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.062317\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.028290\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.053275\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.030629\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.042729\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.077638\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.201674\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.148093\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.056582\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.017166\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.125853\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.008756\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.136691\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.030005\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.043009\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.111838\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.165391\n",
            "\n",
            "Test set: Average loss: 0.0370, Accuracy: 9880/10000 (99%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.061410\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.037502\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.030190\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.087780\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.011019\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.004972\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.003197\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.053976\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.020493\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.011813\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.014481\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.009970\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.055052\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.014635\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.019988\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.123800\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.056213\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.116148\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.066850\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.019367\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.006108\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.008412\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.054187\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.023015\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.021601\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.389882\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.027398\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.036513\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.060951\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.041683\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.068388\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.227020\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.005607\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.027452\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.082419\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.152400\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.260136\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.039923\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.014303\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.157019\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.015272\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.017193\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.029601\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.082170\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.043962\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.187479\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.033753\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.109839\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.063547\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.071165\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.137279\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.011084\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.045015\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.031659\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.074368\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.026750\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.206503\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.046958\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.198999\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.229084\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.041588\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.056919\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.022344\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.024906\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.020860\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.107496\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.124274\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.111087\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.002201\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.045779\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.025853\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.027233\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.111643\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.077050\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.047092\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.076140\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.097972\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.049479\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.084513\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.021276\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.095969\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.016091\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.044366\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.231536\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.124802\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.059888\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.058509\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.015643\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.195827\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.007614\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.008024\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.242689\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.048908\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.041230\n",
            "\n",
            "Test set: Average loss: 0.0356, Accuracy: 9873/10000 (99%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.004878\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.032827\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.150881\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.016804\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.088445\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.101822\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.019019\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.013099\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.027931\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.009487\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.010813\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.062867\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.002137\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.006469\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.072587\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.023950\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.054229\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.013036\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.011480\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.088731\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.120214\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.093574\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.065168\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.062563\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.007835\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.031270\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.002208\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.058721\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.049255\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.060984\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.006218\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.020661\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.066817\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.071313\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.008144\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.019996\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.014592\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.030175\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.038277\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.009225\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.053612\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.007437\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.008427\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.135875\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.105946\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.005662\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.031134\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.137424\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.185636\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.005200\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.002099\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.022964\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.093393\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.005129\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.030204\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.116831\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.013889\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.016229\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.002933\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.006229\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.103400\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.052023\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.003458\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.004271\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.086735\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.004500\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.009828\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.163595\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.044369\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.011204\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.002872\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.009339\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.133931\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.019900\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.126859\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.032269\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.005843\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.008080\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.071701\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.002606\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.006489\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.013792\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.057656\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.023694\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.059563\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.030972\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.001430\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.008252\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.011749\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.003117\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.008432\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.035789\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.041257\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.104705\n",
            "\n",
            "Test set: Average loss: 0.0342, Accuracy: 9882/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.015751\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.040919\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.015944\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.013169\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.000862\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.031247\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.086986\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.062024\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.038525\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.049257\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.002374\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.007333\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.040146\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.018576\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.051790\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.016392\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.003062\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.027045\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.065574\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.007221\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.004160\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.094796\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.009315\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.015290\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.059272\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.128245\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.052728\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.079321\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.044406\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.042476\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.005109\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.008629\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.013417\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.016447\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.064094\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.054114\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.076469\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.066209\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.009061\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.002500\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.010895\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.033848\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.058742\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.096335\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.042689\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.013319\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.016454\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.042307\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.006670\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.042524\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.192164\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.007229\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.011469\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.042365\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.086614\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.007304\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.011614\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.036223\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.000638\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.047062\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.014795\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.001253\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.016481\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.016946\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.025323\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.011967\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.009195\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.049835\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.037910\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.004274\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.005617\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.003910\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.009286\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.015188\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.005175\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.026757\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.017803\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.065738\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.031180\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.018524\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.005734\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.057071\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.026041\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.021389\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.052646\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.021458\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.006173\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.012512\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.012667\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.041936\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.002760\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.040913\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.021455\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.000980\n",
            "\n",
            "Test set: Average loss: 0.0307, Accuracy: 9904/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.027905\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.032042\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.223514\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.260013\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.013093\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.024509\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.055027\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.050339\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.003644\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.008090\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.009228\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.005165\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.013663\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.021617\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.032086\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.032508\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.003627\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.004815\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.010100\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.009498\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.086326\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.034415\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.012842\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.035328\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.014208\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.022223\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.009469\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.008222\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.067955\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.015370\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.023858\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.064207\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.060729\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.035654\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.029097\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.001057\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.027329\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.013817\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.005363\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.029194\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.002975\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.067852\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.029315\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.015128\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.005314\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.007210\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.064719\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.225017\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.049012\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.051635\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.035700\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.038035\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.039248\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.021887\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.009315\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.056114\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.003996\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.011946\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.018312\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.000570\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.003022\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.028332\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.027855\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.001711\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.033655\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.108127\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.052918\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.137224\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.015928\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.117080\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.000881\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.068825\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.108929\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.019167\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.074264\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.035034\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.033727\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.077127\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.104651\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.095705\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.021976\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.004233\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.019366\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.082490\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.033125\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.020918\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.003585\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.017000\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.027176\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.002075\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.101413\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.006083\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.009656\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.085687\n",
            "\n",
            "Test set: Average loss: 0.0294, Accuracy: 9911/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.008522\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.254508\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.003332\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.004885\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.091439\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.092857\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.017132\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.007904\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.029863\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.017291\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.001450\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.009823\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.039857\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.020385\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.001735\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.000507\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.044641\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.004270\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.016116\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.064244\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.013896\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.005751\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.073074\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.015709\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.000483\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.003949\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.007118\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.003544\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.175071\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.017331\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.061304\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.016681\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.007326\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.003093\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.012765\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.005871\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.089234\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.012475\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.015052\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.106075\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.078030\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.023520\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.005522\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.001154\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.016471\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.014314\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.002529\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.091571\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.007063\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.024253\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.012635\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.040152\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.005699\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.004329\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.002535\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.001089\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.017451\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.007111\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.001656\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.081873\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.032369\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.017084\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.065719\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.006917\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.006794\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.008394\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.005782\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.015032\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.061194\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.002885\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.011363\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.043126\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.011461\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.007086\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.010503\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.013138\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.097184\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.185028\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.029550\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.004087\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.034499\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.021071\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.079829\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.002517\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.002928\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.002736\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.018481\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.020591\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.016350\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.005958\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.014041\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.023399\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.157782\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.011122\n",
            "\n",
            "Test set: Average loss: 0.0301, Accuracy: 9911/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.009096\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.002912\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.003937\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.006612\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.020101\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.001036\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.011981\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.008888\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.014578\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.095635\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.002530\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.012289\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.030025\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.019117\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.049659\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.038916\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.032234\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.032260\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.030045\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.007094\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.028523\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.006642\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.004293\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.019227\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.014066\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.007839\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.017718\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.026152\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.009255\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.000743\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.009132\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.004090\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.005660\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.003785\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.001760\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.005353\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.012056\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.009226\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.228435\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.063301\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.005085\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.023508\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.086783\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.107644\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.042986\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.003176\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.052886\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.115244\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.021628\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.004267\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.000970\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.005535\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.043060\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.073042\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.001374\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.005989\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.018784\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.003951\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.034454\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.005912\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.001000\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.031037\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.263720\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.029062\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.188388\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.010688\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.491458\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.008386\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.001277\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.078989\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.020508\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.031664\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.036140\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.001608\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.039690\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.016035\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.059667\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.021784\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.001580\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.007574\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.063618\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.075354\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.000647\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.015265\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.154649\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.004525\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.001512\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.001168\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.004145\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.013249\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.002900\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.028085\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.002804\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.014107\n",
            "\n",
            "Test set: Average loss: 0.0293, Accuracy: 9910/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.018972\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.007817\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.004686\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.074777\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.004376\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.002234\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.006013\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.002490\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.002689\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.032626\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.019637\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.001330\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.033627\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.007749\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.015969\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.017369\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.074550\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.004460\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.004420\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.025817\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.005608\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.065698\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.110926\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.003680\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.006063\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.020424\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.010683\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.040859\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.012465\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.142493\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.067642\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.071751\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.022158\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.016163\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.005691\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.010817\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.022082\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.003204\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.077568\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.065523\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.010711\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.033403\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.002551\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.046278\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.007575\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.016232\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.010300\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.057086\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.012739\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.003766\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.011101\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.003129\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.004148\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.044902\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.012602\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.015478\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.054142\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.006333\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.046736\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.044566\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.043864\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.019366\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.000982\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.065701\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.002524\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.004202\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.051693\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.012768\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.003995\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.009322\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.006811\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.032048\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.059639\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.001979\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.003580\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.006995\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.017192\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.004670\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.251752\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.144755\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.022877\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.001389\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.006499\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.120395\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.009415\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.039009\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.007886\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.004097\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.017758\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.002601\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.091670\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.003673\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.009484\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.033118\n",
            "\n",
            "Test set: Average loss: 0.0285, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.042162\n",
            "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.001931\n",
            "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.028695\n",
            "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.005181\n",
            "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.002794\n",
            "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.025444\n",
            "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.008588\n",
            "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.002610\n",
            "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.001292\n",
            "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.028489\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.013414\n",
            "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.018880\n",
            "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.136800\n",
            "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.065635\n",
            "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.004559\n",
            "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.066854\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.014270\n",
            "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.010569\n",
            "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.001496\n",
            "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.049587\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.007287\n",
            "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.026233\n",
            "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.001162\n",
            "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.002311\n",
            "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.009400\n",
            "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.001996\n",
            "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.061628\n",
            "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.001827\n",
            "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.033532\n",
            "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.041661\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.007027\n",
            "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.040934\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.071538\n",
            "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.004396\n",
            "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.006162\n",
            "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.079675\n",
            "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.015055\n",
            "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.029202\n",
            "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.006731\n",
            "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.045701\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.004533\n",
            "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.006632\n",
            "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.031239\n",
            "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.005209\n",
            "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.006847\n",
            "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.028672\n",
            "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.020220\n",
            "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.071303\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.085414\n",
            "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.016430\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.002634\n",
            "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.001935\n",
            "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.004951\n",
            "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.001021\n",
            "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.013470\n",
            "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.007720\n",
            "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.084931\n",
            "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.166987\n",
            "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.021331\n",
            "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.001360\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.002418\n",
            "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.016606\n",
            "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.015494\n",
            "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.039203\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.001462\n",
            "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.002227\n",
            "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.027005\n",
            "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.001125\n",
            "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.033579\n",
            "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.002285\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.016083\n",
            "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.036294\n",
            "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.013409\n",
            "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.088087\n",
            "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.006388\n",
            "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.005449\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.020681\n",
            "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.001277\n",
            "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.022193\n",
            "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.001838\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.002969\n",
            "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.025441\n",
            "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.010172\n",
            "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.022043\n",
            "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.013856\n",
            "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.002855\n",
            "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.003101\n",
            "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.025871\n",
            "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.012307\n",
            "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.001631\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.021156\n",
            "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.001388\n",
            "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.004384\n",
            "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.001430\n",
            "\n",
            "Test set: Average loss: 0.0274, Accuracy: 9916/10000 (99%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.000924\n",
            "Train Epoch: 11 [640/60000 (1%)]\tLoss: 0.099752\n",
            "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 0.001290\n",
            "Train Epoch: 11 [1920/60000 (3%)]\tLoss: 0.013377\n",
            "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.004154\n",
            "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 0.044380\n",
            "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 0.006110\n",
            "Train Epoch: 11 [4480/60000 (7%)]\tLoss: 0.071655\n",
            "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.006927\n",
            "Train Epoch: 11 [5760/60000 (10%)]\tLoss: 0.004646\n",
            "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.016175\n",
            "Train Epoch: 11 [7040/60000 (12%)]\tLoss: 0.006333\n",
            "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.021063\n",
            "Train Epoch: 11 [8320/60000 (14%)]\tLoss: 0.005429\n",
            "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 0.067192\n",
            "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 0.004577\n",
            "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.043810\n",
            "Train Epoch: 11 [10880/60000 (18%)]\tLoss: 0.000872\n",
            "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 0.015590\n",
            "Train Epoch: 11 [12160/60000 (20%)]\tLoss: 0.002691\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.015994\n",
            "Train Epoch: 11 [13440/60000 (22%)]\tLoss: 0.005647\n",
            "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 0.000435\n",
            "Train Epoch: 11 [14720/60000 (25%)]\tLoss: 0.013573\n",
            "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.085079\n",
            "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.003624\n",
            "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 0.070346\n",
            "Train Epoch: 11 [17280/60000 (29%)]\tLoss: 0.001951\n",
            "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.060094\n",
            "Train Epoch: 11 [18560/60000 (31%)]\tLoss: 0.027577\n",
            "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.019559\n",
            "Train Epoch: 11 [19840/60000 (33%)]\tLoss: 0.020051\n",
            "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.000344\n",
            "Train Epoch: 11 [21120/60000 (35%)]\tLoss: 0.099673\n",
            "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 0.017783\n",
            "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 0.008794\n",
            "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.000959\n",
            "Train Epoch: 11 [23680/60000 (39%)]\tLoss: 0.006830\n",
            "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 0.033749\n",
            "Train Epoch: 11 [24960/60000 (42%)]\tLoss: 0.005954\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.033185\n",
            "Train Epoch: 11 [26240/60000 (44%)]\tLoss: 0.027605\n",
            "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 0.001312\n",
            "Train Epoch: 11 [27520/60000 (46%)]\tLoss: 0.008576\n",
            "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.067744\n",
            "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 0.072224\n",
            "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 0.006242\n",
            "Train Epoch: 11 [30080/60000 (50%)]\tLoss: 0.032105\n",
            "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.001926\n",
            "Train Epoch: 11 [31360/60000 (52%)]\tLoss: 0.000422\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.013969\n",
            "Train Epoch: 11 [32640/60000 (54%)]\tLoss: 0.030307\n",
            "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.002311\n",
            "Train Epoch: 11 [33920/60000 (57%)]\tLoss: 0.016671\n",
            "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 0.051425\n",
            "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 0.002367\n",
            "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.003797\n",
            "Train Epoch: 11 [36480/60000 (61%)]\tLoss: 0.029134\n",
            "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 0.052103\n",
            "Train Epoch: 11 [37760/60000 (63%)]\tLoss: 0.156439\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.002685\n",
            "Train Epoch: 11 [39040/60000 (65%)]\tLoss: 0.003661\n",
            "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 0.000361\n",
            "Train Epoch: 11 [40320/60000 (67%)]\tLoss: 0.067101\n",
            "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.009324\n",
            "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 0.001034\n",
            "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 0.007710\n",
            "Train Epoch: 11 [42880/60000 (71%)]\tLoss: 0.013637\n",
            "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.010755\n",
            "Train Epoch: 11 [44160/60000 (74%)]\tLoss: 0.004539\n",
            "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.002117\n",
            "Train Epoch: 11 [45440/60000 (76%)]\tLoss: 0.026601\n",
            "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.064778\n",
            "Train Epoch: 11 [46720/60000 (78%)]\tLoss: 0.010556\n",
            "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 0.056730\n",
            "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.061034\n",
            "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.027542\n",
            "Train Epoch: 11 [49280/60000 (82%)]\tLoss: 0.073955\n",
            "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 0.078109\n",
            "Train Epoch: 11 [50560/60000 (84%)]\tLoss: 0.001896\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.023163\n",
            "Train Epoch: 11 [51840/60000 (86%)]\tLoss: 0.011690\n",
            "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 0.030021\n",
            "Train Epoch: 11 [53120/60000 (88%)]\tLoss: 0.032852\n",
            "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 0.003660\n",
            "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 0.109478\n",
            "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 0.022892\n",
            "Train Epoch: 11 [55680/60000 (93%)]\tLoss: 0.015425\n",
            "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.014579\n",
            "Train Epoch: 11 [56960/60000 (95%)]\tLoss: 0.048943\n",
            "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.038869\n",
            "Train Epoch: 11 [58240/60000 (97%)]\tLoss: 0.005879\n",
            "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.005836\n",
            "Train Epoch: 11 [59520/60000 (99%)]\tLoss: 0.002341\n",
            "\n",
            "Test set: Average loss: 0.0270, Accuracy: 9917/10000 (99%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.006547\n",
            "Train Epoch: 12 [640/60000 (1%)]\tLoss: 0.003491\n",
            "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 0.007693\n",
            "Train Epoch: 12 [1920/60000 (3%)]\tLoss: 0.062793\n",
            "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.011544\n",
            "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 0.000155\n",
            "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 0.002657\n",
            "Train Epoch: 12 [4480/60000 (7%)]\tLoss: 0.012704\n",
            "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.000566\n",
            "Train Epoch: 12 [5760/60000 (10%)]\tLoss: 0.058825\n",
            "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.023114\n",
            "Train Epoch: 12 [7040/60000 (12%)]\tLoss: 0.001160\n",
            "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.011072\n",
            "Train Epoch: 12 [8320/60000 (14%)]\tLoss: 0.008079\n",
            "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 0.002706\n",
            "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 0.043337\n",
            "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.015015\n",
            "Train Epoch: 12 [10880/60000 (18%)]\tLoss: 0.024929\n",
            "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 0.057879\n",
            "Train Epoch: 12 [12160/60000 (20%)]\tLoss: 0.036895\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.019482\n",
            "Train Epoch: 12 [13440/60000 (22%)]\tLoss: 0.010004\n",
            "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 0.088525\n",
            "Train Epoch: 12 [14720/60000 (25%)]\tLoss: 0.012605\n",
            "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.001979\n",
            "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.005281\n",
            "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 0.014394\n",
            "Train Epoch: 12 [17280/60000 (29%)]\tLoss: 0.028455\n",
            "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.009984\n",
            "Train Epoch: 12 [18560/60000 (31%)]\tLoss: 0.033538\n",
            "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.002927\n",
            "Train Epoch: 12 [19840/60000 (33%)]\tLoss: 0.011482\n",
            "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.020337\n",
            "Train Epoch: 12 [21120/60000 (35%)]\tLoss: 0.005811\n",
            "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 0.008811\n",
            "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 0.005167\n",
            "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.016918\n",
            "Train Epoch: 12 [23680/60000 (39%)]\tLoss: 0.020062\n",
            "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 0.005445\n",
            "Train Epoch: 12 [24960/60000 (42%)]\tLoss: 0.023136\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.091417\n",
            "Train Epoch: 12 [26240/60000 (44%)]\tLoss: 0.102066\n",
            "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 0.137400\n",
            "Train Epoch: 12 [27520/60000 (46%)]\tLoss: 0.008768\n",
            "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.019595\n",
            "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 0.003703\n",
            "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 0.001003\n",
            "Train Epoch: 12 [30080/60000 (50%)]\tLoss: 0.005583\n",
            "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.006705\n",
            "Train Epoch: 12 [31360/60000 (52%)]\tLoss: 0.022819\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.011812\n",
            "Train Epoch: 12 [32640/60000 (54%)]\tLoss: 0.002409\n",
            "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.040864\n",
            "Train Epoch: 12 [33920/60000 (57%)]\tLoss: 0.000424\n",
            "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 0.010061\n",
            "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 0.084623\n",
            "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.001869\n",
            "Train Epoch: 12 [36480/60000 (61%)]\tLoss: 0.002837\n",
            "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 0.002292\n",
            "Train Epoch: 12 [37760/60000 (63%)]\tLoss: 0.016294\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.011616\n",
            "Train Epoch: 12 [39040/60000 (65%)]\tLoss: 0.000653\n",
            "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 0.004637\n",
            "Train Epoch: 12 [40320/60000 (67%)]\tLoss: 0.004762\n",
            "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.018227\n",
            "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 0.003231\n",
            "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 0.009762\n",
            "Train Epoch: 12 [42880/60000 (71%)]\tLoss: 0.016389\n",
            "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.007209\n",
            "Train Epoch: 12 [44160/60000 (74%)]\tLoss: 0.007288\n",
            "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.001828\n",
            "Train Epoch: 12 [45440/60000 (76%)]\tLoss: 0.005522\n",
            "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.001434\n",
            "Train Epoch: 12 [46720/60000 (78%)]\tLoss: 0.038458\n",
            "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 0.056383\n",
            "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.016398\n",
            "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.095792\n",
            "Train Epoch: 12 [49280/60000 (82%)]\tLoss: 0.000382\n",
            "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 0.001993\n",
            "Train Epoch: 12 [50560/60000 (84%)]\tLoss: 0.002943\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.075567\n",
            "Train Epoch: 12 [51840/60000 (86%)]\tLoss: 0.028892\n",
            "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 0.001510\n",
            "Train Epoch: 12 [53120/60000 (88%)]\tLoss: 0.007956\n",
            "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 0.001281\n",
            "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 0.012546\n",
            "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 0.005027\n",
            "Train Epoch: 12 [55680/60000 (93%)]\tLoss: 0.005577\n",
            "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.057217\n",
            "Train Epoch: 12 [56960/60000 (95%)]\tLoss: 0.006320\n",
            "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.016013\n",
            "Train Epoch: 12 [58240/60000 (97%)]\tLoss: 0.005151\n",
            "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.022272\n",
            "Train Epoch: 12 [59520/60000 (99%)]\tLoss: 0.077489\n",
            "\n",
            "Test set: Average loss: 0.0272, Accuracy: 9915/10000 (99%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.002034\n",
            "Train Epoch: 13 [640/60000 (1%)]\tLoss: 0.002907\n",
            "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 0.011461\n",
            "Train Epoch: 13 [1920/60000 (3%)]\tLoss: 0.022892\n",
            "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.009273\n",
            "Train Epoch: 13 [3200/60000 (5%)]\tLoss: 0.021186\n",
            "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 0.002611\n",
            "Train Epoch: 13 [4480/60000 (7%)]\tLoss: 0.034528\n",
            "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.037916\n",
            "Train Epoch: 13 [5760/60000 (10%)]\tLoss: 0.013318\n",
            "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.013335\n",
            "Train Epoch: 13 [7040/60000 (12%)]\tLoss: 0.035938\n",
            "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.043469\n",
            "Train Epoch: 13 [8320/60000 (14%)]\tLoss: 0.007241\n",
            "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 0.009643\n",
            "Train Epoch: 13 [9600/60000 (16%)]\tLoss: 0.009613\n",
            "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.022924\n",
            "Train Epoch: 13 [10880/60000 (18%)]\tLoss: 0.094457\n",
            "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 0.006714\n",
            "Train Epoch: 13 [12160/60000 (20%)]\tLoss: 0.005168\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.023784\n",
            "Train Epoch: 13 [13440/60000 (22%)]\tLoss: 0.008219\n",
            "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 0.050207\n",
            "Train Epoch: 13 [14720/60000 (25%)]\tLoss: 0.011159\n",
            "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.017096\n",
            "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.050147\n",
            "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 0.006393\n",
            "Train Epoch: 13 [17280/60000 (29%)]\tLoss: 0.000773\n",
            "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.045745\n",
            "Train Epoch: 13 [18560/60000 (31%)]\tLoss: 0.011546\n",
            "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.070836\n",
            "Train Epoch: 13 [19840/60000 (33%)]\tLoss: 0.012141\n",
            "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.004963\n",
            "Train Epoch: 13 [21120/60000 (35%)]\tLoss: 0.001058\n",
            "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 0.000628\n",
            "Train Epoch: 13 [22400/60000 (37%)]\tLoss: 0.011677\n",
            "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.085137\n",
            "Train Epoch: 13 [23680/60000 (39%)]\tLoss: 0.001926\n",
            "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 0.017180\n",
            "Train Epoch: 13 [24960/60000 (42%)]\tLoss: 0.008941\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.031177\n",
            "Train Epoch: 13 [26240/60000 (44%)]\tLoss: 0.004977\n",
            "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 0.001906\n",
            "Train Epoch: 13 [27520/60000 (46%)]\tLoss: 0.154268\n",
            "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.023665\n",
            "Train Epoch: 13 [28800/60000 (48%)]\tLoss: 0.025566\n",
            "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 0.074549\n",
            "Train Epoch: 13 [30080/60000 (50%)]\tLoss: 0.027642\n",
            "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.026083\n",
            "Train Epoch: 13 [31360/60000 (52%)]\tLoss: 0.000719\n",
            "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.001387\n",
            "Train Epoch: 13 [32640/60000 (54%)]\tLoss: 0.001325\n",
            "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.008341\n",
            "Train Epoch: 13 [33920/60000 (57%)]\tLoss: 0.026414\n",
            "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.000975\n",
            "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 0.013771\n",
            "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.017891\n",
            "Train Epoch: 13 [36480/60000 (61%)]\tLoss: 0.028070\n",
            "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 0.001123\n",
            "Train Epoch: 13 [37760/60000 (63%)]\tLoss: 0.029310\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.037708\n",
            "Train Epoch: 13 [39040/60000 (65%)]\tLoss: 0.043254\n",
            "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 0.024152\n",
            "Train Epoch: 13 [40320/60000 (67%)]\tLoss: 0.185436\n",
            "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.020184\n",
            "Train Epoch: 13 [41600/60000 (69%)]\tLoss: 0.005278\n",
            "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 0.093891\n",
            "Train Epoch: 13 [42880/60000 (71%)]\tLoss: 0.029307\n",
            "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.009756\n",
            "Train Epoch: 13 [44160/60000 (74%)]\tLoss: 0.012902\n",
            "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.004138\n",
            "Train Epoch: 13 [45440/60000 (76%)]\tLoss: 0.037025\n",
            "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.006562\n",
            "Train Epoch: 13 [46720/60000 (78%)]\tLoss: 0.000673\n",
            "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 0.223461\n",
            "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.007131\n",
            "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.013694\n",
            "Train Epoch: 13 [49280/60000 (82%)]\tLoss: 0.003173\n",
            "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 0.042505\n",
            "Train Epoch: 13 [50560/60000 (84%)]\tLoss: 0.012744\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.022323\n",
            "Train Epoch: 13 [51840/60000 (86%)]\tLoss: 0.018649\n",
            "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 0.015898\n",
            "Train Epoch: 13 [53120/60000 (88%)]\tLoss: 0.225433\n",
            "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 0.045580\n",
            "Train Epoch: 13 [54400/60000 (91%)]\tLoss: 0.004093\n",
            "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 0.007501\n",
            "Train Epoch: 13 [55680/60000 (93%)]\tLoss: 0.000408\n",
            "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.001055\n",
            "Train Epoch: 13 [56960/60000 (95%)]\tLoss: 0.001495\n",
            "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.008279\n",
            "Train Epoch: 13 [58240/60000 (97%)]\tLoss: 0.028851\n",
            "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.034800\n",
            "Train Epoch: 13 [59520/60000 (99%)]\tLoss: 0.007563\n",
            "\n",
            "Test set: Average loss: 0.0279, Accuracy: 9913/10000 (99%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.006143\n",
            "Train Epoch: 14 [640/60000 (1%)]\tLoss: 0.047476\n",
            "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 0.016009\n",
            "Train Epoch: 14 [1920/60000 (3%)]\tLoss: 0.114286\n",
            "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.005981\n",
            "Train Epoch: 14 [3200/60000 (5%)]\tLoss: 0.043703\n",
            "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 0.024318\n",
            "Train Epoch: 14 [4480/60000 (7%)]\tLoss: 0.000933\n",
            "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.095799\n",
            "Train Epoch: 14 [5760/60000 (10%)]\tLoss: 0.067893\n",
            "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.007708\n",
            "Train Epoch: 14 [7040/60000 (12%)]\tLoss: 0.040250\n",
            "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.002770\n",
            "Train Epoch: 14 [8320/60000 (14%)]\tLoss: 0.017494\n",
            "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 0.000439\n",
            "Train Epoch: 14 [9600/60000 (16%)]\tLoss: 0.018860\n",
            "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.006890\n",
            "Train Epoch: 14 [10880/60000 (18%)]\tLoss: 0.002018\n",
            "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 0.003231\n",
            "Train Epoch: 14 [12160/60000 (20%)]\tLoss: 0.001001\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.007017\n",
            "Train Epoch: 14 [13440/60000 (22%)]\tLoss: 0.006275\n",
            "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 0.004656\n",
            "Train Epoch: 14 [14720/60000 (25%)]\tLoss: 0.007264\n",
            "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.003325\n",
            "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.013540\n",
            "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 0.002053\n",
            "Train Epoch: 14 [17280/60000 (29%)]\tLoss: 0.001398\n",
            "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.006784\n",
            "Train Epoch: 14 [18560/60000 (31%)]\tLoss: 0.001805\n",
            "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.004145\n",
            "Train Epoch: 14 [19840/60000 (33%)]\tLoss: 0.053631\n",
            "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.017108\n",
            "Train Epoch: 14 [21120/60000 (35%)]\tLoss: 0.009192\n",
            "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 0.001968\n",
            "Train Epoch: 14 [22400/60000 (37%)]\tLoss: 0.002589\n",
            "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.084569\n",
            "Train Epoch: 14 [23680/60000 (39%)]\tLoss: 0.015540\n",
            "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 0.068954\n",
            "Train Epoch: 14 [24960/60000 (42%)]\tLoss: 0.076118\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.064696\n",
            "Train Epoch: 14 [26240/60000 (44%)]\tLoss: 0.041434\n",
            "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 0.008721\n",
            "Train Epoch: 14 [27520/60000 (46%)]\tLoss: 0.015543\n",
            "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.003207\n",
            "Train Epoch: 14 [28800/60000 (48%)]\tLoss: 0.011151\n",
            "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 0.000597\n",
            "Train Epoch: 14 [30080/60000 (50%)]\tLoss: 0.004575\n",
            "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.027213\n",
            "Train Epoch: 14 [31360/60000 (52%)]\tLoss: 0.001026\n",
            "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.069596\n",
            "Train Epoch: 14 [32640/60000 (54%)]\tLoss: 0.009242\n",
            "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.032503\n",
            "Train Epoch: 14 [33920/60000 (57%)]\tLoss: 0.031119\n",
            "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 0.041392\n",
            "Train Epoch: 14 [35200/60000 (59%)]\tLoss: 0.001725\n",
            "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.010601\n",
            "Train Epoch: 14 [36480/60000 (61%)]\tLoss: 0.114881\n",
            "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 0.004322\n",
            "Train Epoch: 14 [37760/60000 (63%)]\tLoss: 0.000731\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.058046\n",
            "Train Epoch: 14 [39040/60000 (65%)]\tLoss: 0.022792\n",
            "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 0.019045\n",
            "Train Epoch: 14 [40320/60000 (67%)]\tLoss: 0.005441\n",
            "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.013438\n",
            "Train Epoch: 14 [41600/60000 (69%)]\tLoss: 0.005717\n",
            "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 0.007483\n",
            "Train Epoch: 14 [42880/60000 (71%)]\tLoss: 0.010003\n",
            "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.011611\n",
            "Train Epoch: 14 [44160/60000 (74%)]\tLoss: 0.056638\n",
            "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.000543\n",
            "Train Epoch: 14 [45440/60000 (76%)]\tLoss: 0.039799\n",
            "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.004245\n",
            "Train Epoch: 14 [46720/60000 (78%)]\tLoss: 0.024708\n",
            "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 0.044743\n",
            "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.087247\n",
            "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.003015\n",
            "Train Epoch: 14 [49280/60000 (82%)]\tLoss: 0.032244\n",
            "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 0.005684\n",
            "Train Epoch: 14 [50560/60000 (84%)]\tLoss: 0.001272\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.065831\n",
            "Train Epoch: 14 [51840/60000 (86%)]\tLoss: 0.000812\n",
            "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 0.019136\n",
            "Train Epoch: 14 [53120/60000 (88%)]\tLoss: 0.003352\n",
            "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 0.003708\n",
            "Train Epoch: 14 [54400/60000 (91%)]\tLoss: 0.002208\n",
            "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 0.025542\n",
            "Train Epoch: 14 [55680/60000 (93%)]\tLoss: 0.003260\n",
            "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.018648\n",
            "Train Epoch: 14 [56960/60000 (95%)]\tLoss: 0.019554\n",
            "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.001885\n",
            "Train Epoch: 14 [58240/60000 (97%)]\tLoss: 0.011090\n",
            "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.002160\n",
            "Train Epoch: 14 [59520/60000 (99%)]\tLoss: 0.001996\n",
            "\n",
            "Test set: Average loss: 0.0274, Accuracy: 9915/10000 (99%)\n",
            "\n",
            "CPU times: user 54.7 s, sys: 8.01 s, total: 1min 2s\n",
            "Wall time: 2min 56s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    ls = []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            # add to list which gets returned\n",
        "            ls.append((\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            \n",
        "            if args.dry_run:\n",
        "                return(ls)\n",
        "                break\n",
        "    return ls\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return [(test_loss, correct, len(test_loader.dataset),\n",
        "             100. * correct / len(test_loader.dataset))]\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
        "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
        "                        help='input batch size for training (default: 64)')\n",
        "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
        "                        help='input batch size for testing (default: 1000)')\n",
        "    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
        "                        help='number of epochs to train (default: 14)')\n",
        "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
        "                        help='learning rate (default: 1.0)')\n",
        "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
        "                        help='Learning rate step gamma (default: 0.7)')\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                        help='disables CUDA training')\n",
        "    parser.add_argument('--no-mps', action='store_true', default=False,\n",
        "                        help='disables macOS GPU training')\n",
        "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
        "                        help='quickly check a single pass')\n",
        "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                        help='random seed (default: 1)')\n",
        "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "                        help='how many batches to wait before logging training status')\n",
        "    parser.add_argument('--save-model', action='store_true', default=False,\n",
        "                        help='For Saving the current Model')\n",
        "    parser.add_argument('-f')\n",
        "    args = parser.parse_args()\n",
        "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "    use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    if use_cuda:\n",
        "        device = torch.device(\"cuda\")\n",
        "    elif use_mps:\n",
        "        device = torch.device(\"mps\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    train_kwargs = {'batch_size': args.batch_size}\n",
        "    test_kwargs = {'batch_size': args.test_batch_size}\n",
        "    if use_cuda:\n",
        "        cuda_kwargs = {'num_workers': 1,\n",
        "                       'pin_memory': True,\n",
        "                       'shuffle': True}\n",
        "        train_kwargs.update(cuda_kwargs)\n",
        "        test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "    dataset2 = datasets.MNIST('../data', train=False,\n",
        "                       transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
        "\n",
        "    model = Net().to(device)\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
        "\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
        "    train_ls = []\n",
        "    test_ls = []\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        curr_tr_ls = train(args, model, device, train_loader, optimizer, epoch)\n",
        "        train_ls.extend(curr_tr_ls)\n",
        "        curr_te_ls = test(model, device, test_loader)\n",
        "        test_ls.extend(curr_te_ls)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    if args.save_model:\n",
        "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "\n",
        "    base_outpth = '/content/gdrive/My Drive/colab_notebooks/data/'\n",
        "    tr_outpth = os.path.join(base_outpth, 'train_stats.csv')\n",
        "    te_outpth = os.path.join(base_outpth, 'test_stats.csv')\n",
        "\n",
        "    train_stats = pd.DataFrame(train_ls, columns=['epoch', 'train_example_num', 'num_examples', 'percentage_of_examples', 'loss'])\n",
        "    test_stats = pd.DataFrame(test_ls, columns=['average_loss', 'num_correct', 'test_size', 'accuracy'])\n",
        "    \n",
        "    train_stats.to_csv(tr_outpth, index=False)\n",
        "    test_stats.to_csv(te_outpth, index=False)\n",
        "\n",
        "    return train_stats, test_stats\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_stats, test_stats = main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_stats.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "pPEsYjb57QHJ",
        "outputId": "0c9e703a-9a54-4e9c-cf8b-88b74ef2718b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    average_loss  num_correct  test_size  accuracy\n",
              "9       0.027705         9915      10000     99.15\n",
              "10      0.027111         9920      10000     99.20\n",
              "11      0.027103         9916      10000     99.16\n",
              "12      0.027714         9914      10000     99.14\n",
              "13      0.027283         9917      10000     99.17"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4bf60cf3-1b45-4cb0-8b09-5cdec4cb3f5f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>average_loss</th>\n",
              "      <th>num_correct</th>\n",
              "      <th>test_size</th>\n",
              "      <th>accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.027705</td>\n",
              "      <td>9915</td>\n",
              "      <td>10000</td>\n",
              "      <td>99.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.027111</td>\n",
              "      <td>9920</td>\n",
              "      <td>10000</td>\n",
              "      <td>99.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.027103</td>\n",
              "      <td>9916</td>\n",
              "      <td>10000</td>\n",
              "      <td>99.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.027714</td>\n",
              "      <td>9914</td>\n",
              "      <td>10000</td>\n",
              "      <td>99.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.027283</td>\n",
              "      <td>9917</td>\n",
              "      <td>10000</td>\n",
              "      <td>99.17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4bf60cf3-1b45-4cb0-8b09-5cdec4cb3f5f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4bf60cf3-1b45-4cb0-8b09-5cdec4cb3f5f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4bf60cf3-1b45-4cb0-8b09-5cdec4cb3f5f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opacus\n",
        "# !pip install torchvision\n",
        "# !pip install torch==1.8.0\n",
        "# !pip install torchvision==0.9.0\n",
        "# !pip install torchaudio==0.8.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmOEnnU3Wyca",
        "outputId": "46c94335-396c-4dae-dccc-e5e7e42f43fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opacus in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: functorch in /usr/local/lib/python3.7/dist-packages (from opacus) (1.13.0)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.7.3)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from opacus) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8->opacus) (4.1.1)\n",
            "Collecting torch>=1.8\n",
            "  Using cached torch-1.13.0-cp37-cp37m-manylinux1_x86_64.whl (890.2 MB)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.7/dist-packages (from torch>=1.8->opacus) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.7/dist-packages (from torch>=1.8->opacus) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.7/dist-packages (from torch>=1.8->opacus) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.7/dist-packages (from torch>=1.8->opacus) (8.5.0.96)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8->opacus) (0.37.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8->opacus) (57.4.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.8.0\n",
            "    Uninstalling torch-1.8.0:\n",
            "      Successfully uninstalled torch-1.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.9.0 requires torch==1.8.0, but you have torch 1.13.0 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchaudio 0.8.0 requires torch==1.8.0, but you have torch 1.13.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from __future__ import print_function\n",
        "from pandas.core.base import NoNewAttributesMixin\n",
        "from opacus import PrivacyEngine\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        \n",
        "        # self.privacy_engine = PrivacyEngine()\n",
        "        # self.epilon = None\n",
        "        # self.delta = None\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch, privacy_map):\n",
        "    model.train()\n",
        "    ls = []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            curr_privacy = round(privacy_map['privacy_engine'].get_epsilon(privacy_map['target_delta']), 3)\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Privacy loss: {}, {}/{}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item(),\n",
        "                curr_privacy, \n",
        "                privacy_map['target_delta'], privacy_map['target_epsilon']))\n",
        "            # add to list which gets returned\n",
        "            ls.append((\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item(),\n",
        "                curr_privacy))\n",
        "            \n",
        "            if args.dry_run:\n",
        "                return(ls)\n",
        "                break\n",
        "    return ls\n",
        "\n",
        "def test(model, device, test_loader, privacy_map):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    curr_privacy = round(privacy_map['privacy_engine'].get_epsilon(privacy_map['target_delta']), 3)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Privacy: {},{}/{}\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset),\n",
        "        curr_privacy, \n",
        "        privacy_map['target_delta'], privacy_map['target_epsilon']))\n",
        "    return [(test_loss, correct, len(test_loader.dataset),\n",
        "             100. * correct / len(test_loader.dataset), curr_privacy)]\n",
        "\n",
        "def main():\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
        "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
        "                        help='input batch size for training (default: 64)')\n",
        "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
        "                        help='input batch size for testing (default: 1000)')\n",
        "    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
        "                        help='number of epochs to train (default: 14)')\n",
        "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
        "                        help='learning rate (default: 1.0)')\n",
        "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
        "                        help='Learning rate step gamma (default: 0.7)')\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                        help='disables CUDA training')\n",
        "    parser.add_argument('--no-mps', action='store_true', default=False,\n",
        "                        help='disables macOS GPU training')\n",
        "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
        "                        help='quickly check a single pass')\n",
        "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                        help='random seed (default: 1)')\n",
        "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "                        help='how many batches to wait before logging training status')\n",
        "    parser.add_argument('--save-model', action='store_true', default=False,\n",
        "                        help='For Saving the current Model')\n",
        "    parser.add_argument('-f')\n",
        "    args = parser.parse_args()\n",
        "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "    use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    if use_cuda:\n",
        "        device = torch.device(\"cuda\")\n",
        "    elif use_mps:\n",
        "        device = torch.device(\"mps\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    train_kwargs = {'batch_size': args.batch_size}\n",
        "    test_kwargs = {'batch_size': args.test_batch_size}\n",
        "    if use_cuda:\n",
        "        cuda_kwargs = {'num_workers': 1,\n",
        "                       'pin_memory': True,\n",
        "                       'shuffle': True}\n",
        "        train_kwargs.update(cuda_kwargs)\n",
        "        test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "    dataset2 = datasets.MNIST('../data', train=False,\n",
        "                       transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
        "\n",
        "    model = Net().to(device)\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
        "\n",
        "    # Privacy budget - uses settings from this:\n",
        "    # https://github.com/pytorch/opacus/blob/main/tutorials/building_image_classifier.ipynb\n",
        "\n",
        "    MAX_GRAD_NORM = 1.2\n",
        "    EPSILON = 3\n",
        "    DELTA = 1e-5\n",
        "    EPOCHS = args.epochs\n",
        "\n",
        "    LR = 1e-3\n",
        "\n",
        "    privacy_engine = PrivacyEngine()\n",
        "\n",
        "    model, optimizer, data_loader = privacy_engine.make_private_with_epsilon(\n",
        "                                                    module=model,\n",
        "                                                    optimizer=optimizer,\n",
        "                                                    data_loader=train_loader,\n",
        "                                                    epochs=EPOCHS,\n",
        "                                                    target_epsilon=EPSILON,\n",
        "                                                    target_delta=DELTA,\n",
        "                                                    max_grad_norm=MAX_GRAD_NORM)\n",
        "    \n",
        "    # model.epsilon = EPSILON\n",
        "    # model.delta = DELTA\n",
        "\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
        "    train_ls = []\n",
        "    test_ls = []\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        curr_tr_ls = train(args, model, device, train_loader, optimizer, epoch,\n",
        "                           {'privacy_engine': privacy_engine,\n",
        "                           'target_epsilon':EPSILON,\n",
        "                           'target_delta': DELTA})\n",
        "        train_ls.extend(curr_tr_ls)\n",
        "        curr_te_ls = test(model, device, test_loader, \n",
        "                          {'privacy_engine': privacy_engine,\n",
        "                           'target_epsilon':EPSILON,\n",
        "                           'target_delta': DELTA})\n",
        "        test_ls.extend(curr_te_ls)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    if args.save_model:\n",
        "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "\n",
        "    base_outpth = '/content/gdrive/My Drive/colab_notebooks/data/'\n",
        "    tr_outpth = os.path.join(base_outpth, 'p_train_stats.csv')\n",
        "    te_outpth = os.path.join(base_outpth, 'p_test_stats.csv')\n",
        "\n",
        "    train_stats = pd.DataFrame(train_ls, columns=['epoch', 'train_example_num', 'num_examples', 'percentage_of_examples', 'loss', 'curr_privacy'])\n",
        "    test_stats = pd.DataFrame(test_ls, columns=['average_loss', 'num_correct', 'test_size', 'accuracy', 'curr_privacy'])\n",
        "    \n",
        "    train_stats.to_csv(tr_outpth, index=False)\n",
        "    test_stats.to_csv(te_outpth, index=False)\n",
        "\n",
        "    return train_stats, test_stats\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    p_train_stats, p_test_stats = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Tj1oRZhGSzs",
        "outputId": "14b9166a-f32a-4868-8366-2fdd5c797720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299824, Privacy loss: 1.681, 1e-05/3\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.250466, Privacy loss: 1.811, 1e-05/3\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.103608, Privacy loss: 1.851, 1e-05/3\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 1.780792, Privacy loss: 1.875, 1e-05/3\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.566189, Privacy loss: 1.896, 1e-05/3\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.654005, Privacy loss: 1.91, 1e-05/3\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.131251, Privacy loss: 1.923, 1e-05/3\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 1.559924, Privacy loss: 1.936, 1e-05/3\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.270841, Privacy loss: 1.947, 1e-05/3\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.419631, Privacy loss: 1.954, 1e-05/3\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.306369, Privacy loss: 1.961, 1e-05/3\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.593855, Privacy loss: 1.969, 1e-05/3\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.528175, Privacy loss: 1.976, 1e-05/3\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.275162, Privacy loss: 1.984, 1e-05/3\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.520039, Privacy loss: 1.991, 1e-05/3\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.344908, Privacy loss: 1.999, 1e-05/3\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.258347, Privacy loss: 2.003, 1e-05/3\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.568482, Privacy loss: 2.007, 1e-05/3\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.692865, Privacy loss: 2.012, 1e-05/3\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.666701, Privacy loss: 2.016, 1e-05/3\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.432406, Privacy loss: 2.021, 1e-05/3\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.429780, Privacy loss: 2.025, 1e-05/3\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.182471, Privacy loss: 2.03, 1e-05/3\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.245973, Privacy loss: 2.034, 1e-05/3\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.809278, Privacy loss: 2.038, 1e-05/3\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.145459, Privacy loss: 2.043, 1e-05/3\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.305846, Privacy loss: 2.047, 1e-05/3\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 1.424456, Privacy loss: 2.052, 1e-05/3\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.181791, Privacy loss: 2.056, 1e-05/3\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 1.996023, Privacy loss: 2.06, 1e-05/3\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.508360, Privacy loss: 2.062, 1e-05/3\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 1.284542, Privacy loss: 2.065, 1e-05/3\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.581389, Privacy loss: 2.068, 1e-05/3\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 1.270007, Privacy loss: 2.071, 1e-05/3\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 1.932783, Privacy loss: 2.073, 1e-05/3\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.500041, Privacy loss: 2.076, 1e-05/3\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.115327, Privacy loss: 2.079, 1e-05/3\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 1.719810, Privacy loss: 2.082, 1e-05/3\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 1.448238, Privacy loss: 2.085, 1e-05/3\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 2.496607, Privacy loss: 2.087, 1e-05/3\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.788749, Privacy loss: 2.09, 1e-05/3\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 2.055244, Privacy loss: 2.093, 1e-05/3\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 1.885249, Privacy loss: 2.096, 1e-05/3\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 2.048090, Privacy loss: 2.098, 1e-05/3\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 1.500132, Privacy loss: 2.101, 1e-05/3\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 1.881874, Privacy loss: 2.104, 1e-05/3\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 1.442462, Privacy loss: 2.107, 1e-05/3\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 1.617130, Privacy loss: 2.109, 1e-05/3\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.034268, Privacy loss: 2.112, 1e-05/3\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 2.450748, Privacy loss: 2.115, 1e-05/3\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.229719, Privacy loss: 2.118, 1e-05/3\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 1.663200, Privacy loss: 2.121, 1e-05/3\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 1.820786, Privacy loss: 2.123, 1e-05/3\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 1.671713, Privacy loss: 2.126, 1e-05/3\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 1.160837, Privacy loss: 2.127, 1e-05/3\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 2.046572, Privacy loss: 2.129, 1e-05/3\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 1.358305, Privacy loss: 2.131, 1e-05/3\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 2.383597, Privacy loss: 2.133, 1e-05/3\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 2.059946, Privacy loss: 2.135, 1e-05/3\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 1.777690, Privacy loss: 2.137, 1e-05/3\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.238410, Privacy loss: 2.138, 1e-05/3\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.998414, Privacy loss: 2.14, 1e-05/3\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 2.096514, Privacy loss: 2.142, 1e-05/3\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 2.696563, Privacy loss: 2.144, 1e-05/3\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 1.593683, Privacy loss: 2.146, 1e-05/3\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 1.620483, Privacy loss: 2.148, 1e-05/3\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 2.131261, Privacy loss: 2.149, 1e-05/3\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 1.939889, Privacy loss: 2.151, 1e-05/3\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 1.360167, Privacy loss: 2.153, 1e-05/3\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 1.183293, Privacy loss: 2.155, 1e-05/3\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.836094, Privacy loss: 2.157, 1e-05/3\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 2.948676, Privacy loss: 2.159, 1e-05/3\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 1.904752, Privacy loss: 2.16, 1e-05/3\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 1.518819, Privacy loss: 2.162, 1e-05/3\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 3.113249, Privacy loss: 2.164, 1e-05/3\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.831540, Privacy loss: 2.166, 1e-05/3\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 2.429182, Privacy loss: 2.168, 1e-05/3\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 1.630199, Privacy loss: 2.17, 1e-05/3\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 2.084350, Privacy loss: 2.171, 1e-05/3\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 1.580254, Privacy loss: 2.173, 1e-05/3\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.139246, Privacy loss: 2.175, 1e-05/3\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 1.749060, Privacy loss: 2.177, 1e-05/3\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 1.289149, Privacy loss: 2.179, 1e-05/3\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 1.093201, Privacy loss: 2.181, 1e-05/3\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.369241, Privacy loss: 2.182, 1e-05/3\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 2.024646, Privacy loss: 2.184, 1e-05/3\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 2.416713, Privacy loss: 2.186, 1e-05/3\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 1.785861, Privacy loss: 2.188, 1e-05/3\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 2.109566, Privacy loss: 2.19, 1e-05/3\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 1.772725, Privacy loss: 2.192, 1e-05/3\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.736270, Privacy loss: 2.193, 1e-05/3\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 1.001227, Privacy loss: 2.195, 1e-05/3\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 1.435699, Privacy loss: 2.197, 1e-05/3\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.935724, Privacy loss: 2.199, 1e-05/3\n",
            "\n",
            "Test set: Average loss: 0.5377, Accuracy: 8721/10000 (87%), Privacy: 2.2,1e-05/3\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.580933, Privacy loss: 2.2, 1e-05/3\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 1.582718, Privacy loss: 2.202, 1e-05/3\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 1.640441, Privacy loss: 2.203, 1e-05/3\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 2.211079, Privacy loss: 2.205, 1e-05/3\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 1.693082, Privacy loss: 2.206, 1e-05/3\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 1.529090, Privacy loss: 2.207, 1e-05/3\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 1.798714, Privacy loss: 2.208, 1e-05/3\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 1.125091, Privacy loss: 2.21, 1e-05/3\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 1.882040, Privacy loss: 2.211, 1e-05/3\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 1.259303, Privacy loss: 2.212, 1e-05/3\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.842715, Privacy loss: 2.214, 1e-05/3\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 1.700198, Privacy loss: 2.215, 1e-05/3\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 1.866065, Privacy loss: 2.216, 1e-05/3\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 2.099974, Privacy loss: 2.217, 1e-05/3\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 1.527008, Privacy loss: 2.219, 1e-05/3\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 1.797295, Privacy loss: 2.22, 1e-05/3\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 1.818182, Privacy loss: 2.221, 1e-05/3\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 1.714273, Privacy loss: 2.223, 1e-05/3\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 2.435978, Privacy loss: 2.224, 1e-05/3\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 1.106835, Privacy loss: 2.225, 1e-05/3\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.250937, Privacy loss: 2.226, 1e-05/3\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.936237, Privacy loss: 2.228, 1e-05/3\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 1.360940, Privacy loss: 2.229, 1e-05/3\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 1.616116, Privacy loss: 2.23, 1e-05/3\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 1.791748, Privacy loss: 2.232, 1e-05/3\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.781903, Privacy loss: 2.233, 1e-05/3\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 1.971330, Privacy loss: 2.234, 1e-05/3\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 1.495344, Privacy loss: 2.235, 1e-05/3\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 1.049831, Privacy loss: 2.237, 1e-05/3\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 1.706770, Privacy loss: 2.238, 1e-05/3\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.469818, Privacy loss: 2.239, 1e-05/3\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 1.620677, Privacy loss: 2.24, 1e-05/3\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 1.910175, Privacy loss: 2.242, 1e-05/3\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 1.363592, Privacy loss: 2.243, 1e-05/3\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 1.128337, Privacy loss: 2.244, 1e-05/3\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 1.698930, Privacy loss: 2.246, 1e-05/3\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 1.150075, Privacy loss: 2.247, 1e-05/3\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 2.334062, Privacy loss: 2.248, 1e-05/3\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.679293, Privacy loss: 2.249, 1e-05/3\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 1.672301, Privacy loss: 2.251, 1e-05/3\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.614187, Privacy loss: 2.252, 1e-05/3\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 1.937077, Privacy loss: 2.253, 1e-05/3\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 1.123655, Privacy loss: 2.255, 1e-05/3\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 1.521450, Privacy loss: 2.256, 1e-05/3\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 1.972490, Privacy loss: 2.257, 1e-05/3\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 1.819343, Privacy loss: 2.258, 1e-05/3\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 1.915341, Privacy loss: 2.26, 1e-05/3\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 1.217025, Privacy loss: 2.261, 1e-05/3\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 1.267108, Privacy loss: 2.262, 1e-05/3\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 1.218290, Privacy loss: 2.264, 1e-05/3\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.442230, Privacy loss: 2.265, 1e-05/3\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 1.867110, Privacy loss: 2.266, 1e-05/3\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 1.708801, Privacy loss: 2.267, 1e-05/3\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 1.019035, Privacy loss: 2.269, 1e-05/3\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 2.529390, Privacy loss: 2.27, 1e-05/3\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 1.579612, Privacy loss: 2.271, 1e-05/3\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 2.004769, Privacy loss: 2.272, 1e-05/3\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 2.428714, Privacy loss: 2.274, 1e-05/3\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 2.172983, Privacy loss: 2.275, 1e-05/3\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 2.069996, Privacy loss: 2.276, 1e-05/3\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.361187, Privacy loss: 2.278, 1e-05/3\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.953780, Privacy loss: 2.279, 1e-05/3\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 1.318616, Privacy loss: 2.28, 1e-05/3\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 1.203530, Privacy loss: 2.281, 1e-05/3\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 1.691153, Privacy loss: 2.283, 1e-05/3\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 1.626602, Privacy loss: 2.284, 1e-05/3\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 2.325267, Privacy loss: 2.285, 1e-05/3\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 2.142272, Privacy loss: 2.287, 1e-05/3\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 1.602887, Privacy loss: 2.288, 1e-05/3\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 1.598029, Privacy loss: 2.289, 1e-05/3\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.609382, Privacy loss: 2.29, 1e-05/3\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 1.764423, Privacy loss: 2.292, 1e-05/3\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 1.594475, Privacy loss: 2.293, 1e-05/3\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 1.362092, Privacy loss: 2.294, 1e-05/3\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.640394, Privacy loss: 2.295, 1e-05/3\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 1.481437, Privacy loss: 2.296, 1e-05/3\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 2.251720, Privacy loss: 2.297, 1e-05/3\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 2.394761, Privacy loss: 2.298, 1e-05/3\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 1.039464, Privacy loss: 2.299, 1e-05/3\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 1.381218, Privacy loss: 2.3, 1e-05/3\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.368616, Privacy loss: 2.3, 1e-05/3\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 1.462119, Privacy loss: 2.301, 1e-05/3\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 1.390288, Privacy loss: 2.302, 1e-05/3\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 1.979269, Privacy loss: 2.303, 1e-05/3\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 1.533074, Privacy loss: 2.304, 1e-05/3\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 1.824778, Privacy loss: 2.305, 1e-05/3\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 1.186392, Privacy loss: 2.306, 1e-05/3\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 1.823080, Privacy loss: 2.307, 1e-05/3\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 1.363132, Privacy loss: 2.308, 1e-05/3\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 2.068530, Privacy loss: 2.309, 1e-05/3\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.900034, Privacy loss: 2.31, 1e-05/3\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 1.887129, Privacy loss: 2.311, 1e-05/3\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 1.887693, Privacy loss: 2.312, 1e-05/3\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 2.190200, Privacy loss: 2.313, 1e-05/3\n",
            "\n",
            "Test set: Average loss: 0.4893, Accuracy: 8961/10000 (90%), Privacy: 2.314,1e-05/3\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.968486, Privacy loss: 2.314, 1e-05/3\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 2.225862, Privacy loss: 2.315, 1e-05/3\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 1.717604, Privacy loss: 2.316, 1e-05/3\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 1.856008, Privacy loss: 2.316, 1e-05/3\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.695136, Privacy loss: 2.317, 1e-05/3\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 3.310776, Privacy loss: 2.318, 1e-05/3\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 1.503314, Privacy loss: 2.319, 1e-05/3\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 1.379097, Privacy loss: 2.32, 1e-05/3\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 1.810985, Privacy loss: 2.321, 1e-05/3\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 1.699372, Privacy loss: 2.322, 1e-05/3\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.599412, Privacy loss: 2.323, 1e-05/3\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 2.238782, Privacy loss: 2.324, 1e-05/3\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 1.550294, Privacy loss: 2.325, 1e-05/3\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 1.147230, Privacy loss: 2.326, 1e-05/3\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 1.480073, Privacy loss: 2.327, 1e-05/3\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 2.413182, Privacy loss: 2.328, 1e-05/3\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 2.326535, Privacy loss: 2.329, 1e-05/3\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 1.096024, Privacy loss: 2.33, 1e-05/3\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 1.472685, Privacy loss: 2.331, 1e-05/3\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 1.222705, Privacy loss: 2.332, 1e-05/3\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.475936, Privacy loss: 2.333, 1e-05/3\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 1.267213, Privacy loss: 2.334, 1e-05/3\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 1.557643, Privacy loss: 2.335, 1e-05/3\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.871297, Privacy loss: 2.335, 1e-05/3\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 2.217655, Privacy loss: 2.336, 1e-05/3\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 2.043042, Privacy loss: 2.337, 1e-05/3\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 1.053571, Privacy loss: 2.338, 1e-05/3\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 2.543535, Privacy loss: 2.339, 1e-05/3\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 3.223556, Privacy loss: 2.34, 1e-05/3\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 2.186709, Privacy loss: 2.341, 1e-05/3\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.032799, Privacy loss: 2.342, 1e-05/3\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 1.272671, Privacy loss: 2.343, 1e-05/3\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 1.037509, Privacy loss: 2.344, 1e-05/3\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 1.325783, Privacy loss: 2.345, 1e-05/3\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 1.123341, Privacy loss: 2.346, 1e-05/3\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 1.912985, Privacy loss: 2.347, 1e-05/3\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 1.082392, Privacy loss: 2.348, 1e-05/3\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 1.147678, Privacy loss: 2.349, 1e-05/3\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 1.210117, Privacy loss: 2.35, 1e-05/3\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 1.360810, Privacy loss: 2.351, 1e-05/3\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.390461, Privacy loss: 2.352, 1e-05/3\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 2.353995, Privacy loss: 2.353, 1e-05/3\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 1.685520, Privacy loss: 2.354, 1e-05/3\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 2.257228, Privacy loss: 2.355, 1e-05/3\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 1.173044, Privacy loss: 2.355, 1e-05/3\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 2.341914, Privacy loss: 2.356, 1e-05/3\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.988632, Privacy loss: 2.357, 1e-05/3\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 1.748096, Privacy loss: 2.358, 1e-05/3\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 1.750498, Privacy loss: 2.359, 1e-05/3\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 1.664809, Privacy loss: 2.36, 1e-05/3\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.850793, Privacy loss: 2.361, 1e-05/3\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 1.622100, Privacy loss: 2.362, 1e-05/3\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 1.041381, Privacy loss: 2.363, 1e-05/3\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 1.064883, Privacy loss: 2.364, 1e-05/3\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 1.464922, Privacy loss: 2.365, 1e-05/3\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 1.576402, Privacy loss: 2.366, 1e-05/3\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 1.507253, Privacy loss: 2.367, 1e-05/3\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 1.596125, Privacy loss: 2.368, 1e-05/3\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 1.649557, Privacy loss: 2.369, 1e-05/3\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 2.313163, Privacy loss: 2.37, 1e-05/3\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.210289, Privacy loss: 2.371, 1e-05/3\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 1.481245, Privacy loss: 2.372, 1e-05/3\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 1.501192, Privacy loss: 2.373, 1e-05/3\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 1.742946, Privacy loss: 2.374, 1e-05/3\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 1.881169, Privacy loss: 2.374, 1e-05/3\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 1.478018, Privacy loss: 2.375, 1e-05/3\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 1.017330, Privacy loss: 2.376, 1e-05/3\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 1.568424, Privacy loss: 2.377, 1e-05/3\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 1.263315, Privacy loss: 2.378, 1e-05/3\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 2.008051, Privacy loss: 2.379, 1e-05/3\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.410384, Privacy loss: 2.38, 1e-05/3\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 2.394321, Privacy loss: 2.381, 1e-05/3\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 1.603397, Privacy loss: 2.382, 1e-05/3\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 1.391283, Privacy loss: 2.383, 1e-05/3\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 1.435069, Privacy loss: 2.384, 1e-05/3\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 1.252669, Privacy loss: 2.385, 1e-05/3\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 1.224508, Privacy loss: 2.386, 1e-05/3\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 1.901101, Privacy loss: 2.387, 1e-05/3\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.677966, Privacy loss: 2.388, 1e-05/3\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 1.426427, Privacy loss: 2.389, 1e-05/3\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.464040, Privacy loss: 2.39, 1e-05/3\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.874620, Privacy loss: 2.391, 1e-05/3\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 1.274709, Privacy loss: 2.392, 1e-05/3\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 2.683679, Privacy loss: 2.393, 1e-05/3\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 1.772763, Privacy loss: 2.394, 1e-05/3\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 1.496708, Privacy loss: 2.394, 1e-05/3\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 1.451027, Privacy loss: 2.395, 1e-05/3\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 1.806822, Privacy loss: 2.396, 1e-05/3\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 2.534302, Privacy loss: 2.397, 1e-05/3\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.595063, Privacy loss: 2.398, 1e-05/3\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 1.155187, Privacy loss: 2.399, 1e-05/3\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 1.446502, Privacy loss: 2.4, 1e-05/3\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 1.927639, Privacy loss: 2.401, 1e-05/3\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 2.247543, Privacy loss: 2.402, 1e-05/3\n",
            "\n",
            "Test set: Average loss: 0.4898, Accuracy: 8998/10000 (90%), Privacy: 2.403,1e-05/3\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.485819, Privacy loss: 2.403, 1e-05/3\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 1.871130, Privacy loss: 2.403, 1e-05/3\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 2.324017, Privacy loss: 2.404, 1e-05/3\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.668234, Privacy loss: 2.405, 1e-05/3\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 1.642119, Privacy loss: 2.406, 1e-05/3\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 1.857119, Privacy loss: 2.406, 1e-05/3\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 1.129574, Privacy loss: 2.407, 1e-05/3\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 2.017557, Privacy loss: 2.408, 1e-05/3\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 1.080068, Privacy loss: 2.409, 1e-05/3\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 1.048267, Privacy loss: 2.409, 1e-05/3\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.942003, Privacy loss: 2.41, 1e-05/3\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 1.310516, Privacy loss: 2.411, 1e-05/3\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.864920, Privacy loss: 2.412, 1e-05/3\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 2.513821, Privacy loss: 2.412, 1e-05/3\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 2.374945, Privacy loss: 2.413, 1e-05/3\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 1.010707, Privacy loss: 2.414, 1e-05/3\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.963456, Privacy loss: 2.415, 1e-05/3\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 1.355676, Privacy loss: 2.415, 1e-05/3\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.948276, Privacy loss: 2.416, 1e-05/3\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 1.269108, Privacy loss: 2.417, 1e-05/3\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.121181, Privacy loss: 2.418, 1e-05/3\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 2.293990, Privacy loss: 2.418, 1e-05/3\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 1.107914, Privacy loss: 2.419, 1e-05/3\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.980557, Privacy loss: 2.42, 1e-05/3\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 1.264768, Privacy loss: 2.421, 1e-05/3\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 1.646573, Privacy loss: 2.421, 1e-05/3\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 1.232986, Privacy loss: 2.422, 1e-05/3\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 1.489787, Privacy loss: 2.423, 1e-05/3\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 1.083486, Privacy loss: 2.424, 1e-05/3\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 1.422641, Privacy loss: 2.424, 1e-05/3\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 1.364314, Privacy loss: 2.425, 1e-05/3\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 1.307596, Privacy loss: 2.426, 1e-05/3\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 1.048593, Privacy loss: 2.427, 1e-05/3\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.932342, Privacy loss: 2.427, 1e-05/3\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.866168, Privacy loss: 2.428, 1e-05/3\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 2.080127, Privacy loss: 2.429, 1e-05/3\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 2.694259, Privacy loss: 2.43, 1e-05/3\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 1.353187, Privacy loss: 2.43, 1e-05/3\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.779788, Privacy loss: 2.431, 1e-05/3\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 1.421771, Privacy loss: 2.432, 1e-05/3\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.009760, Privacy loss: 2.433, 1e-05/3\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 1.198455, Privacy loss: 2.433, 1e-05/3\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.533571, Privacy loss: 2.434, 1e-05/3\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 2.440179, Privacy loss: 2.435, 1e-05/3\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.615465, Privacy loss: 2.436, 1e-05/3\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 1.561739, Privacy loss: 2.436, 1e-05/3\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 1.187092, Privacy loss: 2.437, 1e-05/3\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 1.153768, Privacy loss: 2.438, 1e-05/3\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 1.790074, Privacy loss: 2.439, 1e-05/3\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.650236, Privacy loss: 2.439, 1e-05/3\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.038688, Privacy loss: 2.44, 1e-05/3\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 1.293449, Privacy loss: 2.441, 1e-05/3\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 1.970376, Privacy loss: 2.442, 1e-05/3\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 1.793872, Privacy loss: 2.442, 1e-05/3\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 1.450019, Privacy loss: 2.443, 1e-05/3\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 1.999932, Privacy loss: 2.444, 1e-05/3\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 1.573956, Privacy loss: 2.445, 1e-05/3\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.895187, Privacy loss: 2.445, 1e-05/3\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 2.005345, Privacy loss: 2.446, 1e-05/3\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 1.556175, Privacy loss: 2.447, 1e-05/3\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.384050, Privacy loss: 2.448, 1e-05/3\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 1.226260, Privacy loss: 2.448, 1e-05/3\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.968911, Privacy loss: 2.449, 1e-05/3\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 2.359132, Privacy loss: 2.45, 1e-05/3\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.842636, Privacy loss: 2.451, 1e-05/3\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 1.241548, Privacy loss: 2.451, 1e-05/3\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.738930, Privacy loss: 2.452, 1e-05/3\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 1.047563, Privacy loss: 2.453, 1e-05/3\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 1.445717, Privacy loss: 2.454, 1e-05/3\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 1.511090, Privacy loss: 2.454, 1e-05/3\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 1.438644, Privacy loss: 2.455, 1e-05/3\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 1.349989, Privacy loss: 2.456, 1e-05/3\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 2.788392, Privacy loss: 2.456, 1e-05/3\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.751936, Privacy loss: 2.457, 1e-05/3\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 2.007833, Privacy loss: 2.458, 1e-05/3\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 1.696614, Privacy loss: 2.459, 1e-05/3\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 2.174698, Privacy loss: 2.459, 1e-05/3\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 2.304775, Privacy loss: 2.46, 1e-05/3\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 1.999161, Privacy loss: 2.461, 1e-05/3\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.753647, Privacy loss: 2.462, 1e-05/3\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.927294, Privacy loss: 2.462, 1e-05/3\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 1.474623, Privacy loss: 2.463, 1e-05/3\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 1.904051, Privacy loss: 2.464, 1e-05/3\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 1.097507, Privacy loss: 2.465, 1e-05/3\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 1.540104, Privacy loss: 2.465, 1e-05/3\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 1.323967, Privacy loss: 2.466, 1e-05/3\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 1.218098, Privacy loss: 2.467, 1e-05/3\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.845941, Privacy loss: 2.468, 1e-05/3\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 1.450238, Privacy loss: 2.468, 1e-05/3\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 1.937218, Privacy loss: 2.469, 1e-05/3\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 1.272786, Privacy loss: 2.47, 1e-05/3\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.886533, Privacy loss: 2.471, 1e-05/3\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 1.091927, Privacy loss: 2.471, 1e-05/3\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 2.176778, Privacy loss: 2.472, 1e-05/3\n",
            "\n",
            "Test set: Average loss: 0.4469, Accuracy: 9121/10000 (91%), Privacy: 2.473,1e-05/3\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.839384, Privacy loss: 2.473, 1e-05/3\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 1.222441, Privacy loss: 2.474, 1e-05/3\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 1.279946, Privacy loss: 2.474, 1e-05/3\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 1.302344, Privacy loss: 2.475, 1e-05/3\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 1.057548, Privacy loss: 2.476, 1e-05/3\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 1.951943, Privacy loss: 2.477, 1e-05/3\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 1.460731, Privacy loss: 2.477, 1e-05/3\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 1.399301, Privacy loss: 2.478, 1e-05/3\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 1.571671, Privacy loss: 2.479, 1e-05/3\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 1.556721, Privacy loss: 2.48, 1e-05/3\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.998065, Privacy loss: 2.48, 1e-05/3\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 1.210762, Privacy loss: 2.481, 1e-05/3\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.937530, Privacy loss: 2.482, 1e-05/3\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 1.164486, Privacy loss: 2.482, 1e-05/3\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 1.688080, Privacy loss: 2.483, 1e-05/3\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 1.511261, Privacy loss: 2.484, 1e-05/3\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 1.178598, Privacy loss: 2.485, 1e-05/3\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 1.464010, Privacy loss: 2.485, 1e-05/3\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 1.410591, Privacy loss: 2.486, 1e-05/3\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 1.223520, Privacy loss: 2.487, 1e-05/3\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.071118, Privacy loss: 2.488, 1e-05/3\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.733780, Privacy loss: 2.488, 1e-05/3\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 1.244318, Privacy loss: 2.489, 1e-05/3\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.760697, Privacy loss: 2.49, 1e-05/3\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 1.393976, Privacy loss: 2.491, 1e-05/3\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 1.373921, Privacy loss: 2.491, 1e-05/3\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 1.727957, Privacy loss: 2.492, 1e-05/3\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 1.820749, Privacy loss: 2.493, 1e-05/3\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 1.808063, Privacy loss: 2.494, 1e-05/3\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.918181, Privacy loss: 2.494, 1e-05/3\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.891815, Privacy loss: 2.495, 1e-05/3\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 1.362024, Privacy loss: 2.496, 1e-05/3\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 1.188407, Privacy loss: 2.497, 1e-05/3\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.666393, Privacy loss: 2.497, 1e-05/3\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.763387, Privacy loss: 2.498, 1e-05/3\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.992070, Privacy loss: 2.499, 1e-05/3\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 1.434460, Privacy loss: 2.5, 1e-05/3\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 1.199795, Privacy loss: 2.5, 1e-05/3\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 1.742007, Privacy loss: 2.501, 1e-05/3\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.852872, Privacy loss: 2.502, 1e-05/3\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.042499, Privacy loss: 2.503, 1e-05/3\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 1.301476, Privacy loss: 2.503, 1e-05/3\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 1.643478, Privacy loss: 2.504, 1e-05/3\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 2.158620, Privacy loss: 2.505, 1e-05/3\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 1.069043, Privacy loss: 2.506, 1e-05/3\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.883365, Privacy loss: 2.506, 1e-05/3\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.725445, Privacy loss: 2.507, 1e-05/3\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 1.587471, Privacy loss: 2.508, 1e-05/3\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.554752, Privacy loss: 2.509, 1e-05/3\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 1.574991, Privacy loss: 2.509, 1e-05/3\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.885272, Privacy loss: 2.51, 1e-05/3\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 1.507738, Privacy loss: 2.511, 1e-05/3\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 1.651997, Privacy loss: 2.512, 1e-05/3\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.859042, Privacy loss: 2.512, 1e-05/3\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 1.743848, Privacy loss: 2.513, 1e-05/3\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 1.256969, Privacy loss: 2.514, 1e-05/3\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 1.241968, Privacy loss: 2.515, 1e-05/3\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.584439, Privacy loss: 2.515, 1e-05/3\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 1.067822, Privacy loss: 2.516, 1e-05/3\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 1.658160, Privacy loss: 2.517, 1e-05/3\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.667810, Privacy loss: 2.518, 1e-05/3\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 1.170560, Privacy loss: 2.518, 1e-05/3\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 1.655263, Privacy loss: 2.519, 1e-05/3\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 1.397360, Privacy loss: 2.52, 1e-05/3\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 1.680979, Privacy loss: 2.521, 1e-05/3\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 1.365974, Privacy loss: 2.521, 1e-05/3\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 1.976015, Privacy loss: 2.522, 1e-05/3\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.881126, Privacy loss: 2.523, 1e-05/3\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 1.085432, Privacy loss: 2.524, 1e-05/3\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.953610, Privacy loss: 2.524, 1e-05/3\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.115840, Privacy loss: 2.525, 1e-05/3\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.494637, Privacy loss: 2.526, 1e-05/3\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 1.292722, Privacy loss: 2.527, 1e-05/3\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 1.187099, Privacy loss: 2.527, 1e-05/3\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 1.943123, Privacy loss: 2.528, 1e-05/3\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 1.421754, Privacy loss: 2.529, 1e-05/3\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 2.451895, Privacy loss: 2.53, 1e-05/3\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 1.159018, Privacy loss: 2.53, 1e-05/3\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 1.930289, Privacy loss: 2.531, 1e-05/3\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 1.389873, Privacy loss: 2.532, 1e-05/3\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.597949, Privacy loss: 2.533, 1e-05/3\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 1.124161, Privacy loss: 2.533, 1e-05/3\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 1.859193, Privacy loss: 2.534, 1e-05/3\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 1.476417, Privacy loss: 2.535, 1e-05/3\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 1.666676, Privacy loss: 2.536, 1e-05/3\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 1.106270, Privacy loss: 2.536, 1e-05/3\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 1.175377, Privacy loss: 2.537, 1e-05/3\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 1.163117, Privacy loss: 2.538, 1e-05/3\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 1.418181, Privacy loss: 2.538, 1e-05/3\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.912304, Privacy loss: 2.539, 1e-05/3\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 1.081378, Privacy loss: 2.539, 1e-05/3\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 1.619387, Privacy loss: 2.54, 1e-05/3\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 1.217167, Privacy loss: 2.541, 1e-05/3\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 1.074599, Privacy loss: 2.541, 1e-05/3\n",
            "\n",
            "Test set: Average loss: 0.4185, Accuracy: 9154/10000 (92%), Privacy: 2.542,1e-05/3\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.207936, Privacy loss: 2.542, 1e-05/3\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 1.424722, Privacy loss: 2.542, 1e-05/3\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 1.235547, Privacy loss: 2.543, 1e-05/3\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 2.160381, Privacy loss: 2.544, 1e-05/3\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 1.166089, Privacy loss: 2.544, 1e-05/3\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 1.077735, Privacy loss: 2.545, 1e-05/3\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 1.293597, Privacy loss: 2.545, 1e-05/3\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 1.887279, Privacy loss: 2.546, 1e-05/3\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.930528, Privacy loss: 2.547, 1e-05/3\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 1.390940, Privacy loss: 2.547, 1e-05/3\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 1.725606, Privacy loss: 2.548, 1e-05/3\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.425392, Privacy loss: 2.549, 1e-05/3\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 1.089944, Privacy loss: 2.549, 1e-05/3\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 1.284806, Privacy loss: 2.55, 1e-05/3\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 2.390635, Privacy loss: 2.55, 1e-05/3\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 2.875379, Privacy loss: 2.551, 1e-05/3\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.999382, Privacy loss: 2.552, 1e-05/3\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 1.282155, Privacy loss: 2.552, 1e-05/3\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 1.650229, Privacy loss: 2.553, 1e-05/3\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.896460, Privacy loss: 2.553, 1e-05/3\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 1.612080, Privacy loss: 2.554, 1e-05/3\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 1.148661, Privacy loss: 2.555, 1e-05/3\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 1.774428, Privacy loss: 2.555, 1e-05/3\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 1.658466, Privacy loss: 2.556, 1e-05/3\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 1.047244, Privacy loss: 2.557, 1e-05/3\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 1.572259, Privacy loss: 2.557, 1e-05/3\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 1.285387, Privacy loss: 2.558, 1e-05/3\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.678789, Privacy loss: 2.558, 1e-05/3\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 1.079793, Privacy loss: 2.559, 1e-05/3\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 1.595873, Privacy loss: 2.56, 1e-05/3\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 1.254449, Privacy loss: 2.56, 1e-05/3\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.635831, Privacy loss: 2.561, 1e-05/3\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 1.802429, Privacy loss: 2.561, 1e-05/3\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 1.159921, Privacy loss: 2.562, 1e-05/3\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 1.136746, Privacy loss: 2.563, 1e-05/3\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.921737, Privacy loss: 2.563, 1e-05/3\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 1.173395, Privacy loss: 2.564, 1e-05/3\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 1.240858, Privacy loss: 2.565, 1e-05/3\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.670613, Privacy loss: 2.565, 1e-05/3\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.944105, Privacy loss: 2.566, 1e-05/3\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.794772, Privacy loss: 2.566, 1e-05/3\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.974409, Privacy loss: 2.567, 1e-05/3\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.998134, Privacy loss: 2.568, 1e-05/3\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 1.300885, Privacy loss: 2.568, 1e-05/3\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 1.087285, Privacy loss: 2.569, 1e-05/3\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 1.262651, Privacy loss: 2.57, 1e-05/3\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 1.901342, Privacy loss: 2.57, 1e-05/3\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 1.532126, Privacy loss: 2.571, 1e-05/3\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 1.514481, Privacy loss: 2.571, 1e-05/3\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 1.811405, Privacy loss: 2.572, 1e-05/3\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.775134, Privacy loss: 2.573, 1e-05/3\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 2.319494, Privacy loss: 2.573, 1e-05/3\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 1.274818, Privacy loss: 2.574, 1e-05/3\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 1.721645, Privacy loss: 2.574, 1e-05/3\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 1.814984, Privacy loss: 2.575, 1e-05/3\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 1.801497, Privacy loss: 2.576, 1e-05/3\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 1.743265, Privacy loss: 2.576, 1e-05/3\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 1.197989, Privacy loss: 2.577, 1e-05/3\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 1.178026, Privacy loss: 2.578, 1e-05/3\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 1.095873, Privacy loss: 2.578, 1e-05/3\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 1.580880, Privacy loss: 2.579, 1e-05/3\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.866722, Privacy loss: 2.579, 1e-05/3\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 2.022836, Privacy loss: 2.58, 1e-05/3\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.962995, Privacy loss: 2.581, 1e-05/3\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 1.773136, Privacy loss: 2.581, 1e-05/3\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 1.216800, Privacy loss: 2.582, 1e-05/3\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.831619, Privacy loss: 2.582, 1e-05/3\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 1.199819, Privacy loss: 2.583, 1e-05/3\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 1.809967, Privacy loss: 2.584, 1e-05/3\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.700532, Privacy loss: 2.584, 1e-05/3\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 1.685734, Privacy loss: 2.585, 1e-05/3\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 1.266495, Privacy loss: 2.586, 1e-05/3\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 1.532127, Privacy loss: 2.586, 1e-05/3\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 1.058013, Privacy loss: 2.587, 1e-05/3\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 1.680445, Privacy loss: 2.587, 1e-05/3\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 1.307124, Privacy loss: 2.588, 1e-05/3\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 1.336214, Privacy loss: 2.589, 1e-05/3\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 1.350771, Privacy loss: 2.589, 1e-05/3\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 1.767795, Privacy loss: 2.59, 1e-05/3\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.966832, Privacy loss: 2.59, 1e-05/3\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 1.460699, Privacy loss: 2.591, 1e-05/3\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 2.157682, Privacy loss: 2.592, 1e-05/3\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.673093, Privacy loss: 2.592, 1e-05/3\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.955346, Privacy loss: 2.593, 1e-05/3\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 1.160308, Privacy loss: 2.594, 1e-05/3\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 2.215096, Privacy loss: 2.594, 1e-05/3\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 2.012629, Privacy loss: 2.595, 1e-05/3\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 1.126581, Privacy loss: 2.595, 1e-05/3\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.890445, Privacy loss: 2.596, 1e-05/3\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.796239, Privacy loss: 2.597, 1e-05/3\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 1.448906, Privacy loss: 2.597, 1e-05/3\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 1.205527, Privacy loss: 2.598, 1e-05/3\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 1.067274, Privacy loss: 2.598, 1e-05/3\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.810062, Privacy loss: 2.599, 1e-05/3\n",
            "\n",
            "Test set: Average loss: 0.4158, Accuracy: 9176/10000 (92%), Privacy: 2.6,1e-05/3\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.154085, Privacy loss: 2.6, 1e-05/3\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 1.761350, Privacy loss: 2.6, 1e-05/3\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.652684, Privacy loss: 2.601, 1e-05/3\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 1.274957, Privacy loss: 2.601, 1e-05/3\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 1.150216, Privacy loss: 2.602, 1e-05/3\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 1.600946, Privacy loss: 2.603, 1e-05/3\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.958924, Privacy loss: 2.603, 1e-05/3\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 1.656107, Privacy loss: 2.604, 1e-05/3\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 1.604397, Privacy loss: 2.605, 1e-05/3\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.424587, Privacy loss: 2.605, 1e-05/3\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.881608, Privacy loss: 2.606, 1e-05/3\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.997240, Privacy loss: 2.606, 1e-05/3\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 1.004686, Privacy loss: 2.607, 1e-05/3\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 1.262707, Privacy loss: 2.608, 1e-05/3\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.503176, Privacy loss: 2.608, 1e-05/3\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.451957, Privacy loss: 2.609, 1e-05/3\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 1.233646, Privacy loss: 2.609, 1e-05/3\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.755665, Privacy loss: 2.61, 1e-05/3\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 1.285373, Privacy loss: 2.611, 1e-05/3\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 1.267250, Privacy loss: 2.611, 1e-05/3\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.652678, Privacy loss: 2.612, 1e-05/3\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 1.831916, Privacy loss: 2.613, 1e-05/3\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 2.348169, Privacy loss: 2.613, 1e-05/3\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 1.844003, Privacy loss: 2.614, 1e-05/3\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.964435, Privacy loss: 2.614, 1e-05/3\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.865362, Privacy loss: 2.615, 1e-05/3\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.755646, Privacy loss: 2.616, 1e-05/3\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 1.461669, Privacy loss: 2.616, 1e-05/3\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.858411, Privacy loss: 2.617, 1e-05/3\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.968875, Privacy loss: 2.617, 1e-05/3\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 1.307410, Privacy loss: 2.618, 1e-05/3\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 1.686655, Privacy loss: 2.619, 1e-05/3\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.867869, Privacy loss: 2.619, 1e-05/3\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 1.157871, Privacy loss: 2.62, 1e-05/3\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 1.186294, Privacy loss: 2.621, 1e-05/3\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 2.142852, Privacy loss: 2.621, 1e-05/3\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 1.048623, Privacy loss: 2.622, 1e-05/3\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.381411, Privacy loss: 2.622, 1e-05/3\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 1.384362, Privacy loss: 2.623, 1e-05/3\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 1.866935, Privacy loss: 2.624, 1e-05/3\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.875723, Privacy loss: 2.624, 1e-05/3\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 1.131585, Privacy loss: 2.625, 1e-05/3\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 1.099688, Privacy loss: 2.625, 1e-05/3\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 1.019676, Privacy loss: 2.626, 1e-05/3\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 1.524355, Privacy loss: 2.627, 1e-05/3\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 1.470958, Privacy loss: 2.627, 1e-05/3\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.834513, Privacy loss: 2.628, 1e-05/3\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 1.859244, Privacy loss: 2.629, 1e-05/3\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.788772, Privacy loss: 2.629, 1e-05/3\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.983402, Privacy loss: 2.63, 1e-05/3\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.348733, Privacy loss: 2.63, 1e-05/3\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.998978, Privacy loss: 2.631, 1e-05/3\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.921467, Privacy loss: 2.632, 1e-05/3\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.475684, Privacy loss: 2.632, 1e-05/3\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 1.195752, Privacy loss: 2.633, 1e-05/3\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.709681, Privacy loss: 2.633, 1e-05/3\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 1.092428, Privacy loss: 2.634, 1e-05/3\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 1.139119, Privacy loss: 2.635, 1e-05/3\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.713005, Privacy loss: 2.635, 1e-05/3\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 1.256880, Privacy loss: 2.636, 1e-05/3\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.351469, Privacy loss: 2.637, 1e-05/3\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 1.602757, Privacy loss: 2.637, 1e-05/3\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 2.370981, Privacy loss: 2.638, 1e-05/3\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.569510, Privacy loss: 2.638, 1e-05/3\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.301912, Privacy loss: 2.639, 1e-05/3\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.702556, Privacy loss: 2.64, 1e-05/3\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.986884, Privacy loss: 2.64, 1e-05/3\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 1.291445, Privacy loss: 2.641, 1e-05/3\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 1.187337, Privacy loss: 2.641, 1e-05/3\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 1.636964, Privacy loss: 2.642, 1e-05/3\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 1.588505, Privacy loss: 2.643, 1e-05/3\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 2.491532, Privacy loss: 2.643, 1e-05/3\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 1.512738, Privacy loss: 2.644, 1e-05/3\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 1.419347, Privacy loss: 2.645, 1e-05/3\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.962237, Privacy loss: 2.645, 1e-05/3\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 1.123593, Privacy loss: 2.646, 1e-05/3\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 1.396395, Privacy loss: 2.646, 1e-05/3\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 1.537987, Privacy loss: 2.647, 1e-05/3\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.646227, Privacy loss: 2.648, 1e-05/3\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.703470, Privacy loss: 2.648, 1e-05/3\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.289075, Privacy loss: 2.649, 1e-05/3\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 1.428581, Privacy loss: 2.649, 1e-05/3\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 1.145642, Privacy loss: 2.65, 1e-05/3\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.967352, Privacy loss: 2.651, 1e-05/3\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 1.533361, Privacy loss: 2.651, 1e-05/3\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 1.148694, Privacy loss: 2.652, 1e-05/3\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 1.628370, Privacy loss: 2.653, 1e-05/3\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 1.410639, Privacy loss: 2.653, 1e-05/3\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.675216, Privacy loss: 2.654, 1e-05/3\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 1.199825, Privacy loss: 2.654, 1e-05/3\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 1.027041, Privacy loss: 2.655, 1e-05/3\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 1.462823, Privacy loss: 2.656, 1e-05/3\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 1.901689, Privacy loss: 2.656, 1e-05/3\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.855904, Privacy loss: 2.657, 1e-05/3\n",
            "\n",
            "Test set: Average loss: 0.4059, Accuracy: 9196/10000 (92%), Privacy: 2.657,1e-05/3\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.738117, Privacy loss: 2.657, 1e-05/3\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 1.285234, Privacy loss: 2.658, 1e-05/3\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.845190, Privacy loss: 2.659, 1e-05/3\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.802830, Privacy loss: 2.659, 1e-05/3\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 1.880046, Privacy loss: 2.66, 1e-05/3\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.869940, Privacy loss: 2.66, 1e-05/3\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 1.615079, Privacy loss: 2.661, 1e-05/3\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.582375, Privacy loss: 2.662, 1e-05/3\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 1.290524, Privacy loss: 2.662, 1e-05/3\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 1.105423, Privacy loss: 2.663, 1e-05/3\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 1.010404, Privacy loss: 2.664, 1e-05/3\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.988118, Privacy loss: 2.664, 1e-05/3\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.684366, Privacy loss: 2.665, 1e-05/3\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 1.136053, Privacy loss: 2.665, 1e-05/3\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.713356, Privacy loss: 2.666, 1e-05/3\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.474028, Privacy loss: 2.667, 1e-05/3\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 1.167060, Privacy loss: 2.667, 1e-05/3\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 1.275948, Privacy loss: 2.668, 1e-05/3\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 1.368092, Privacy loss: 2.668, 1e-05/3\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 1.187743, Privacy loss: 2.669, 1e-05/3\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 1.137629, Privacy loss: 2.67, 1e-05/3\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 1.107104, Privacy loss: 2.67, 1e-05/3\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.863086, Privacy loss: 2.671, 1e-05/3\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.752693, Privacy loss: 2.672, 1e-05/3\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.592968, Privacy loss: 2.672, 1e-05/3\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.864863, Privacy loss: 2.673, 1e-05/3\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.852766, Privacy loss: 2.673, 1e-05/3\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 1.493668, Privacy loss: 2.674, 1e-05/3\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 1.213164, Privacy loss: 2.675, 1e-05/3\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 1.031380, Privacy loss: 2.675, 1e-05/3\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 1.363052, Privacy loss: 2.676, 1e-05/3\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.933775, Privacy loss: 2.676, 1e-05/3\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 2.042008, Privacy loss: 2.677, 1e-05/3\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 1.131615, Privacy loss: 2.678, 1e-05/3\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 1.000527, Privacy loss: 2.678, 1e-05/3\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.562508, Privacy loss: 2.679, 1e-05/3\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 1.009085, Privacy loss: 2.68, 1e-05/3\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 1.344596, Privacy loss: 2.68, 1e-05/3\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.912974, Privacy loss: 2.681, 1e-05/3\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 1.472944, Privacy loss: 2.681, 1e-05/3\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.092066, Privacy loss: 2.682, 1e-05/3\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 1.608451, Privacy loss: 2.683, 1e-05/3\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.741628, Privacy loss: 2.683, 1e-05/3\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 1.827119, Privacy loss: 2.684, 1e-05/3\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 1.171810, Privacy loss: 2.684, 1e-05/3\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.762244, Privacy loss: 2.685, 1e-05/3\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 1.243861, Privacy loss: 2.686, 1e-05/3\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 1.450148, Privacy loss: 2.686, 1e-05/3\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.525366, Privacy loss: 2.687, 1e-05/3\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.486008, Privacy loss: 2.688, 1e-05/3\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.836384, Privacy loss: 2.688, 1e-05/3\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 1.401053, Privacy loss: 2.689, 1e-05/3\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.879305, Privacy loss: 2.689, 1e-05/3\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 2.083827, Privacy loss: 2.69, 1e-05/3\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.679183, Privacy loss: 2.691, 1e-05/3\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 1.209789, Privacy loss: 2.691, 1e-05/3\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 1.589632, Privacy loss: 2.692, 1e-05/3\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 1.435187, Privacy loss: 2.692, 1e-05/3\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.986872, Privacy loss: 2.693, 1e-05/3\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 2.030954, Privacy loss: 2.694, 1e-05/3\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.554658, Privacy loss: 2.694, 1e-05/3\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.688916, Privacy loss: 2.695, 1e-05/3\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 1.655262, Privacy loss: 2.696, 1e-05/3\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.930397, Privacy loss: 2.696, 1e-05/3\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 1.736295, Privacy loss: 2.697, 1e-05/3\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.833956, Privacy loss: 2.697, 1e-05/3\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 1.910825, Privacy loss: 2.698, 1e-05/3\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 1.703742, Privacy loss: 2.699, 1e-05/3\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 1.420975, Privacy loss: 2.699, 1e-05/3\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.919234, Privacy loss: 2.7, 1e-05/3\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.434126, Privacy loss: 2.701, 1e-05/3\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.929629, Privacy loss: 2.701, 1e-05/3\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 1.177464, Privacy loss: 2.702, 1e-05/3\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 1.136679, Privacy loss: 2.702, 1e-05/3\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 1.027313, Privacy loss: 2.703, 1e-05/3\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.719503, Privacy loss: 2.703, 1e-05/3\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 1.021120, Privacy loss: 2.704, 1e-05/3\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 1.676900, Privacy loss: 2.705, 1e-05/3\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.774953, Privacy loss: 2.705, 1e-05/3\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.822231, Privacy loss: 2.706, 1e-05/3\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 1.003102, Privacy loss: 2.706, 1e-05/3\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 1.442921, Privacy loss: 2.707, 1e-05/3\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.803245, Privacy loss: 2.707, 1e-05/3\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 1.096696, Privacy loss: 2.708, 1e-05/3\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 1.143255, Privacy loss: 2.708, 1e-05/3\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.904935, Privacy loss: 2.709, 1e-05/3\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.707174, Privacy loss: 2.709, 1e-05/3\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.970134, Privacy loss: 2.71, 1e-05/3\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.343870, Privacy loss: 2.71, 1e-05/3\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 1.139804, Privacy loss: 2.711, 1e-05/3\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 1.241676, Privacy loss: 2.711, 1e-05/3\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 1.034892, Privacy loss: 2.712, 1e-05/3\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.928240, Privacy loss: 2.712, 1e-05/3\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 1.160152, Privacy loss: 2.713, 1e-05/3\n",
            "\n",
            "Test set: Average loss: 0.4002, Accuracy: 9209/10000 (92%), Privacy: 2.713,1e-05/3\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.592001, Privacy loss: 2.713, 1e-05/3\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: 1.312070, Privacy loss: 2.714, 1e-05/3\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 1.203812, Privacy loss: 2.714, 1e-05/3\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 2.000366, Privacy loss: 2.715, 1e-05/3\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.936434, Privacy loss: 2.716, 1e-05/3\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.730077, Privacy loss: 2.716, 1e-05/3\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.647898, Privacy loss: 2.717, 1e-05/3\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.510212, Privacy loss: 2.717, 1e-05/3\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 1.009099, Privacy loss: 2.718, 1e-05/3\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.562147, Privacy loss: 2.718, 1e-05/3\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 1.368404, Privacy loss: 2.719, 1e-05/3\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.847367, Privacy loss: 2.719, 1e-05/3\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 1.016315, Privacy loss: 2.72, 1e-05/3\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.579893, Privacy loss: 2.72, 1e-05/3\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 2.047020, Privacy loss: 2.721, 1e-05/3\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 1.291953, Privacy loss: 2.721, 1e-05/3\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.784005, Privacy loss: 2.722, 1e-05/3\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 1.258251, Privacy loss: 2.722, 1e-05/3\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.957952, Privacy loss: 2.723, 1e-05/3\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 1.349452, Privacy loss: 2.723, 1e-05/3\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.839617, Privacy loss: 2.724, 1e-05/3\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 1.894641, Privacy loss: 2.725, 1e-05/3\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 1.466667, Privacy loss: 2.725, 1e-05/3\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 1.421345, Privacy loss: 2.726, 1e-05/3\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 1.493971, Privacy loss: 2.726, 1e-05/3\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.724580, Privacy loss: 2.727, 1e-05/3\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 1.237307, Privacy loss: 2.727, 1e-05/3\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 1.373071, Privacy loss: 2.728, 1e-05/3\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.550968, Privacy loss: 2.728, 1e-05/3\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 1.500698, Privacy loss: 2.729, 1e-05/3\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.983068, Privacy loss: 2.729, 1e-05/3\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 1.476879, Privacy loss: 2.73, 1e-05/3\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.918629, Privacy loss: 2.73, 1e-05/3\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 1.071330, Privacy loss: 2.731, 1e-05/3\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 1.824814, Privacy loss: 2.731, 1e-05/3\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.725303, Privacy loss: 2.732, 1e-05/3\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.802602, Privacy loss: 2.732, 1e-05/3\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.858300, Privacy loss: 2.733, 1e-05/3\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 1.540573, Privacy loss: 2.734, 1e-05/3\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 1.182538, Privacy loss: 2.734, 1e-05/3\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.834796, Privacy loss: 2.735, 1e-05/3\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 2.110224, Privacy loss: 2.735, 1e-05/3\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 1.239990, Privacy loss: 2.736, 1e-05/3\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 1.440100, Privacy loss: 2.736, 1e-05/3\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.798792, Privacy loss: 2.737, 1e-05/3\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 1.531481, Privacy loss: 2.737, 1e-05/3\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 1.326820, Privacy loss: 2.738, 1e-05/3\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.981104, Privacy loss: 2.738, 1e-05/3\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.748184, Privacy loss: 2.739, 1e-05/3\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.985239, Privacy loss: 2.739, 1e-05/3\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.444260, Privacy loss: 2.74, 1e-05/3\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 1.605108, Privacy loss: 2.74, 1e-05/3\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 1.180545, Privacy loss: 2.741, 1e-05/3\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.723210, Privacy loss: 2.741, 1e-05/3\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 1.213297, Privacy loss: 2.742, 1e-05/3\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.688784, Privacy loss: 2.742, 1e-05/3\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 1.373302, Privacy loss: 2.743, 1e-05/3\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.741454, Privacy loss: 2.744, 1e-05/3\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.978260, Privacy loss: 2.744, 1e-05/3\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 1.640171, Privacy loss: 2.745, 1e-05/3\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 1.291701, Privacy loss: 2.745, 1e-05/3\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 1.506710, Privacy loss: 2.746, 1e-05/3\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 1.155625, Privacy loss: 2.746, 1e-05/3\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 1.386998, Privacy loss: 2.747, 1e-05/3\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 1.848888, Privacy loss: 2.747, 1e-05/3\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 1.229837, Privacy loss: 2.748, 1e-05/3\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 1.643655, Privacy loss: 2.748, 1e-05/3\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 1.454602, Privacy loss: 2.749, 1e-05/3\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.567983, Privacy loss: 2.749, 1e-05/3\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.373576, Privacy loss: 2.75, 1e-05/3\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.724305, Privacy loss: 2.75, 1e-05/3\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.753348, Privacy loss: 2.751, 1e-05/3\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 1.557400, Privacy loss: 2.751, 1e-05/3\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 1.376429, Privacy loss: 2.752, 1e-05/3\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 1.144074, Privacy loss: 2.753, 1e-05/3\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 1.493273, Privacy loss: 2.753, 1e-05/3\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 1.916603, Privacy loss: 2.754, 1e-05/3\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 1.126909, Privacy loss: 2.754, 1e-05/3\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 1.821411, Privacy loss: 2.755, 1e-05/3\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 1.489741, Privacy loss: 2.755, 1e-05/3\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.683088, Privacy loss: 2.756, 1e-05/3\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.762855, Privacy loss: 2.756, 1e-05/3\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 1.535665, Privacy loss: 2.757, 1e-05/3\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 2.071948, Privacy loss: 2.757, 1e-05/3\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.930709, Privacy loss: 2.758, 1e-05/3\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.648634, Privacy loss: 2.758, 1e-05/3\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.952012, Privacy loss: 2.759, 1e-05/3\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 1.084608, Privacy loss: 2.759, 1e-05/3\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.564593, Privacy loss: 2.76, 1e-05/3\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.337275, Privacy loss: 2.76, 1e-05/3\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 1.145268, Privacy loss: 2.761, 1e-05/3\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.772863, Privacy loss: 2.762, 1e-05/3\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 1.201874, Privacy loss: 2.762, 1e-05/3\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.641755, Privacy loss: 2.763, 1e-05/3\n",
            "\n",
            "Test set: Average loss: 0.4039, Accuracy: 9225/10000 (92%), Privacy: 2.763,1e-05/3\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.651470, Privacy loss: 2.763, 1e-05/3\n",
            "Train Epoch: 10 [640/60000 (1%)]\tLoss: 1.051963, Privacy loss: 2.764, 1e-05/3\n",
            "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 1.614108, Privacy loss: 2.764, 1e-05/3\n",
            "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 1.255461, Privacy loss: 2.765, 1e-05/3\n",
            "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.615162, Privacy loss: 2.765, 1e-05/3\n",
            "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 2.584935, Privacy loss: 2.766, 1e-05/3\n",
            "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 1.125661, Privacy loss: 2.766, 1e-05/3\n",
            "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.323301, Privacy loss: 2.767, 1e-05/3\n",
            "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.653388, Privacy loss: 2.767, 1e-05/3\n",
            "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 1.751331, Privacy loss: 2.768, 1e-05/3\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 1.465929, Privacy loss: 2.768, 1e-05/3\n",
            "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.757248, Privacy loss: 2.769, 1e-05/3\n",
            "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 1.324653, Privacy loss: 2.769, 1e-05/3\n",
            "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.794054, Privacy loss: 2.77, 1e-05/3\n",
            "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 1.362322, Privacy loss: 2.77, 1e-05/3\n",
            "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 1.912261, Privacy loss: 2.771, 1e-05/3\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 1.873365, Privacy loss: 2.771, 1e-05/3\n",
            "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.447342, Privacy loss: 2.772, 1e-05/3\n",
            "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.556180, Privacy loss: 2.773, 1e-05/3\n",
            "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.483963, Privacy loss: 2.773, 1e-05/3\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.653460, Privacy loss: 2.774, 1e-05/3\n",
            "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.835062, Privacy loss: 2.774, 1e-05/3\n",
            "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.382460, Privacy loss: 2.775, 1e-05/3\n",
            "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.843118, Privacy loss: 2.775, 1e-05/3\n",
            "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.309861, Privacy loss: 2.776, 1e-05/3\n",
            "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.913578, Privacy loss: 2.776, 1e-05/3\n",
            "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 1.221843, Privacy loss: 2.777, 1e-05/3\n",
            "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 1.454789, Privacy loss: 2.777, 1e-05/3\n",
            "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 2.116248, Privacy loss: 2.778, 1e-05/3\n",
            "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 1.004176, Privacy loss: 2.778, 1e-05/3\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 1.211897, Privacy loss: 2.779, 1e-05/3\n",
            "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.832751, Privacy loss: 2.779, 1e-05/3\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.647299, Privacy loss: 2.78, 1e-05/3\n",
            "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 1.391915, Privacy loss: 2.78, 1e-05/3\n",
            "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.943644, Privacy loss: 2.781, 1e-05/3\n",
            "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 1.240994, Privacy loss: 2.782, 1e-05/3\n",
            "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 2.081795, Privacy loss: 2.782, 1e-05/3\n",
            "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 1.562737, Privacy loss: 2.783, 1e-05/3\n",
            "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.900271, Privacy loss: 2.783, 1e-05/3\n",
            "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.784307, Privacy loss: 2.784, 1e-05/3\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 1.239977, Privacy loss: 2.784, 1e-05/3\n",
            "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 1.567546, Privacy loss: 2.785, 1e-05/3\n",
            "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 1.967544, Privacy loss: 2.785, 1e-05/3\n",
            "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 1.084500, Privacy loss: 2.786, 1e-05/3\n",
            "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.882946, Privacy loss: 2.786, 1e-05/3\n",
            "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 1.093820, Privacy loss: 2.787, 1e-05/3\n",
            "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 1.781095, Privacy loss: 2.787, 1e-05/3\n",
            "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.919462, Privacy loss: 2.788, 1e-05/3\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.652345, Privacy loss: 2.788, 1e-05/3\n",
            "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 1.185318, Privacy loss: 2.789, 1e-05/3\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.538894, Privacy loss: 2.789, 1e-05/3\n",
            "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 1.698069, Privacy loss: 2.79, 1e-05/3\n",
            "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.869358, Privacy loss: 2.791, 1e-05/3\n",
            "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.570316, Privacy loss: 2.791, 1e-05/3\n",
            "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.910239, Privacy loss: 2.792, 1e-05/3\n",
            "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 1.491900, Privacy loss: 2.792, 1e-05/3\n",
            "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 1.045205, Privacy loss: 2.793, 1e-05/3\n",
            "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.865418, Privacy loss: 2.793, 1e-05/3\n",
            "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.869664, Privacy loss: 2.794, 1e-05/3\n",
            "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 2.239907, Privacy loss: 2.794, 1e-05/3\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.717489, Privacy loss: 2.795, 1e-05/3\n",
            "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.962571, Privacy loss: 2.795, 1e-05/3\n",
            "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 1.580211, Privacy loss: 2.796, 1e-05/3\n",
            "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 1.032341, Privacy loss: 2.796, 1e-05/3\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.942908, Privacy loss: 2.797, 1e-05/3\n",
            "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.704531, Privacy loss: 2.797, 1e-05/3\n",
            "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 1.073299, Privacy loss: 2.798, 1e-05/3\n",
            "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.723423, Privacy loss: 2.798, 1e-05/3\n",
            "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 1.303082, Privacy loss: 2.799, 1e-05/3\n",
            "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.812386, Privacy loss: 2.8, 1e-05/3\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.904063, Privacy loss: 2.8, 1e-05/3\n",
            "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 1.010932, Privacy loss: 2.801, 1e-05/3\n",
            "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.570101, Privacy loss: 2.801, 1e-05/3\n",
            "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 1.804711, Privacy loss: 2.802, 1e-05/3\n",
            "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.722344, Privacy loss: 2.802, 1e-05/3\n",
            "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.957325, Privacy loss: 2.803, 1e-05/3\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.916417, Privacy loss: 2.803, 1e-05/3\n",
            "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.834895, Privacy loss: 2.804, 1e-05/3\n",
            "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.872562, Privacy loss: 2.804, 1e-05/3\n",
            "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.763338, Privacy loss: 2.805, 1e-05/3\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.532176, Privacy loss: 2.805, 1e-05/3\n",
            "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 1.276182, Privacy loss: 2.806, 1e-05/3\n",
            "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 1.367202, Privacy loss: 2.806, 1e-05/3\n",
            "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.898625, Privacy loss: 2.807, 1e-05/3\n",
            "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.725621, Privacy loss: 2.807, 1e-05/3\n",
            "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 1.691795, Privacy loss: 2.808, 1e-05/3\n",
            "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 1.480152, Privacy loss: 2.809, 1e-05/3\n",
            "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.969175, Privacy loss: 2.809, 1e-05/3\n",
            "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 1.628295, Privacy loss: 2.81, 1e-05/3\n",
            "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 1.084473, Privacy loss: 2.81, 1e-05/3\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 1.271469, Privacy loss: 2.811, 1e-05/3\n",
            "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 1.322618, Privacy loss: 2.811, 1e-05/3\n",
            "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 1.111356, Privacy loss: 2.812, 1e-05/3\n",
            "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 1.125595, Privacy loss: 2.812, 1e-05/3\n",
            "\n",
            "Test set: Average loss: 0.4044, Accuracy: 9217/10000 (92%), Privacy: 2.813,1e-05/3\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.507120, Privacy loss: 2.813, 1e-05/3\n",
            "Train Epoch: 11 [640/60000 (1%)]\tLoss: 1.263702, Privacy loss: 2.813, 1e-05/3\n",
            "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 1.401620, Privacy loss: 2.814, 1e-05/3\n",
            "Train Epoch: 11 [1920/60000 (3%)]\tLoss: 0.460663, Privacy loss: 2.814, 1e-05/3\n",
            "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 1.576690, Privacy loss: 2.815, 1e-05/3\n",
            "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 1.299986, Privacy loss: 2.815, 1e-05/3\n",
            "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 1.192612, Privacy loss: 2.816, 1e-05/3\n",
            "Train Epoch: 11 [4480/60000 (7%)]\tLoss: 1.407956, Privacy loss: 2.816, 1e-05/3\n",
            "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.666675, Privacy loss: 2.817, 1e-05/3\n",
            "Train Epoch: 11 [5760/60000 (10%)]\tLoss: 1.553981, Privacy loss: 2.817, 1e-05/3\n",
            "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 1.203995, Privacy loss: 2.818, 1e-05/3\n",
            "Train Epoch: 11 [7040/60000 (12%)]\tLoss: 1.327729, Privacy loss: 2.818, 1e-05/3\n",
            "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 1.064735, Privacy loss: 2.819, 1e-05/3\n",
            "Train Epoch: 11 [8320/60000 (14%)]\tLoss: 0.393238, Privacy loss: 2.82, 1e-05/3\n",
            "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 0.559364, Privacy loss: 2.82, 1e-05/3\n",
            "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 0.777350, Privacy loss: 2.821, 1e-05/3\n",
            "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 1.230600, Privacy loss: 2.821, 1e-05/3\n",
            "Train Epoch: 11 [10880/60000 (18%)]\tLoss: 1.115595, Privacy loss: 2.822, 1e-05/3\n",
            "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 1.667055, Privacy loss: 2.822, 1e-05/3\n",
            "Train Epoch: 11 [12160/60000 (20%)]\tLoss: 0.808753, Privacy loss: 2.823, 1e-05/3\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 1.253132, Privacy loss: 2.823, 1e-05/3\n",
            "Train Epoch: 11 [13440/60000 (22%)]\tLoss: 0.354698, Privacy loss: 2.824, 1e-05/3\n",
            "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 0.718689, Privacy loss: 2.824, 1e-05/3\n",
            "Train Epoch: 11 [14720/60000 (25%)]\tLoss: 0.928878, Privacy loss: 2.825, 1e-05/3\n",
            "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 1.078822, Privacy loss: 2.825, 1e-05/3\n",
            "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 1.525514, Privacy loss: 2.826, 1e-05/3\n",
            "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 1.375270, Privacy loss: 2.826, 1e-05/3\n",
            "Train Epoch: 11 [17280/60000 (29%)]\tLoss: 0.977804, Privacy loss: 2.827, 1e-05/3\n",
            "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 1.478253, Privacy loss: 2.827, 1e-05/3\n",
            "Train Epoch: 11 [18560/60000 (31%)]\tLoss: 1.321142, Privacy loss: 2.828, 1e-05/3\n",
            "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 2.088187, Privacy loss: 2.829, 1e-05/3\n",
            "Train Epoch: 11 [19840/60000 (33%)]\tLoss: 1.325332, Privacy loss: 2.829, 1e-05/3\n",
            "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.901809, Privacy loss: 2.83, 1e-05/3\n",
            "Train Epoch: 11 [21120/60000 (35%)]\tLoss: 1.701155, Privacy loss: 2.83, 1e-05/3\n",
            "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 1.034443, Privacy loss: 2.831, 1e-05/3\n",
            "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 1.348366, Privacy loss: 2.831, 1e-05/3\n",
            "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 1.259513, Privacy loss: 2.832, 1e-05/3\n",
            "Train Epoch: 11 [23680/60000 (39%)]\tLoss: 0.326116, Privacy loss: 2.832, 1e-05/3\n",
            "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 0.686141, Privacy loss: 2.833, 1e-05/3\n",
            "Train Epoch: 11 [24960/60000 (42%)]\tLoss: 1.205932, Privacy loss: 2.833, 1e-05/3\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 1.671249, Privacy loss: 2.834, 1e-05/3\n",
            "Train Epoch: 11 [26240/60000 (44%)]\tLoss: 1.737179, Privacy loss: 2.834, 1e-05/3\n",
            "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 0.762693, Privacy loss: 2.835, 1e-05/3\n",
            "Train Epoch: 11 [27520/60000 (46%)]\tLoss: 1.774974, Privacy loss: 2.835, 1e-05/3\n",
            "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 1.264986, Privacy loss: 2.836, 1e-05/3\n",
            "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 0.829503, Privacy loss: 2.836, 1e-05/3\n",
            "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 1.249264, Privacy loss: 2.837, 1e-05/3\n",
            "Train Epoch: 11 [30080/60000 (50%)]\tLoss: 1.713977, Privacy loss: 2.838, 1e-05/3\n",
            "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.783921, Privacy loss: 2.838, 1e-05/3\n",
            "Train Epoch: 11 [31360/60000 (52%)]\tLoss: 0.700384, Privacy loss: 2.839, 1e-05/3\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 1.053086, Privacy loss: 2.839, 1e-05/3\n",
            "Train Epoch: 11 [32640/60000 (54%)]\tLoss: 1.716959, Privacy loss: 2.84, 1e-05/3\n",
            "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 1.132167, Privacy loss: 2.84, 1e-05/3\n",
            "Train Epoch: 11 [33920/60000 (57%)]\tLoss: 0.674028, Privacy loss: 2.841, 1e-05/3\n",
            "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 1.431007, Privacy loss: 2.841, 1e-05/3\n",
            "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 0.773034, Privacy loss: 2.842, 1e-05/3\n",
            "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 1.601200, Privacy loss: 2.842, 1e-05/3\n",
            "Train Epoch: 11 [36480/60000 (61%)]\tLoss: 0.487719, Privacy loss: 2.843, 1e-05/3\n",
            "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 1.084899, Privacy loss: 2.843, 1e-05/3\n",
            "Train Epoch: 11 [37760/60000 (63%)]\tLoss: 0.652317, Privacy loss: 2.844, 1e-05/3\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.875519, Privacy loss: 2.844, 1e-05/3\n",
            "Train Epoch: 11 [39040/60000 (65%)]\tLoss: 0.591750, Privacy loss: 2.845, 1e-05/3\n",
            "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 1.059426, Privacy loss: 2.845, 1e-05/3\n",
            "Train Epoch: 11 [40320/60000 (67%)]\tLoss: 1.309988, Privacy loss: 2.846, 1e-05/3\n",
            "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 1.008411, Privacy loss: 2.847, 1e-05/3\n",
            "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 0.983929, Privacy loss: 2.847, 1e-05/3\n",
            "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 1.382990, Privacy loss: 2.848, 1e-05/3\n",
            "Train Epoch: 11 [42880/60000 (71%)]\tLoss: 0.973098, Privacy loss: 2.848, 1e-05/3\n",
            "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.975824, Privacy loss: 2.849, 1e-05/3\n",
            "Train Epoch: 11 [44160/60000 (74%)]\tLoss: 0.583864, Privacy loss: 2.849, 1e-05/3\n",
            "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.707155, Privacy loss: 2.85, 1e-05/3\n",
            "Train Epoch: 11 [45440/60000 (76%)]\tLoss: 1.543882, Privacy loss: 2.85, 1e-05/3\n",
            "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.984628, Privacy loss: 2.851, 1e-05/3\n",
            "Train Epoch: 11 [46720/60000 (78%)]\tLoss: 1.460484, Privacy loss: 2.851, 1e-05/3\n",
            "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 2.395845, Privacy loss: 2.852, 1e-05/3\n",
            "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 1.246544, Privacy loss: 2.852, 1e-05/3\n",
            "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 1.067733, Privacy loss: 2.853, 1e-05/3\n",
            "Train Epoch: 11 [49280/60000 (82%)]\tLoss: 1.439984, Privacy loss: 2.853, 1e-05/3\n",
            "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 1.199230, Privacy loss: 2.854, 1e-05/3\n",
            "Train Epoch: 11 [50560/60000 (84%)]\tLoss: 0.428171, Privacy loss: 2.854, 1e-05/3\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.395540, Privacy loss: 2.855, 1e-05/3\n",
            "Train Epoch: 11 [51840/60000 (86%)]\tLoss: 1.079947, Privacy loss: 2.856, 1e-05/3\n",
            "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 1.655407, Privacy loss: 2.856, 1e-05/3\n",
            "Train Epoch: 11 [53120/60000 (88%)]\tLoss: 0.593959, Privacy loss: 2.857, 1e-05/3\n",
            "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 1.192518, Privacy loss: 2.857, 1e-05/3\n",
            "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 1.878276, Privacy loss: 2.858, 1e-05/3\n",
            "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 0.731773, Privacy loss: 2.858, 1e-05/3\n",
            "Train Epoch: 11 [55680/60000 (93%)]\tLoss: 1.091960, Privacy loss: 2.859, 1e-05/3\n",
            "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.659223, Privacy loss: 2.859, 1e-05/3\n",
            "Train Epoch: 11 [56960/60000 (95%)]\tLoss: 1.270000, Privacy loss: 2.86, 1e-05/3\n",
            "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 1.413036, Privacy loss: 2.86, 1e-05/3\n",
            "Train Epoch: 11 [58240/60000 (97%)]\tLoss: 0.831449, Privacy loss: 2.861, 1e-05/3\n",
            "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.829303, Privacy loss: 2.861, 1e-05/3\n",
            "Train Epoch: 11 [59520/60000 (99%)]\tLoss: 0.804533, Privacy loss: 2.862, 1e-05/3\n",
            "\n",
            "Test set: Average loss: 0.4009, Accuracy: 9231/10000 (92%), Privacy: 2.862,1e-05/3\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.712891, Privacy loss: 2.862, 1e-05/3\n",
            "Train Epoch: 12 [640/60000 (1%)]\tLoss: 0.601131, Privacy loss: 2.863, 1e-05/3\n",
            "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 0.888043, Privacy loss: 2.863, 1e-05/3\n",
            "Train Epoch: 12 [1920/60000 (3%)]\tLoss: 0.764127, Privacy loss: 2.864, 1e-05/3\n",
            "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.862756, Privacy loss: 2.864, 1e-05/3\n",
            "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 1.014203, Privacy loss: 2.865, 1e-05/3\n",
            "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 0.575661, Privacy loss: 2.865, 1e-05/3\n",
            "Train Epoch: 12 [4480/60000 (7%)]\tLoss: 0.562639, Privacy loss: 2.866, 1e-05/3\n",
            "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.911738, Privacy loss: 2.867, 1e-05/3\n",
            "Train Epoch: 12 [5760/60000 (10%)]\tLoss: 1.641941, Privacy loss: 2.867, 1e-05/3\n",
            "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 1.676984, Privacy loss: 2.868, 1e-05/3\n",
            "Train Epoch: 12 [7040/60000 (12%)]\tLoss: 0.897174, Privacy loss: 2.868, 1e-05/3\n",
            "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.598370, Privacy loss: 2.869, 1e-05/3\n",
            "Train Epoch: 12 [8320/60000 (14%)]\tLoss: 1.107469, Privacy loss: 2.869, 1e-05/3\n",
            "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 1.417882, Privacy loss: 2.87, 1e-05/3\n",
            "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 0.785160, Privacy loss: 2.87, 1e-05/3\n",
            "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 1.340590, Privacy loss: 2.871, 1e-05/3\n",
            "Train Epoch: 12 [10880/60000 (18%)]\tLoss: 1.017854, Privacy loss: 2.871, 1e-05/3\n",
            "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 1.171283, Privacy loss: 2.872, 1e-05/3\n",
            "Train Epoch: 12 [12160/60000 (20%)]\tLoss: 1.003439, Privacy loss: 2.872, 1e-05/3\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.848000, Privacy loss: 2.873, 1e-05/3\n",
            "Train Epoch: 12 [13440/60000 (22%)]\tLoss: 0.681750, Privacy loss: 2.873, 1e-05/3\n",
            "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 1.147790, Privacy loss: 2.874, 1e-05/3\n",
            "Train Epoch: 12 [14720/60000 (25%)]\tLoss: 0.924923, Privacy loss: 2.874, 1e-05/3\n",
            "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 1.087869, Privacy loss: 2.875, 1e-05/3\n",
            "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 2.375350, Privacy loss: 2.876, 1e-05/3\n",
            "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 1.295287, Privacy loss: 2.876, 1e-05/3\n",
            "Train Epoch: 12 [17280/60000 (29%)]\tLoss: 1.860430, Privacy loss: 2.877, 1e-05/3\n",
            "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.339155, Privacy loss: 2.877, 1e-05/3\n",
            "Train Epoch: 12 [18560/60000 (31%)]\tLoss: 1.427487, Privacy loss: 2.878, 1e-05/3\n",
            "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.732830, Privacy loss: 2.878, 1e-05/3\n",
            "Train Epoch: 12 [19840/60000 (33%)]\tLoss: 2.813008, Privacy loss: 2.879, 1e-05/3\n",
            "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 1.778711, Privacy loss: 2.879, 1e-05/3\n",
            "Train Epoch: 12 [21120/60000 (35%)]\tLoss: 1.240759, Privacy loss: 2.88, 1e-05/3\n",
            "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 0.973285, Privacy loss: 2.88, 1e-05/3\n",
            "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 0.999254, Privacy loss: 2.881, 1e-05/3\n",
            "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 1.120710, Privacy loss: 2.881, 1e-05/3\n",
            "Train Epoch: 12 [23680/60000 (39%)]\tLoss: 1.339786, Privacy loss: 2.882, 1e-05/3\n",
            "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 0.799506, Privacy loss: 2.882, 1e-05/3\n",
            "Train Epoch: 12 [24960/60000 (42%)]\tLoss: 0.794371, Privacy loss: 2.883, 1e-05/3\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 1.095295, Privacy loss: 2.883, 1e-05/3\n",
            "Train Epoch: 12 [26240/60000 (44%)]\tLoss: 0.821032, Privacy loss: 2.884, 1e-05/3\n",
            "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 0.959414, Privacy loss: 2.885, 1e-05/3\n",
            "Train Epoch: 12 [27520/60000 (46%)]\tLoss: 0.755853, Privacy loss: 2.885, 1e-05/3\n",
            "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.624962, Privacy loss: 2.886, 1e-05/3\n",
            "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 0.804389, Privacy loss: 2.886, 1e-05/3\n",
            "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 1.379061, Privacy loss: 2.887, 1e-05/3\n",
            "Train Epoch: 12 [30080/60000 (50%)]\tLoss: 1.510494, Privacy loss: 2.887, 1e-05/3\n",
            "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 1.611328, Privacy loss: 2.888, 1e-05/3\n",
            "Train Epoch: 12 [31360/60000 (52%)]\tLoss: 1.107668, Privacy loss: 2.888, 1e-05/3\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.786039, Privacy loss: 2.889, 1e-05/3\n",
            "Train Epoch: 12 [32640/60000 (54%)]\tLoss: 1.634946, Privacy loss: 2.889, 1e-05/3\n",
            "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 1.810237, Privacy loss: 2.89, 1e-05/3\n",
            "Train Epoch: 12 [33920/60000 (57%)]\tLoss: 1.540206, Privacy loss: 2.89, 1e-05/3\n",
            "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 1.041605, Privacy loss: 2.891, 1e-05/3\n",
            "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 0.935406, Privacy loss: 2.891, 1e-05/3\n",
            "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 1.106323, Privacy loss: 2.892, 1e-05/3\n",
            "Train Epoch: 12 [36480/60000 (61%)]\tLoss: 1.955296, Privacy loss: 2.892, 1e-05/3\n",
            "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 0.169011, Privacy loss: 2.893, 1e-05/3\n",
            "Train Epoch: 12 [37760/60000 (63%)]\tLoss: 0.841905, Privacy loss: 2.894, 1e-05/3\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.631220, Privacy loss: 2.894, 1e-05/3\n",
            "Train Epoch: 12 [39040/60000 (65%)]\tLoss: 0.782398, Privacy loss: 2.895, 1e-05/3\n",
            "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 1.617813, Privacy loss: 2.895, 1e-05/3\n",
            "Train Epoch: 12 [40320/60000 (67%)]\tLoss: 0.673288, Privacy loss: 2.896, 1e-05/3\n",
            "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.647972, Privacy loss: 2.896, 1e-05/3\n",
            "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 0.765301, Privacy loss: 2.897, 1e-05/3\n",
            "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 0.913179, Privacy loss: 2.897, 1e-05/3\n",
            "Train Epoch: 12 [42880/60000 (71%)]\tLoss: 0.913611, Privacy loss: 2.898, 1e-05/3\n",
            "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 1.094220, Privacy loss: 2.898, 1e-05/3\n",
            "Train Epoch: 12 [44160/60000 (74%)]\tLoss: 1.577315, Privacy loss: 2.899, 1e-05/3\n",
            "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 1.768563, Privacy loss: 2.899, 1e-05/3\n",
            "Train Epoch: 12 [45440/60000 (76%)]\tLoss: 1.403671, Privacy loss: 2.9, 1e-05/3\n",
            "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 1.095350, Privacy loss: 2.9, 1e-05/3\n",
            "Train Epoch: 12 [46720/60000 (78%)]\tLoss: 1.775153, Privacy loss: 2.901, 1e-05/3\n",
            "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 1.540851, Privacy loss: 2.901, 1e-05/3\n",
            "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 1.448484, Privacy loss: 2.902, 1e-05/3\n",
            "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 2.139260, Privacy loss: 2.903, 1e-05/3\n",
            "Train Epoch: 12 [49280/60000 (82%)]\tLoss: 1.178121, Privacy loss: 2.903, 1e-05/3\n",
            "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 1.075150, Privacy loss: 2.904, 1e-05/3\n",
            "Train Epoch: 12 [50560/60000 (84%)]\tLoss: 1.015584, Privacy loss: 2.904, 1e-05/3\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.652348, Privacy loss: 2.905, 1e-05/3\n",
            "Train Epoch: 12 [51840/60000 (86%)]\tLoss: 1.011085, Privacy loss: 2.905, 1e-05/3\n",
            "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 1.172381, Privacy loss: 2.906, 1e-05/3\n",
            "Train Epoch: 12 [53120/60000 (88%)]\tLoss: 1.336523, Privacy loss: 2.906, 1e-05/3\n",
            "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 0.828743, Privacy loss: 2.907, 1e-05/3\n",
            "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 1.029539, Privacy loss: 2.907, 1e-05/3\n",
            "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 0.585403, Privacy loss: 2.908, 1e-05/3\n",
            "Train Epoch: 12 [55680/60000 (93%)]\tLoss: 1.298039, Privacy loss: 2.908, 1e-05/3\n",
            "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 1.338277, Privacy loss: 2.908, 1e-05/3\n",
            "Train Epoch: 12 [56960/60000 (95%)]\tLoss: 0.939423, Privacy loss: 2.909, 1e-05/3\n",
            "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.403386, Privacy loss: 2.909, 1e-05/3\n",
            "Train Epoch: 12 [58240/60000 (97%)]\tLoss: 0.919664, Privacy loss: 2.91, 1e-05/3\n",
            "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.624196, Privacy loss: 2.91, 1e-05/3\n",
            "Train Epoch: 12 [59520/60000 (99%)]\tLoss: 1.952172, Privacy loss: 2.911, 1e-05/3\n",
            "\n",
            "Test set: Average loss: 0.3972, Accuracy: 9230/10000 (92%), Privacy: 2.911,1e-05/3\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 1.024381, Privacy loss: 2.911, 1e-05/3\n",
            "Train Epoch: 13 [640/60000 (1%)]\tLoss: 1.421876, Privacy loss: 2.912, 1e-05/3\n",
            "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 1.185961, Privacy loss: 2.912, 1e-05/3\n",
            "Train Epoch: 13 [1920/60000 (3%)]\tLoss: 1.606394, Privacy loss: 2.913, 1e-05/3\n",
            "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.973446, Privacy loss: 2.913, 1e-05/3\n",
            "Train Epoch: 13 [3200/60000 (5%)]\tLoss: 0.868386, Privacy loss: 2.914, 1e-05/3\n",
            "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 0.512518, Privacy loss: 2.914, 1e-05/3\n",
            "Train Epoch: 13 [4480/60000 (7%)]\tLoss: 0.836727, Privacy loss: 2.914, 1e-05/3\n",
            "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 1.263457, Privacy loss: 2.915, 1e-05/3\n",
            "Train Epoch: 13 [5760/60000 (10%)]\tLoss: 1.231332, Privacy loss: 2.915, 1e-05/3\n",
            "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.666031, Privacy loss: 2.916, 1e-05/3\n",
            "Train Epoch: 13 [7040/60000 (12%)]\tLoss: 0.923714, Privacy loss: 2.916, 1e-05/3\n",
            "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 1.323606, Privacy loss: 2.917, 1e-05/3\n",
            "Train Epoch: 13 [8320/60000 (14%)]\tLoss: 0.681660, Privacy loss: 2.917, 1e-05/3\n",
            "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 1.115860, Privacy loss: 2.918, 1e-05/3\n",
            "Train Epoch: 13 [9600/60000 (16%)]\tLoss: 0.395643, Privacy loss: 2.918, 1e-05/3\n",
            "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 1.513965, Privacy loss: 2.919, 1e-05/3\n",
            "Train Epoch: 13 [10880/60000 (18%)]\tLoss: 1.633366, Privacy loss: 2.919, 1e-05/3\n",
            "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 1.268866, Privacy loss: 2.92, 1e-05/3\n",
            "Train Epoch: 13 [12160/60000 (20%)]\tLoss: 0.970884, Privacy loss: 2.92, 1e-05/3\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 1.110411, Privacy loss: 2.921, 1e-05/3\n",
            "Train Epoch: 13 [13440/60000 (22%)]\tLoss: 1.141443, Privacy loss: 2.921, 1e-05/3\n",
            "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 2.388871, Privacy loss: 2.922, 1e-05/3\n",
            "Train Epoch: 13 [14720/60000 (25%)]\tLoss: 0.837430, Privacy loss: 2.922, 1e-05/3\n",
            "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 1.225503, Privacy loss: 2.922, 1e-05/3\n",
            "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 1.794536, Privacy loss: 2.923, 1e-05/3\n",
            "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 0.612403, Privacy loss: 2.923, 1e-05/3\n",
            "Train Epoch: 13 [17280/60000 (29%)]\tLoss: 0.975476, Privacy loss: 2.924, 1e-05/3\n",
            "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 1.006499, Privacy loss: 2.924, 1e-05/3\n",
            "Train Epoch: 13 [18560/60000 (31%)]\tLoss: 1.651338, Privacy loss: 2.925, 1e-05/3\n",
            "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 1.658578, Privacy loss: 2.925, 1e-05/3\n",
            "Train Epoch: 13 [19840/60000 (33%)]\tLoss: 1.090306, Privacy loss: 2.926, 1e-05/3\n",
            "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 1.068573, Privacy loss: 2.926, 1e-05/3\n",
            "Train Epoch: 13 [21120/60000 (35%)]\tLoss: 0.844298, Privacy loss: 2.927, 1e-05/3\n",
            "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 0.405302, Privacy loss: 2.927, 1e-05/3\n",
            "Train Epoch: 13 [22400/60000 (37%)]\tLoss: 1.058141, Privacy loss: 2.928, 1e-05/3\n",
            "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 1.434456, Privacy loss: 2.928, 1e-05/3\n",
            "Train Epoch: 13 [23680/60000 (39%)]\tLoss: 1.171065, Privacy loss: 2.929, 1e-05/3\n",
            "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 0.825396, Privacy loss: 2.929, 1e-05/3\n",
            "Train Epoch: 13 [24960/60000 (42%)]\tLoss: 1.154013, Privacy loss: 2.929, 1e-05/3\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 1.057217, Privacy loss: 2.93, 1e-05/3\n",
            "Train Epoch: 13 [26240/60000 (44%)]\tLoss: 1.191678, Privacy loss: 2.93, 1e-05/3\n",
            "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 0.208247, Privacy loss: 2.931, 1e-05/3\n",
            "Train Epoch: 13 [27520/60000 (46%)]\tLoss: 1.525390, Privacy loss: 2.931, 1e-05/3\n",
            "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.620951, Privacy loss: 2.932, 1e-05/3\n",
            "Train Epoch: 13 [28800/60000 (48%)]\tLoss: 1.310943, Privacy loss: 2.932, 1e-05/3\n",
            "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 1.025157, Privacy loss: 2.933, 1e-05/3\n",
            "Train Epoch: 13 [30080/60000 (50%)]\tLoss: 0.833169, Privacy loss: 2.933, 1e-05/3\n",
            "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 1.612007, Privacy loss: 2.934, 1e-05/3\n",
            "Train Epoch: 13 [31360/60000 (52%)]\tLoss: 0.809581, Privacy loss: 2.934, 1e-05/3\n",
            "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 1.144689, Privacy loss: 2.935, 1e-05/3\n",
            "Train Epoch: 13 [32640/60000 (54%)]\tLoss: 1.284966, Privacy loss: 2.935, 1e-05/3\n",
            "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.737433, Privacy loss: 2.936, 1e-05/3\n",
            "Train Epoch: 13 [33920/60000 (57%)]\tLoss: 0.815935, Privacy loss: 2.936, 1e-05/3\n",
            "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.618333, Privacy loss: 2.937, 1e-05/3\n",
            "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 1.168399, Privacy loss: 2.937, 1e-05/3\n",
            "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 1.825275, Privacy loss: 2.937, 1e-05/3\n",
            "Train Epoch: 13 [36480/60000 (61%)]\tLoss: 1.720546, Privacy loss: 2.938, 1e-05/3\n",
            "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 0.528033, Privacy loss: 2.938, 1e-05/3\n",
            "Train Epoch: 13 [37760/60000 (63%)]\tLoss: 1.760733, Privacy loss: 2.939, 1e-05/3\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 1.457352, Privacy loss: 2.939, 1e-05/3\n",
            "Train Epoch: 13 [39040/60000 (65%)]\tLoss: 0.835354, Privacy loss: 2.94, 1e-05/3\n",
            "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 1.532207, Privacy loss: 2.94, 1e-05/3\n",
            "Train Epoch: 13 [40320/60000 (67%)]\tLoss: 1.102672, Privacy loss: 2.941, 1e-05/3\n",
            "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.870277, Privacy loss: 2.941, 1e-05/3\n",
            "Train Epoch: 13 [41600/60000 (69%)]\tLoss: 1.033671, Privacy loss: 2.942, 1e-05/3\n",
            "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 0.605170, Privacy loss: 2.942, 1e-05/3\n",
            "Train Epoch: 13 [42880/60000 (71%)]\tLoss: 0.888318, Privacy loss: 2.943, 1e-05/3\n",
            "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.977419, Privacy loss: 2.943, 1e-05/3\n",
            "Train Epoch: 13 [44160/60000 (74%)]\tLoss: 0.970939, Privacy loss: 2.944, 1e-05/3\n",
            "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 1.122496, Privacy loss: 2.944, 1e-05/3\n",
            "Train Epoch: 13 [45440/60000 (76%)]\tLoss: 1.880849, Privacy loss: 2.945, 1e-05/3\n",
            "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.772584, Privacy loss: 2.945, 1e-05/3\n",
            "Train Epoch: 13 [46720/60000 (78%)]\tLoss: 0.630528, Privacy loss: 2.945, 1e-05/3\n",
            "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 2.451709, Privacy loss: 2.946, 1e-05/3\n",
            "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 1.047404, Privacy loss: 2.946, 1e-05/3\n",
            "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 1.159761, Privacy loss: 2.947, 1e-05/3\n",
            "Train Epoch: 13 [49280/60000 (82%)]\tLoss: 0.794104, Privacy loss: 2.947, 1e-05/3\n",
            "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 1.593330, Privacy loss: 2.948, 1e-05/3\n",
            "Train Epoch: 13 [50560/60000 (84%)]\tLoss: 0.962688, Privacy loss: 2.948, 1e-05/3\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.624027, Privacy loss: 2.949, 1e-05/3\n",
            "Train Epoch: 13 [51840/60000 (86%)]\tLoss: 1.136119, Privacy loss: 2.949, 1e-05/3\n",
            "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 0.572929, Privacy loss: 2.95, 1e-05/3\n",
            "Train Epoch: 13 [53120/60000 (88%)]\tLoss: 1.428846, Privacy loss: 2.95, 1e-05/3\n",
            "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 0.637553, Privacy loss: 2.951, 1e-05/3\n",
            "Train Epoch: 13 [54400/60000 (91%)]\tLoss: 1.630236, Privacy loss: 2.951, 1e-05/3\n",
            "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 0.907342, Privacy loss: 2.952, 1e-05/3\n",
            "Train Epoch: 13 [55680/60000 (93%)]\tLoss: 1.051013, Privacy loss: 2.952, 1e-05/3\n",
            "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 1.162684, Privacy loss: 2.952, 1e-05/3\n",
            "Train Epoch: 13 [56960/60000 (95%)]\tLoss: 0.935174, Privacy loss: 2.953, 1e-05/3\n",
            "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.877742, Privacy loss: 2.953, 1e-05/3\n",
            "Train Epoch: 13 [58240/60000 (97%)]\tLoss: 0.913979, Privacy loss: 2.954, 1e-05/3\n",
            "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 2.086787, Privacy loss: 2.954, 1e-05/3\n",
            "Train Epoch: 13 [59520/60000 (99%)]\tLoss: 0.791663, Privacy loss: 2.955, 1e-05/3\n",
            "\n",
            "Test set: Average loss: 0.3976, Accuracy: 9225/10000 (92%), Privacy: 2.955,1e-05/3\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 1.876031, Privacy loss: 2.955, 1e-05/3\n",
            "Train Epoch: 14 [640/60000 (1%)]\tLoss: 1.696801, Privacy loss: 2.956, 1e-05/3\n",
            "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 0.828548, Privacy loss: 2.956, 1e-05/3\n",
            "Train Epoch: 14 [1920/60000 (3%)]\tLoss: 1.878961, Privacy loss: 2.957, 1e-05/3\n",
            "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 1.250699, Privacy loss: 2.957, 1e-05/3\n",
            "Train Epoch: 14 [3200/60000 (5%)]\tLoss: 1.153189, Privacy loss: 2.958, 1e-05/3\n",
            "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 0.807294, Privacy loss: 2.958, 1e-05/3\n",
            "Train Epoch: 14 [4480/60000 (7%)]\tLoss: 0.442431, Privacy loss: 2.958, 1e-05/3\n",
            "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 1.124826, Privacy loss: 2.959, 1e-05/3\n",
            "Train Epoch: 14 [5760/60000 (10%)]\tLoss: 1.621378, Privacy loss: 2.959, 1e-05/3\n",
            "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.583701, Privacy loss: 2.96, 1e-05/3\n",
            "Train Epoch: 14 [7040/60000 (12%)]\tLoss: 2.118235, Privacy loss: 2.96, 1e-05/3\n",
            "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 1.224430, Privacy loss: 2.961, 1e-05/3\n",
            "Train Epoch: 14 [8320/60000 (14%)]\tLoss: 1.311357, Privacy loss: 2.961, 1e-05/3\n",
            "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 0.805219, Privacy loss: 2.962, 1e-05/3\n",
            "Train Epoch: 14 [9600/60000 (16%)]\tLoss: 1.302575, Privacy loss: 2.962, 1e-05/3\n",
            "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.841069, Privacy loss: 2.963, 1e-05/3\n",
            "Train Epoch: 14 [10880/60000 (18%)]\tLoss: 0.328395, Privacy loss: 2.963, 1e-05/3\n",
            "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 0.951911, Privacy loss: 2.964, 1e-05/3\n",
            "Train Epoch: 14 [12160/60000 (20%)]\tLoss: 0.413445, Privacy loss: 2.964, 1e-05/3\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.956668, Privacy loss: 2.965, 1e-05/3\n",
            "Train Epoch: 14 [13440/60000 (22%)]\tLoss: 0.547164, Privacy loss: 2.965, 1e-05/3\n",
            "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 1.636250, Privacy loss: 2.966, 1e-05/3\n",
            "Train Epoch: 14 [14720/60000 (25%)]\tLoss: 0.649215, Privacy loss: 2.966, 1e-05/3\n",
            "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 1.038327, Privacy loss: 2.966, 1e-05/3\n",
            "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.906936, Privacy loss: 2.967, 1e-05/3\n",
            "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 0.965797, Privacy loss: 2.967, 1e-05/3\n",
            "Train Epoch: 14 [17280/60000 (29%)]\tLoss: 0.913036, Privacy loss: 2.968, 1e-05/3\n",
            "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.812330, Privacy loss: 2.968, 1e-05/3\n",
            "Train Epoch: 14 [18560/60000 (31%)]\tLoss: 0.577877, Privacy loss: 2.969, 1e-05/3\n",
            "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.664984, Privacy loss: 2.969, 1e-05/3\n",
            "Train Epoch: 14 [19840/60000 (33%)]\tLoss: 1.086995, Privacy loss: 2.97, 1e-05/3\n",
            "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 1.095775, Privacy loss: 2.97, 1e-05/3\n",
            "Train Epoch: 14 [21120/60000 (35%)]\tLoss: 0.730586, Privacy loss: 2.971, 1e-05/3\n",
            "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 0.747049, Privacy loss: 2.971, 1e-05/3\n",
            "Train Epoch: 14 [22400/60000 (37%)]\tLoss: 0.658566, Privacy loss: 2.972, 1e-05/3\n",
            "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.616925, Privacy loss: 2.972, 1e-05/3\n",
            "Train Epoch: 14 [23680/60000 (39%)]\tLoss: 2.015219, Privacy loss: 2.973, 1e-05/3\n",
            "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 1.682550, Privacy loss: 2.973, 1e-05/3\n",
            "Train Epoch: 14 [24960/60000 (42%)]\tLoss: 0.875198, Privacy loss: 2.973, 1e-05/3\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 1.014293, Privacy loss: 2.974, 1e-05/3\n",
            "Train Epoch: 14 [26240/60000 (44%)]\tLoss: 1.577381, Privacy loss: 2.974, 1e-05/3\n",
            "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 0.972032, Privacy loss: 2.975, 1e-05/3\n",
            "Train Epoch: 14 [27520/60000 (46%)]\tLoss: 1.634695, Privacy loss: 2.975, 1e-05/3\n",
            "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 1.130322, Privacy loss: 2.976, 1e-05/3\n",
            "Train Epoch: 14 [28800/60000 (48%)]\tLoss: 0.948825, Privacy loss: 2.976, 1e-05/3\n",
            "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 0.840598, Privacy loss: 2.977, 1e-05/3\n",
            "Train Epoch: 14 [30080/60000 (50%)]\tLoss: 1.053789, Privacy loss: 2.977, 1e-05/3\n",
            "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 1.506451, Privacy loss: 2.978, 1e-05/3\n",
            "Train Epoch: 14 [31360/60000 (52%)]\tLoss: 0.970343, Privacy loss: 2.978, 1e-05/3\n",
            "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.957992, Privacy loss: 2.979, 1e-05/3\n",
            "Train Epoch: 14 [32640/60000 (54%)]\tLoss: 1.452651, Privacy loss: 2.979, 1e-05/3\n",
            "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.911242, Privacy loss: 2.98, 1e-05/3\n",
            "Train Epoch: 14 [33920/60000 (57%)]\tLoss: 1.022929, Privacy loss: 2.98, 1e-05/3\n",
            "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 0.917665, Privacy loss: 2.981, 1e-05/3\n",
            "Train Epoch: 14 [35200/60000 (59%)]\tLoss: 1.032915, Privacy loss: 2.981, 1e-05/3\n",
            "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 1.020082, Privacy loss: 2.981, 1e-05/3\n",
            "Train Epoch: 14 [36480/60000 (61%)]\tLoss: 2.824270, Privacy loss: 2.982, 1e-05/3\n",
            "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 1.688138, Privacy loss: 2.982, 1e-05/3\n",
            "Train Epoch: 14 [37760/60000 (63%)]\tLoss: 0.807386, Privacy loss: 2.983, 1e-05/3\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.917400, Privacy loss: 2.983, 1e-05/3\n",
            "Train Epoch: 14 [39040/60000 (65%)]\tLoss: 1.579918, Privacy loss: 2.984, 1e-05/3\n",
            "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 1.118634, Privacy loss: 2.984, 1e-05/3\n",
            "Train Epoch: 14 [40320/60000 (67%)]\tLoss: 1.039422, Privacy loss: 2.985, 1e-05/3\n",
            "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 1.379578, Privacy loss: 2.985, 1e-05/3\n",
            "Train Epoch: 14 [41600/60000 (69%)]\tLoss: 1.011120, Privacy loss: 2.986, 1e-05/3\n",
            "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 1.237700, Privacy loss: 2.986, 1e-05/3\n",
            "Train Epoch: 14 [42880/60000 (71%)]\tLoss: 0.741341, Privacy loss: 2.987, 1e-05/3\n",
            "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.660910, Privacy loss: 2.987, 1e-05/3\n",
            "Train Epoch: 14 [44160/60000 (74%)]\tLoss: 1.640722, Privacy loss: 2.988, 1e-05/3\n",
            "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.346939, Privacy loss: 2.988, 1e-05/3\n",
            "Train Epoch: 14 [45440/60000 (76%)]\tLoss: 1.335071, Privacy loss: 2.988, 1e-05/3\n",
            "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.581235, Privacy loss: 2.989, 1e-05/3\n",
            "Train Epoch: 14 [46720/60000 (78%)]\tLoss: 0.296221, Privacy loss: 2.989, 1e-05/3\n",
            "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 1.157681, Privacy loss: 2.99, 1e-05/3\n",
            "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.554445, Privacy loss: 2.99, 1e-05/3\n",
            "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 1.023368, Privacy loss: 2.991, 1e-05/3\n",
            "Train Epoch: 14 [49280/60000 (82%)]\tLoss: 0.932231, Privacy loss: 2.991, 1e-05/3\n",
            "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 0.754838, Privacy loss: 2.992, 1e-05/3\n",
            "Train Epoch: 14 [50560/60000 (84%)]\tLoss: 0.814085, Privacy loss: 2.992, 1e-05/3\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 1.327647, Privacy loss: 2.993, 1e-05/3\n",
            "Train Epoch: 14 [51840/60000 (86%)]\tLoss: 1.055063, Privacy loss: 2.993, 1e-05/3\n",
            "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 1.012379, Privacy loss: 2.994, 1e-05/3\n",
            "Train Epoch: 14 [53120/60000 (88%)]\tLoss: 0.841475, Privacy loss: 2.994, 1e-05/3\n",
            "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 0.863865, Privacy loss: 2.995, 1e-05/3\n",
            "Train Epoch: 14 [54400/60000 (91%)]\tLoss: 1.653337, Privacy loss: 2.995, 1e-05/3\n",
            "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 1.485149, Privacy loss: 2.996, 1e-05/3\n",
            "Train Epoch: 14 [55680/60000 (93%)]\tLoss: 0.808572, Privacy loss: 2.996, 1e-05/3\n",
            "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.760511, Privacy loss: 2.996, 1e-05/3\n",
            "Train Epoch: 14 [56960/60000 (95%)]\tLoss: 0.916809, Privacy loss: 2.997, 1e-05/3\n",
            "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.897480, Privacy loss: 2.997, 1e-05/3\n",
            "Train Epoch: 14 [58240/60000 (97%)]\tLoss: 2.434109, Privacy loss: 2.998, 1e-05/3\n",
            "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.788815, Privacy loss: 2.998, 1e-05/3\n",
            "Train Epoch: 14 [59520/60000 (99%)]\tLoss: 1.250232, Privacy loss: 2.999, 1e-05/3\n",
            "\n",
            "Test set: Average loss: 0.3976, Accuracy: 9237/10000 (92%), Privacy: 2.999,1e-05/3\n",
            "\n",
            "CPU times: user 2min 48s, sys: 10.3 s, total: 2min 58s\n",
            "Wall time: 3min 26s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!ls -l '/content/gdrive/My Drive/colab_notebooks/data/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpmYfPuDc3sj",
        "outputId": "3e8c772c-61d9-4b28-98bc-db0cebcdd012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 137\n",
            "-rw------- 1 root root    42 Oct 16 04:31 first_file.csv\n",
            "-rw------- 1 root root   543 Oct 16 04:59 p_test_stats.csv\n",
            "-rw------- 1 root root 67255 Oct 16 04:59 p_train_stats.csv\n",
            "-rw------- 1 root root   568 Oct 16 04:41 test_stats.csv\n",
            "-rw------- 1 root root 69634 Oct 16 04:41 train_stats.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_stats.tail(), p_train_stats.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSRbe28KGS2g",
        "outputId": "236c88bf-9044-404c-9911-8960467cb725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(      epoch  train_example_num  num_examples  percentage_of_examples      loss\n",
              " 1311     14              56960         60000               94.882729  0.068968\n",
              " 1312     14              57600         60000               95.948827  0.001255\n",
              " 1313     14              58240         60000               97.014925  0.011428\n",
              " 1314     14              58880         60000               98.081023  0.004694\n",
              " 1315     14              59520         60000               99.147122  0.003094,\n",
              "       epoch  train_example_num  num_examples  percentage_of_examples      loss\n",
              " 1311     14              56960         60000               94.882729  0.915253\n",
              " 1312     14              57600         60000               95.948827  0.862054\n",
              " 1313     14              58240         60000               97.014925  2.390769\n",
              " 1314     14              58880         60000               98.081023  0.797129\n",
              " 1315     14              59520         60000               99.147122  1.240519)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_stats.tail(), p_test_stats.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGIdmRsRGS5o",
        "outputId": "734f8100-0dfb-40ef-c07a-2e6780650c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(    average_loss  num_correct  test_size  accuracy\n",
              " 9       0.027705         9915      10000     99.15\n",
              " 10      0.027111         9920      10000     99.20\n",
              " 11      0.027103         9916      10000     99.16\n",
              " 12      0.027714         9914      10000     99.14\n",
              " 13      0.027283         9917      10000     99.17,\n",
              "     average_loss  num_correct  test_size  accuracy\n",
              " 9       0.403490         9225      10000     92.25\n",
              " 10      0.399987         9235      10000     92.35\n",
              " 11      0.396432         9226      10000     92.26\n",
              " 12      0.396736         9236      10000     92.36\n",
              " 13      0.396802         9239      10000     92.39)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_stats.tail(), p_train_stats.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhILEBaBL3EW",
        "outputId": "5e2f64b2-48c9-43d7-bf71-ba1a49dd6cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(      epoch  train_example_num  num_examples  percentage_of_examples      loss\n",
              " 1311     14              56960         60000               94.882729  0.068968\n",
              " 1312     14              57600         60000               95.948827  0.001255\n",
              " 1313     14              58240         60000               97.014925  0.011428\n",
              " 1314     14              58880         60000               98.081023  0.004694\n",
              " 1315     14              59520         60000               99.147122  0.003094,\n",
              "       epoch  train_example_num  num_examples  percentage_of_examples      loss\n",
              " 1311     14              56960         60000               94.882729  0.915253\n",
              " 1312     14              57600         60000               95.948827  0.862054\n",
              " 1313     14              58240         60000               97.014925  2.390769\n",
              " 1314     14              58880         60000               98.081023  0.797129\n",
              " 1315     14              59520         60000               99.147122  1.240519)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l gdrive/MyDrive/colab_notebooks/data"
      ],
      "metadata": {
        "id": "qcN2Zfw5Opta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c38643dd-2df3-475a-dbc1-19d6a236fb6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 145\n",
            "-rw------- 1 root root    42 Oct 16 04:31 first_file.csv\n",
            "-rw------- 1 root root   641 Nov  5 21:46 p_test_stats.csv\n",
            "-rw------- 1 root root 75030 Nov  5 21:46 p_train_stats.csv\n",
            "-rw------- 1 root root   563 Nov  5 20:40 test_stats.csv\n",
            "-rw------- 1 root root 69657 Nov  5 20:40 train_stats.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "base = '/content/gdrive/MyDrive/colab_notebooks/data'\n",
        "\n",
        "train_df = pd.read_csv(os.path.join(base, 'train_stats.csv'))\n",
        "test_df = pd.read_csv(os.path.join(base, 'test_stats.csv'))\n",
        "\n",
        "p_train_df = pd.read_csv(os.path.join(base, 'p_train_stats.csv'))\n",
        "p_test_df = pd.read_csv(os.path.join(base, 'p_test_stats.csv'))"
      ],
      "metadata": {
        "id": "wctZAMNH2_-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "9MMiBYUJ3UzE",
        "outputId": "80944ca0-360e-4beb-d578-ce0e58262c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   epoch  train_example_num  num_examples  percentage_of_examples      loss\n",
              "0      1                  0         60000                0.000000  2.299824\n",
              "1      1                640         60000                1.066098  1.771508\n",
              "2      1               1280         60000                2.132196  0.950519\n",
              "3      1               1920         60000                3.198294  0.658959\n",
              "4      1               2560         60000                4.264392  0.303416"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0788dff6-4d8b-4041-8b1c-98a9b28ee548\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch</th>\n",
              "      <th>train_example_num</th>\n",
              "      <th>num_examples</th>\n",
              "      <th>percentage_of_examples</th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>60000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.299824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>640</td>\n",
              "      <td>60000</td>\n",
              "      <td>1.066098</td>\n",
              "      <td>1.771508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1280</td>\n",
              "      <td>60000</td>\n",
              "      <td>2.132196</td>\n",
              "      <td>0.950519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1920</td>\n",
              "      <td>60000</td>\n",
              "      <td>3.198294</td>\n",
              "      <td>0.658959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2560</td>\n",
              "      <td>60000</td>\n",
              "      <td>4.264392</td>\n",
              "      <td>0.303416</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0788dff6-4d8b-4041-8b1c-98a9b28ee548')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0788dff6-4d8b-4041-8b1c-98a9b28ee548 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0788dff6-4d8b-4041-8b1c-98a9b28ee548');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "p_train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Md4I22JI3WWK",
        "outputId": "12d12098-5fb9-4faa-9626-240f03c7805b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   epoch  train_example_num  num_examples  percentage_of_examples      loss  \\\n",
              "0      1                  0         60000                0.000000  2.299824   \n",
              "1      1                640         60000                1.066098  2.250466   \n",
              "2      1               1280         60000                2.132196  2.103608   \n",
              "3      1               1920         60000                3.198294  1.780792   \n",
              "4      1               2560         60000                4.264392  1.566189   \n",
              "\n",
              "   curr_privacy  \n",
              "0         1.681  \n",
              "1         1.811  \n",
              "2         1.851  \n",
              "3         1.875  \n",
              "4         1.896  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8cc668e2-b7f0-46f6-b835-799447ccc7cf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch</th>\n",
              "      <th>train_example_num</th>\n",
              "      <th>num_examples</th>\n",
              "      <th>percentage_of_examples</th>\n",
              "      <th>loss</th>\n",
              "      <th>curr_privacy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>60000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.299824</td>\n",
              "      <td>1.681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>640</td>\n",
              "      <td>60000</td>\n",
              "      <td>1.066098</td>\n",
              "      <td>2.250466</td>\n",
              "      <td>1.811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1280</td>\n",
              "      <td>60000</td>\n",
              "      <td>2.132196</td>\n",
              "      <td>2.103608</td>\n",
              "      <td>1.851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1920</td>\n",
              "      <td>60000</td>\n",
              "      <td>3.198294</td>\n",
              "      <td>1.780792</td>\n",
              "      <td>1.875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2560</td>\n",
              "      <td>60000</td>\n",
              "      <td>4.264392</td>\n",
              "      <td>1.566189</td>\n",
              "      <td>1.896</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8cc668e2-b7f0-46f6-b835-799447ccc7cf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8cc668e2-b7f0-46f6-b835-799447ccc7cf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8cc668e2-b7f0-46f6-b835-799447ccc7cf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p_test_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Ei1jkyX-4--1",
        "outputId": "576b36b9-14f7-46e4-8020-c5a9cacf5da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   average_loss  num_correct  test_size  accuracy  curr_privacy\n",
              "0      0.537703         8721      10000     87.21         2.200\n",
              "1      0.489345         8961      10000     89.61         2.314\n",
              "2      0.489763         8998      10000     89.98         2.403\n",
              "3      0.446895         9121      10000     91.21         2.473\n",
              "4      0.418516         9154      10000     91.54         2.542"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9e460733-135c-42af-a2d4-c8cf9d3fcb45\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>average_loss</th>\n",
              "      <th>num_correct</th>\n",
              "      <th>test_size</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>curr_privacy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.537703</td>\n",
              "      <td>8721</td>\n",
              "      <td>10000</td>\n",
              "      <td>87.21</td>\n",
              "      <td>2.200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.489345</td>\n",
              "      <td>8961</td>\n",
              "      <td>10000</td>\n",
              "      <td>89.61</td>\n",
              "      <td>2.314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.489763</td>\n",
              "      <td>8998</td>\n",
              "      <td>10000</td>\n",
              "      <td>89.98</td>\n",
              "      <td>2.403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.446895</td>\n",
              "      <td>9121</td>\n",
              "      <td>10000</td>\n",
              "      <td>91.21</td>\n",
              "      <td>2.473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.418516</td>\n",
              "      <td>9154</td>\n",
              "      <td>10000</td>\n",
              "      <td>91.54</td>\n",
              "      <td>2.542</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e460733-135c-42af-a2d4-c8cf9d3fcb45')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9e460733-135c-42af-a2d4-c8cf9d3fcb45 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9e460733-135c-42af-a2d4-c8cf9d3fcb45');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "sns.set_context(\"paper\")\n",
        "plt.style.context('seaborn-poster')\n",
        "import numpy as np\n",
        "\n",
        "# Privacy budget epsilon = 3 ==> full 10% lower --> 7% lower\n",
        "plt.plot(p_test_df['accuracy'], p_test_df['curr_privacy'])\n",
        "# plt.plot(p_train_df.index.values, p_train_df['curr_privacy'])\n",
        "plt.xlabel('accuracy')\n",
        "plt.ylabel('privacy')\n",
        "plt.title('MNIST test accuracy - accuracy vs privacy')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "dXKTZWOI3j7J",
        "outputId": "a2f66d4a-38ed-4582-d09f-303e477c82ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'MNIST test accuracy - accuracy vs privacy')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Privacy budget epsilon = 3 ==> full 10% lower --> 7% lower\n",
        "plt.plot(p_test_df.index.values, p_test_df['accuracy']/p_test_df['curr_privacy'])\n",
        "# plt.plot(p_train_df.index.values, p_train_df['curr_privacy'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy/privacy')\n",
        "plt.title('MNIST test accuracy/privacy by epoch')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "a0KbO0C-5Lbb",
        "outputId": "f211c662-58cc-41f9-f74e-551371ab77d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'MNIST test accuracy/privacy by epoch')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAGBCAYAAABclgTiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdZ2CUVdrG8f9Meu89pJBGCRBIAgmh95aEYkMFcQHbsvu+u2tBBVlEbLuu+wrrrl1REBZFivQmCIRAAEE6BEgooRMggARC3g9IVlbKMGQyk+T6fUommWdu7hnh8pzznGMoLy8vR0REREQsymjtAkRERERqA4UuERERkSqg0CUiIiJSBRS6RERERKqAQpeIiIhIFVDoEhEREakCCl0iNiYhIYFXXnml4vs9e/aQkJDAl19+CcDw4cPp3LkzZWVlAOTm5vKHP/wBgGnTpvHXv/4VgPz8fB566CGys7Pp1q0b77zzDsuWLSM7O5vs7GwSExPJysoiOzubCRMmXFfDokWLKCgoMKv+jz76yKzn2aLNmzfzwgsvmPz7P/74Y0X/q8qAAQPIz8+v0tc0ha3WJWJN9tYuQESu5+vry/r16ykvL8dgMDBv3jzi4+Ov+52LFy8yf/58evTocdPrjB07lmHDhpGenk5ZWRl79+4lNjaWtm3bAtChQwemTp2Kk5PTr567aNEiHB0diYyMvOP6P/74YwYPHnzHz6ssZWVl2NnZVcq1Vq1aRatWrUx+3UaNGtGoUaNKeW0RqXk00iViY4xGI40bN2bDhg0ALF++nDZt2lz3O4888ggffvjhLa9z7NgxgoKCALCzsyM2Ntak19+4cSNLlizh5ZdfJjs7m5KSEjZt2sSDDz5Inz59+P3vf8+FCxc4d+4cgwcPJjMzk8zMTHJzc3n77bcpLi4mOzubV1999brrlpSUMHDgQPr06UPv3r3Jy8ur+Nn48ePp1asXmZmZTJ48GYDFixeTnZ1NVlYWL774InD96MkvR/iGDx/On//8Z/r168dnn33GokWLuPfee8nOzubJJ5/k3LlzABw5coTHHnuMrKws+vbtS1FREffffz+FhYUAXL58ma5du3Lx4kUAcnJySE9PZ9q0afz+97+nf//+dO3ataLG3NxcBg0axODBgxk0aFBFTWVlZXTs2JGffvoJgDNnztC9e3cAvvzyS/r160dmZibPP/88V65cAa6OTD788MNkZWVx7733UlpaSteuXStqP3nyJJmZmTd8z6ZMmUJWVhb9+vWjoKCAM2fO0K1bN67tfb19+/YbBuEbva8ALVu25KWXXqJHjx4MGzas4vFly5aRmZlJr169GDduXMV1bvRewdWR1759+9K7d2+KiopuWLtIbaKRLhEb1K1bN+bNm4ePjw/BwcE4Oztf9/O4uDgCAgJYtWrVTUd1Bg4cyH333UdKSgqtW7emX79+v7rOjTRp0oQOHTrQo0cP2rRpQ2lpKW+++Sb//Oc/8fLy4sMPP2TSpEmEh4fj7+/PRx99xJUrVzh//jwtWrTgq6++YsaMGb+6rpOTE++++y7u7u4cPnyY3/3ud0ydOpWlS5eyfv16pk2bhqOjI8XFxRw/fpzXX3+diRMnEhgYSHFx8W3rPnv2LF999RUGg4HTp0/TqVMnAD744AO++uorHnnkEcaOHUu3bt3o27dvRSDq3bs3M2fOZNiwYSxfvpzU1FScnJy4cOECFy9exMfHB7g61Thz5kwA+vXrR/v27QHYsmULc+bMISAggNzcXOBqyM3IyGDFihV06tSJxYsXV/x+9+7d6d+/PwB//vOfWbJkCZ06deLZZ5/lmWeeIS0tjbNnz2Jvb0/nzp1ZsGABffr0Yfbs2Tcd2TQYDMycOZNFixbx6quv8t5771G/fn3Wrl1L8+bNmT59OtnZ2dc952bv6+DBgzlx4gRt27bl5Zdf5tVXX2XSpEk89NBDjB49mokTJ+Lv78+AAQNIT08nKirqpu+Vq6sr06ZN4+OPP2by5MkVIVmkttJIl4gNSk1NZf369cydO5euXbve8HeGDh3KBx98cNNr3HvvvcyaNYv27dszb948s6f89u7dy44dOxg4cCDZ2dlMmzaNQ4cOER8fz5o1a/jLX/7C5s2bcXd3v+V1ysvL+ctf/kJmZiaPP/44u3fvBmD16tX069cPR0dHALy9vdm4cSPp6ekEBgZWPHY7Xbt2xWAwAFBUVMSgQYPIzMxkypQpFaNjGzZsoE+fPgA4Ozvj7OxMz549mT9/PgDTp0+nd+/eAKxdu5bU1NSK67dp0wZ3d3fc3d1JT0/nxx9/BCA5OZmAgIBf1XMtOAPMnz+fbt26AVdHnR544AEyMzNZunQp+fn5lJSUcO7cOdLS0gDw8PDAaDTSp0+figA7c+ZMsrKybvhn79mzJwCdOnVi69atAPTt25cZM2ZQVlbG0qVL6dy583XPudn7CuDi4kLHjh0B6NWrF+vWrWPv3r3ExMQQEhKCg4MDPXr0YP369bd8r65do379+hw4cOAm75xI7aGRLhEbZGdnR2JiIpMnT2bevHkVAeWXUlJSuHDhQsU/sjcSEhLC/fffT79+/UhPT+fkyZP4+vreUS3l5eU0atSIjz/++Fc/+/rrr/nuu+94+eWX6d+/P/369bvpdWbNmsWlS5eYPn06dnZ2NG3a9I7qgKtTr9emzEpLS6/72S9H8V555RV+97vf0aJFC+bNm8d3331302t6enoSFxfHsmXL2LVrF8nJycDV9VzXRqeAikD331+7uLjc8LrNmzdn1KhRnDx5kr1799K4cWMARo4cyQcffEBUVBQfffQR58+fv2ltMTExXLx4kVWrVuHs7ExYWNhNf/e/a8vIyOD1119nyZIlNG3a9Fd13up9vdk175SDgwNw9X27No0qUptppEvERg0YMICnn34aV1fXm/7OkCFD+PTTT2/4sxUrVnD58mUACgsLMRqNeHp6mvTabm5uFWuJ6taty4EDB9ixYwcA58+fp6CggCNHjuDm5kbfvn156KGH2L59O3D1H+gb/QNbUlKCn58fdnZ2zJs3ryJstGzZkq+//roiRBUXF5OUlEROTg5Hjx6teAwgNDSUbdu2AbBkyZKb1l9SUkJAQABXrly5bqqzWbNmfPPNN8DVmxGuTTH26dOHF198kW7dulUEjA0bNlwXDJcvX05JSQklJSXk5OTcdsG8vb09LVq04JVXXqFdu3YVj1+4cAFfX18uXrzInDlzAHB3d8fDw4PVq1cDV6dKr/UwOzubZ5999lfTg780d+5cAJYuXUqDBg2Aq0GnQ4cOjB49+obPvdn7eq3GpUuXVlw7OTmZ6Oho8vPzOXLkCJcvX2bevHk0a9bspu+ViPyaQpeIjYqJibnpdNI1HTt2xM3N7YY/W758OT179iQrK4s//elPvPnmm9jbmza43aNHD8aNG0d2djalpaX89a9/ZdSoUWRlZXH//fdTUFDAzp07ueeee8jOzmbKlCk8/PDDAGRlZZGZmfmrhfSZmZmsXr2azMxM8vLy8Pf3B6Bt27Y0a9aMPn36kJWVxYIFC/Dz8+P5559nyJAhZGVl8dZbbwEwaNAg/vnPf9K3b99b3qH41FNPMXToUO69917Cw8MrHn/xxReZO3cumZmZ9O/fvyIgZGRkcOnSpYpwcvToUXx8fCqmPAEaNmzI0KFD6devH4MGDaq4SeFWunXrxuzZs6+bIn7iiSfo06cPAwcOrAhIAG+88Qbjxo0jKyuLIUOGVATm7t27U1JSUjE9eSNlZWVkZWUxfvx4nn/++YrHe/bsWRH+/pujo+MN31cAPz8/li5dSs+ePSksLKR///44OzszatQohg4dSu/evUlPTyclJeWm75WI/Jqh/NpYvYhILZWfn8/IkSOZNGkScHVt15kzZxg4cCBw9S68PXv28PTTT1d5bcuXL2fmzJlm7f/15Zdfcvjw4TtewJ6RkcHKlSvv+PVE5Na0pktEarWpU6fy7rvv8tprr1U8dm0xvbWNHz+eb775hvfff/+OnztixAg2bNjwq41vRcR6NNIlIiIiUgW0pktERESkCih0iYiIiFQBi4euvLw8EhISOHnyJCdPnmTIkCH079//uiMkRERERGo6iy+k/+yzz0hMTASuHsfRr18/unfvzmOPPcbu3btNPg/u2LGzliwTb29Xiotvvkmh3Jj6Zh71zTzqm3nUN/Oob+ZR3yAgwOOGj1t0pGvp0qUkJydXbO64fv36ih2e27Vrx9q1ay358iIiIiI2w2IjXVeuXGHSpEmMHz+exYsXA1d3PL52VIenp+cdncXl7X3zXbkrg52d0eKvUROpb+ZR38yjvplHfTOP+mYe9e3mLBa6Zs2aRYcOHXBycqp4zMXFhYsXL+Lk5MTZs2fx8vIy+XqWHqrUcKh51DfzqG/mUd/Mo76ZR30zj/pmhenFnTt3Mn/+fAYPHsyOHTt4+umnSU5OZtmyZcDVXZZTUlIs9fIiIiIiNsViI13PPPNMxdcDBgyoOMLi2Wef5ZNPPiEtLY24uDhLvbyIiIiITamSY4A+//zziq8//PDDqnhJEREREZuizVFFREREqoBCl4iIiEgVUOgSERERqQIKXSIiIiJVQKFLREREpAoodImIiIhUAYUu4NTZi2wvOGntMkRERKQGU+gCDh0/x8j3cvhm+R6uXCm3djkiIiJSAyl0AQ2jfRk1uAXf/XCQ//tqEyUXLlm7JBEREalhFLp+lhjjz6hBqZRcuMTLn66l8MhZa5ckIiIiNYhC1y/4ejoz/KFmJEb78urn68jZfNjaJYmIiEgNUSVnL1YnDvZGBnarR3SIJ5/M3c6eojPc3yEWezvlUxERETGfQtdNtG4SSnigO+9+8yMFR87yVO9EvN2drF2WiIiIVFMavrmF6BBPXhqUioOdkdGfrGXXgWJrlyQiIiLVlELXbXi4OvLH+5vQslEwb07awKK8/ZSXa1sJERERuTOaXjSBndHIve1iiQ725KM529hbdIaB3erh5GBn7dJERESkmtBI1x1IqRfIyIEp7C06y6ufr+No8QVrlyQiIiLVhELXHQr1d2PkIykEeLsw5tO1bMo/Ye2SREREpBpQ6DKDi5M9v+2TSPe0SMZ9vYmZK/dyReu8RERE5Ba0pstMBoOBHmmRRAZ58N7MLewrOsuQXvVxdXawdmkiIiJigzTSdZcaRvvy0qAUTpVc5OXP8jhwrMTaJYmIiIgNUuiqBP5eLrzwcDPiw715ZUIea7YdsXZJIiIiYmM0vVhJHOzteLRHPeqGevLht1vZc+gM97SL0fFBIiIiAih0VSqDwUC7pmHUCXTn3embKTh8lid6J+Ll5mjt0kRERMTKNAxjATFhXrw0KBWAlz9dS/7B01auSERERKxNoctCvNwc+dMDSaTWC+SNSetZuuGgjg8SERGpxTS9aEH2dkYe6BhH3VBPPp6zjb2HzvBwl3gcdXyQiIhIraORrirQvH4QIwamsPNAMa99sZ7jp3V8kIiISG2j0FVFwgPceemRFHw8nHj50zy27D1p7ZJERESkCil0VSFXZweG9WtEp5Rw/j51I7Nz9mmdl4iISC2hNV1VzGgwkJURTVSwJx/M2sLeorMM7lkfFye9FSIiIjWZRrqspHGMHyMHpXKs+AJjPsvj0PFz1i5JRERELEihy4oCvV14YUAy0SEejJmQR972o9YuSURERCxEocvKnBzsGNKrAfe0jeG9mVuYunQ3ZVeuWLssERERqWRaSGQDDAYDHZPDqRPozj+nb2bf4bM8nt0QT1cdHyQiIlJTaKTLhsTX8WbUo6lcKrvCy5+uZW/RGWuXJCIiIpVEocvGeLs78Wz/pjSNC+C1L9bz/cZD1i5JREREKoGmF22QvZ2RhzrHUzfEk8/mbWf/0RL6d4rDYDBYuzQRERExk0KXDUtPDCbE35W/fPkDLk729GlT19oliYiIiJk0vWjjooI9+X2/RszNLWTxugPWLkdERETMpNBVDSRE+PBEdkMmL97Fmm1HrF2OiIiImEGhq5poFh/AgK4JfPjtVrbu02HZIiIi1Y1CVzXSpkkomRnRjJv2IwWHz1q7HBEREbkDCl3VTK/0SFolhvD2v3/gyKnz1i5HRERETKTQVc0YDAb6d44jIcKHtyb/QHHJRWuXJCIiIiZQ6KqGjAYDQ3o1IMDbhbf/vZHzP122dkkiIiJyGwpd1ZSDvZFhfRthNBoY9/UmLl0us3ZJIiIicgsKXdWYi5M9f7i3CadKLvL+zK1cuVJu7ZJERETkJhS6qjlPN0f+dH8Suw+e5vMFOygvV/ASERGxRQpdNUCAtwt/uK8Ja7YdYcaKvdYuR0RERG5AoauGiAjy4Pf9GjNndSFL1uu4IBEREVuj0FWDJET48HhWQ75ctIu1249auxwRERH5BYWuGiY5IYCHu8TzwawtbNNxQSIiIjZDoasGapsURmbLKN7RcUEiIiI2Q6GrhurVMoqMxGAdFyQiImIjFLpqKIPBwIOd4kmI8OFvU37gtI4LEhERsSqFrhrMaLx6XJC/lwt/03FBIiIiVqXQVcNVHBdkMDB+mo4LEhERsRaFrlrAxcmeP9zXhJNndVyQiIiItSh01RK/PC7oCx0XJCIiUuXsLXXh48ePM2zYMOzt7SkrK2P06NG4urry3HPPYTQacXV15e2338bV1dVSJch/uXZc0BuT1uPp5kjv1nWtXZKIiEitYSi30JBHWVkZBoMBo9FIbm4uU6dOJSwsjOjoaHr37s0//vEPgoKCuOeee0y63rFjlt1vytvbleLi2rG1wo7CU7w1ZSP9O8bSvln4XV2rNvWtMqlv5lHfzKO+mUd9M4/6BgEBHjd83GIjXXZ2dhVfnz17lnr16hEUFERxcXHFY/Xr1zf5et7elh0Rs7MzWvw1bEULb1f+aDTy1qT1BAW407JRqNnXqk19q0zqm3nUN/Oob+ZR38yjvt2cxUa6AHbv3s2IESMoKipi3LhxBAQEMHjwYOzs7HB1deWLL77AwcHBpGtppKvyfffDQSYt3Mkf7m1C/Shfs65RG/tWGdQ386hv5lHfzKO+mUd9u/lIl0VD1zXbt29n5MiRREREkJWVRdu2bZk4cSJnzpzhySefNOkaCl2WMXPlXublFvLcg82IDL7xh+RWamvf7pb6Zh71zTzqm3nUN/OobzcPXRa7e7G0tLTiaw8PD5ydnbly5Qo+Pj4A+Pj4cObMGUu9vJgos2UULX8+LuiojgsSERGxGIut6dqyZQtvvfUWBoMBgOHDh2Nvb8+f//xn7O2vvuwbb7xhqZcXE107LujM+Uu8NeUHXng4GS93J2uXJSIiUuNUyfRiZdD0omVdunyFv0/dyLkLl3juoWa4OJmWx2t738ylvplHfTOP+mYe9c086psVphelerl2XJDBYGDc1zouSEREpLIpdEkFFyd7/ve+Jpw8c5H3Z+m4IBERkcqk0CXX8XJz5I8PJLHrwGm+WLhTxwWJiIhUEoUu+ZVAbxf+eF8TcrceZubKfdYuR0REpEZQ6JIbigjy4Hd9GzM7p4ClGw5auxwREZFqT6FLbqpepA+PZzVg0sKd5G0/au1yREREqjWL7dMlNUNyQiAPdb7E+7O24ubiQP1IH2uXJCIiUi1ppEtuq13TMHqlRzLu600UHLbsfmkiIiI1lUKXmCQzI4r0xGDenrpRxwWJiIiYQaFLTGIwGHioUzzx4V78bcpGTp8rvf2TREREpIJCl5jMaDQwNLMhfl7OvP3vH7hw8bK1SxIREak2FLrkjlw7LgjQcUEiIiJ3QKFL7piLkz1/uC+Jk2cu8sona9hbdMbaJYmIiNg8hS4xi5ebI8/0b4qXuxOvTMjjb//+gZ37i61dloiIiM3SPl1iNj8vZ/7Yvxk9WkQwJ6eAv3y5gdgwL3plRNEg0geDwWDtEkVERGyGQpfctWBfV37Tsz5ZGVHMzS3k/6ZuJCLIg17pUTSJ9VP4EhERQaFLKpG/twsDuibQq2UU89cU8q+ZmwnycaVXyyiS4wMwGhW+RESk9lLokkrn4+HEAx3j6JEeycK1+/lkzjamf7+HnumRtGgQhJ1RSwlFRKT2UegSi/F0daRf2xi6tYhg8boDfLloF9O/30uP9EgyEkNwsFf4EhGR2kOhSyzOzdmBrIxouqTW4bsNh5j+/V5mrdxHtxYRtGkSipODnbVLFBERsTiFLqkyzo72dGsRQYdmYXy/qYg5qwv4dtU+ujaPoH3TMFyc9HEUEZGaS//KSZVzdLCjY3I4bZNCWbX5MHNyCpi7uoBOKXXomByOu4uDtUsUERGpdApdYjX2dkbaNAklo1Ewa7Yd5dtV+5i/ppD2zcLomhqBp5ujtUsUERGpNApdYnV2RiPpDYNp0SCI9TuO8W3OPhbnHaBNUijdmkfg6+ls7RJFRETumkKX2AyjwUBKvUCSEwL4cc8JZq3cx/D3cshoFEL3tEgCvV2sXaKIiIjZFLrE5hgMBhrH+NOorh/bC04xa9U+XnhvNWkNg+iZHkmIn5u1SxQREbljCl1iswwGA/WjfKkf5cvuA6f5NmcfIz7MJSUhkJ7pkUQEeVi7RBEREZMpdEm1EBvuxf/e24SCw2f5dtU+Rn+yliax/vRsGUlMqJe1yxMREbkthS6pViKDPfht30YcPFbC7NUFvPr5OhpE+tCrZRQJET7WLk9EROSmFLqkWgoLcOexzIZkt4pmTk4Bf538A3VDPclsGUXDaF8MBh2uLSIitkWhS6q1IB9XHu1Rn6yMaOblFvLO1z8SHuBGZssomsT5Y1T4EhERG6HQJTWCn5czD3WJp2fLSBas2c/7s7YS6u/G49kNtdWEiIjYBKO1CxCpTN7uTtzXIZY3n0zHx8OJ0Z+sIW/7UWuXJSIiotAlNZOHqyO/7ZNI79Z1eX/WFiYu2Mmly1esXZaIiNRiml6UGstgMNA5pQ6xYV78c/pmdh86zZPZDQn0cbV2aSIiUgtppEtqvOgQT/78aCr+ns6M/nStphtFRMQqFLqkVnB1duCpPon0+Xm68YsFO7h0uczaZYmISC2i6UWpNQwGA51S6hBzbbrx4Gme7J1IkKYbRUSkCmikS2qda9ONAd4ujP5kLWs13SgiIlVAoUtqJVdnB57qnUi/tjF8MGsLn2u6UURELEyhS2otg8FAx+RwXhiQzOY9Jxj7+TqOnDpv7bJERKSGUuiSWi8q2JNRg5oT+PN045ptR6xdkoiI1EAKXSKAq7M9T/483fjht1v5fL6mG0VEpHIpdIn87Np044sDUtiy9yRjJ6zjyElNN4qISOVQ6BL5L5HBHrw0KJVAX1f+/KmmG0VEpHIodIncgKuzPU9mN+S+djF8+O02Jmi6UURE7pJCl8hNGAwG2jcL58UByWzde5JXJqzjsKYbRUTETApdIrcRGezBqEdTCfZ1ZfSna1m99bC1SxIRkWpIoUvEBC5O9jyR3ZD72sfy8eztfDZvO6WXNN0oIiKmMyl0lZaWWroOEZtnMBho3zSMFwcks63gFK9MWEfRiXPWLktERKoJk0JXZmYmL730EuvWrbN0PSI2LzLYg1GDUgn1d+Xlz/I03SgiIiYxKXTNmTOH9u3bM2HCBHr27Mn48eMpLCy0dG0iNsvFyZ7Hsxpyv6YbRUTERIby8vLyO3nCsmXLePHFFykrKyMmJob/+Z//ITU11VL1VTh27KxFr+/t7Upxse5Mu1PqGxQeOcs/p2/Gwd6OJ3s3JMTP7bbPUd/Mo76ZR30zj/pmHvUNAgI8bvi4vSlP3r9/PzNmzGDBggXExcXx+uuvk5GRwZ49exg6dChLliyp1GJFqpOIoKubqX42bzsvf5rHwG4JpDcMtnZZIiJiY0wKXc888wx9+vRh4sSJeHj8J73FxMTw5JNPWqw4keri2nTjso2H+GTOdnYUnuLBTvE4OthZuzQREbERJk0vHj58GB8fH5ycnAC4ePEixcXFBAUFWbzAazS9aJvUt18rPHKWf87YgoOdgSd7J95wulF9M4/6Zh71zTzqm3nUt5tPL5q0kP63v/0tBoOh4nuDwcBvf/vbyqlMpIaJCPLgpUdSCAtw5+VP88jZorsbRUTExNB1+fJlHB0dK753dHTU3l0it+DiZM9jmQ14oGMsn87dzidztnFRdzeKiNRqJoWu4OBgpk+fXvH9N998Q3CwFgqL3IrBYKBtUhgjBqaw88BpXpmQp81URURqMZNC15gxY1iyZAlt2rShbdu2LF++nLFjx97yOcePH+eBBx7g4Ycfpn///uzcuROAd999l0cffZQBAwawY8eOu/8TiNi4OoHuvPRICnV+nm5ctbnI2iWJiIgV3PE+XaYqKyvDYDBgNBrJzc1l6tSp9OzZkx07dvDEE0/c8fW0kN42qW+mKy8v5/tNRUxcuJOMxqH0bR2Nu4uDtcuqVvR5M4/6Zh71zTzq280X0psUui5cuMBXX33F7t27r1vL9dprr5n04osWLWLfvn3k5+fj5eXF1q1biYqKYsSIEdetFbuVSxZeD2NnZ6Ss7IpFX6MmUt/uXMHhM/zjq00cPXWegd3r0z45/LobVeTm9Hkzj/pmHvXNPOobONxkuyCT9ul6+umnadq0KTk5OfzpT39ixowZ1KlT57bP2717NyNGjKCoqIhx48aRk5NDSEgIEyZM4O233+arr77iwQcfNOkPYOnUrGRuHvXtznk52/PaUxlMX7qLj2ZtZkFuAQO7JhDqf/ud7Gs7fd7Mo76ZR30zj/p2l1tGHDp0iCFDhuDk5ETXrl0ZP34869evv+3zYmNjmTx5Mu+99x5jxozB09OTjIwMAFq3bl2xzkuktrEzGuiYHM4rQ9Lwdndk1Mdr+HpZvu5wFBGpwUwKXQ4OV9ed+Pv7k5eXR2FhIadOnbrlc345Denh4YGzszOpqals2bIFgM2bNxMREWFu3SI1go+HE09kJ/I/9zRm7bajjPwwl035J6xdloiIWIBJ04uPPfYYZ8+e5bnnnmPs2LGcO3eO55577pbP2bJlC2+99VbFWpXhw4cTExPDiy++yIABA3B3d+evf/3r3f8JRGqAxLp+vDy4ObNzChg/bRNJsf707xSPj4eTtUsTEZFKYlvZxAoAACAASURBVNJC+tLSUpMXvFuK7l60TeqbeW7Vt6IT5/h8/g72HT5LnzZ16dAsDDujSYPSNZ4+b+ZR38yjvplHfbv5mi6TRrp69epFw4YN6dq1K+3atcPZ2blSixOR/wjxc+OZ/k3J2XKYKUt2s+rHwwzslkB0iKe1SxMRkbtg0v8+z58/nwcffJC8vDyysrL4/e9/z5w5cyxdm0itZTAYaJkYwtihaUSFeDB2wjq+WLCD8z9dtnZpIiJipjveHPXIkSO8+eabzJkzh23btlmqrl/R9KJtUt/Mc6d9233gNBPmb+fs+Us80DGO5vUDa+XeXvq8mUd9M4/6Zh717S6nFw8fPsyCBQtYsGAB58+fp2vXrvzud7+r1AJF5OZiw714aVAqi/IO8MncbazYdIiHuyYQ5ONq7dJERMREJoWup556im7duvHqq69qmwcRK7G3M9KtRQSp9QKZtGgnIz9cQ6/0SLqnReJgr4X2IiK2zqTQNW3aNEvXISIm8vNy5nf9GrNh1zEmLtxJztYjDOwST/0oX2uXJiIit3DL0DV8+HBef/117r///uvWj5SXl2MwGJg8ebLFCxSRG2saF0D9SB9mrtzH3/69keb1A7mvQxxebtbd3kVERG7slgvpjx49SmBgIAcPHrzhz8PCwixW2H/TQnrbpL6Zp7L7duBoCRPm7+DQ8XPc0y6GNkmhGGvgQnt93syjvplHfTOP+mbmQvrAwECuXLnCkCFDmDt3rkUKE5G7Fx7ozvCHm7FiUxFTl+5m5Y9FDOiaQETQjf/DFxGRqnfb1bdGo5GEhATy8/Oroh4RMZPRYKBNk1DGPpZGsK8rYz7LY8qSXfxUqr29RERsgUkL6fft20d2djYxMTG4uLhoTZeIDfN0dWRwrwZkNArh8wU7WLPtKA91jqdpnH+t3NtLRMRWmBS6/vGPf1i6DhGpZPUifRj9m+bMzS3kvZlbaBjly4Od4/D3crF2aSIitZJJm/uEhYVRXFzM4sWLWbJkCcXFxVW6iF5EzGNvZySzZRRjBjfn8pUrjPgwl7mrC7hcdsXapYmI1Domha7x48fz8ssvU1paSmlpKWPGjGH8+PGWrk1EKkmgjyt/uLcJg3s2YEHefkZ/upZdB4qtXZaISK1i0vTi7NmzmTlzJg4ODgAMHDiQrKwshg0bZtHiRKTyGAwGUusFkhjtyzfL9/DGxA20ahzMPe1icXdxsHZ5IiI1nkkjXSEhIRQX/+f/iouLiwkNDbVYUSJiOS5O9jzYOZ4RjyRTeKSEF95fzcofi7jFln0iIlIJTBrpAujWrRspKSkArFu3jiZNmvCnP/0JgLfeessy1YmIxUQFezJiYApLNxxk4sKdrNh0dW+vUH83a5cmIlIjmRS6nnjiieu+Hzx4sEWKEZGqZTQa6JgcTrP4AKYs2cWoj9fQrUUEvdKjcHK0s3Z5IiI1yi1D129+8xtatmxJq1atqFevXlXVJCJVzMfDiSeyE9m85wQTF+5k1ebD3Nc+lub1A7W3l4hIJbnl2YunTp0iJyeHlStXsmPHDqKjo8nIyCAjI4OAgICqrFNnL9oo9c08tty3y2VXWJi3n5kr9xEZ6M6DneNt5jghW+6bLVPfzKO+mUd9u/nZi7cMXf9tz549rFq1ipUrV3Lq1CmaNm3Kc889V2lF3opCl21S38xTHfpWXHKRqUvzyd16hLZJofRpU9fqdzlWh77ZIvXNPOqbedS3uwxdR48eJTAw8LrHLl++zA8//FCxuN7SFLpsk/pmnurUt90HTjNx0U6OF1+gT5u6tE0Kxc5o0o3Pla469c2WqG/mUd/Mo77dPHSZtJD+ySefxNvbm+zsbLp06YKzszP29vZVFrhExHpiw70YOTCFFT8W8dV3+Xy34RAPdY4jIcLH2qWJiFQrJk8v7tq1i2+++YYlS5aQlJREdnY26enplq6vgka6bJP6Zp7q2rfzP11i+oq9LF1/kOSEAO5rH4uvp3OVvX517Zu1qW/mUd/Mo75V0pqu0tJS5s2bx1/+8hf8/f25dOkSTz31FD169Ki0Qm9Gocs2qW/mqe59O3ishEmLdpF/8DQ90yPp1iICB3vLbzFR3ftmLeqbedQ386hvdzm9mJuby/Tp09m0aROdOnViwoQJREdHc+rUKfr161cloUtEbEdYgDtPP5DE+p3HmLx4N99vKuKBjnE0jfPXFhMiIjdhUuiaMmUKffv2ZezYsRh/sYDWx8eHUaNGWaw4EbFdBoOB5IRAGtX1Y15uIe/N3EJ8uBf9O8VrV3sRkRsw6Rak3/zmNzRt2rQicJ07d46tW7cC0LZtW8tVJyI2z9HBjqxW0Ywd2gIXZwdGfbyGyYt3cf6ny9YuTUTEppgUukaOHImLi0vF987OzowYMcJiRYlI9ePv5cJTvRP54/1JbNl3khfez+H7jYe4ooO0RUQAE0NXWVnZddOKdnZ2XLp0yWJFiUj1VT/Shz8/mkpmRjRTluxm7IQ88g+etnZZIiJWZ1LoiouLY/z48Zw4cYITJ04wfvx44uPjLV2biFRTdkYjHZPDefXxNCKDPHjti/V89O1WTpdctHZpIiJWY9KWESUlJfzjH/8gNzcXgPT0dJ566inc3Kpusay2jLBN6pt5alvfCg6fZeKinRw4WkJWRjSdUsKxt7vzXe1rW98qi/pmHvXNPOpbJe3TZU0KXbZJfTNPbexbeXk5uVuP8O+lu3F2tKd/pzga1fW7o2vUxr5VBvXNPOqbedS3u9yn6/jx43zwwQfk5+dTWlpa8fiECRMqpzoRqfEMBgNpDYNJivPn21UFjPt6E4nRfjzQMZZAH1drlyciYnEmje8//fTTJCUlUVRUxLPPPktsbCxJSUmWrk1EaiBnR3vuaRfDmMEtKC8vZ8SHuXy9LJ+fSrXFhIjUbCaFrjNnztC9e3eMRiOJiYm89NJLrFixwtK1iUgNFuTryv/c24RhfRuRt/0oL7y/mtVbDlNNVjyIiNwxk6YXHR0dKS8vJzw8nOnTpxMQEMC5c+csXZuI1AKNY/xpEOXLwrz9fDZ/B0s3HOShzvFEBN14TYSISHVl0kL6H374gbi4OE6fPs0777zDuXPnePTRR2nWrFlV1AhoIb2tUt/Mo77dWHHJRb76Lp/VW47QJimUPq2j8XB1rPi5+mYe9c086pt51Le7WEh/5coV5s+fT1JSEm5ubrz++uuVXpyICIC3uxNDejWgXdMwJi7cyQvvr6Z367q0axqKnfHOt5gQEbElt/1bzGg0smHDBq5cuVIV9YiIEBvmxchHUri3fSwzV+5l9Cdr2V5wytpliYjcFZPWdEVHR/PII4/QsWPH685gvP/++y1WmIjUbkaDgTZNQklJCGDGin28NeUHWmw+TO+MKPy9XW5/ARERG2NS6AoLCyMsLIySkhJKSkosXZOISAVXZwf6d4qjTZMQvlq+hxc+yKVbizr0SIvE2dGkv8JERGyCdqT/mRb+mUd9M4/6Zh4vLxeWrdvPlMW7+OlSGfe0jSE9MRijwWDt0myaPm/mUd/Mo77d5Y70999/P4Yb/KU2efLku6tKROQOGAwGkmL9SYz2ZfG6A0xatIvF6w7Qv1McceHe1i5PROSWTApdf/vb3yq+Li0tZfHixZw+fdpiRYmI3Iq9nZGuzSNITwxm+vd7eWPiBlLqBXBvu1j8vJytXZ6IyA2ZdA/2tTVdYWFhREdHM2TIEFauXGnp2kREbsnT1ZGBXRMY9WgqZ89f4sUPVjP9+z1cLC2zdmkiIr9i0kjXL4/8uXLlCtu2bbvhdKOIiDXUCXTn6QeS+GHXcaYs2c33m4q4p20MLRoGab2XiNgMk0LX7NmzK742Go2Ehoby7rvvWqwoEZE7ZTAYaBofQGJdPxavO8AXC3eweP0B+neMIybMy9rliYjo7sVrdLeFedQ386hv5rmTvp05V8o33+/h+41FNG8QyD1tY/D1rJ3rvfR5M4/6Zh717eZ3L5q0puuxxx7jzJkzFd+fPn2aJ554onIqExGxAE83Rx7pVo+XBqVQfPYiL7y/mhkr9nLxktZ7iYh1mDS9ePToUTw9PSu+9/Ly4vDhwxYrSkSkskQEefBM/6as33ns5/Veh7inXQwt6gdpbaqIVCmTRrocHBzYu3dvxfd79uzB3l47QYtI9WAwGEhOCGTs0BZ0aBbOZ/N28OoX69hbdOb2TxYRqSQmJafnn3+exx9/nMjISMrLy9m/fz9vvPGGpWsTEalUDvZ29EiLJCMxmGnL9zB2wjrSGgbRr20MPh5O1i5PRGo4kxfSl5aWVox21a1bFwcHB4sW9t+0kN42qW/mUd/MU9l9Kzh8li8X7WTfkbP0TIuka/MIHB3sKu36tkKfN/Oob+ZR3+5yIf348eP56aefSEhIICEhgfPnzzN+/PhKLVBEpKpFBnvw3EPNGNKzAd9vKuLFD1azZtsRqslN3SJSzZgUuhYtWvSrhfSLFi2yWFEiIlXFYDCQUu/qeq92TcP4ZM52Xp+4nn2Htd5LRCqXSaGrrKyM8+f/M1RYUlJCWZluuxaRmsPB3o6e6VG89ngagT4ujPksj49mb6W45KK1SxORGsKkhfQDBgzgoYceokePHpSXlzNnzhweeeQRS9cmIlLlvN2dGNyzAR2ahfPl4l08//5qeqVH0iW1Dg72NW+9l4hUHZMX0m/fvp3Fixfj5eVFWloarq6uhIaGWrq+ClpIb5vUN/Oob+ap6r6Vl5ezdvtRpi7djcFg4L72sSQnBFS7/b30eTOP+mYe9e3mC+lNGulatGgRf//73zl48CDh4eG89tprJCQkMG3atJs+5/jx4wwbNgx7e3vKysoYPXo08fHxAIwYMYJjx47x3nvvmfFHERGpGgaDgeb1g0iK9Wf+mkI+nL2VRes86d8xjsjgG/+lKiJyMyat6XrnnXeYPHkyERERzJo1iy+//JLo6OhbPsfHx4dJkybxxRdf8L//+7+8//77wNWNVU+cOHH3lYuIVBFHBzsyM6J57bF0/L2cGfNZHp/M2cbpc6XWLk1EqhGTRrocHBxwd3cH4PLlyzRu3JidO3fe8jl2dv9Z+3D27Fnq1asHwL/+9S+GDh16x6Nc3t6ud/T7d8rOzmjx16iJ1DfzqG/msXbfvL1defrhFHbtP8XHs7bywvur6dc+lsxW0Ta93svafauu1DfzqG83Z1LoCggI4MyZM7Rv354nnngCLy8vAgMDb/u83bt3M2LECIqKihg3bhw//vgjfn5+Jj33v1l6flhz0OZR38yjvpnHVvoW4OHEs/2TyN12hKlL85m/eh/3toulWUIARhtc72Urfatu1DfzqG83X9Nl8kL6a3JzcykpKaF169Y4Ojqa9Jzt27czcuRIAgMDGTNmDOfPn2fMmDF3NNqlhfS2SX0zj/pmHlvs28VLZczLLWTemkL8PZ3pkR5J8/qB2BlNWr1RJWyxb9WB+mYe9e0uF9L/UosWLUz6vdLS0opQ5uHhgbOzM4WFhTzzzDNcvHiR/Px8Pv30UwYNGnSnJYiI2AwnBzuyW0XTMTmcxesOMGnhTr5ZvufqGY+NQnCwt53wJSLWdcehy1Rbtmzhrbfeqri1evjw4TRs2BCAAwcOMGbMGAUuEakx3F0cyG4VTZfUOiz74RAzVuxl5sq9dG0eQdukUJwdLfbXrYhUE3c8vWgtml60TeqbedQ381Snvl26XMaKTUXMzS3kp9IyOqWE0zE5HDdnhyqvpTr1zZaob+ZR3ypxelFERG7Pwd6O9s3Cad0klNytR5izuoB5uYW0bxZGl9QIvNxMWxMrIjWHQpeIiAXZ2xnJaBRCemIwG3Ye49tVBSzKO0DrxiF0axGBv5eLtUsUkSqi0CUiUgWMBgPJCYE0iw9gy96TfLtqH8+/t5q0hkH0SIskxM/N2iWKiIUpdImIVCGDwUBiXT8S6/qxc38x3+bsY8SHuSQnBNIrPZKIIB0vJFJTKXSJiFhJfB1v/lgniYLDZ5mds4/Rn66lUV0/eqZHEhfube3yRKSSKXSJiFhZZLAHT/VpxKHj55i7uoA3Jm4gLtyLni0jaRjlW7H1johUbwpdIiI2ItTfjcG9GpDdKpq5awp556sfCQ9wo2d6FE3j/W3yiCERMZ1Cl4iIjfH3dmFAlwSyWkYxf+1+Ppy9Fb/vnemZFknzBrZ1xJCImE6hS0TERnm5O3Ff+1h6pEVePWJo0U6++X4P3dMiadUoGAd7O2uXKCJ3QKFLRMTG/fcRQzOvHTGUGkG7pjpiSKS60H+pIiLVhIuTPd1aRNAxOYwVPx5m7uoCZufso3NKHTqmWOeIIRExnUKXiEg142BvR/umYbRuHMKabUeYnVPA3DWFdGgaRpfUOni5O1m7RBG5AYUuEZFqyt7OSMvEENIa/nzEUE4Bi9bpiCERW6XQJSJSzf3qiKGcgqtHDDUIokd6JN7ertYuUURQ6BIRqTH++4ih2TkFjPgwl/TEEHqmReh8RxErU+gSEamB4ut4E1/H++oRQ7mFjPxwDa0ah5DdKhofD635ErEG7bAnIlKDRQZ78MIjqTz7YFMOnTjH8PdymPrdbs79dMnapYnUOhrpEhGpBeLrePP8Q83YmH+Cr5fls2zDIXqmR9IxORxHB22yKlIVFLpERGoJg8FAUqw/jev6kbPlMNO/38vCvP1kt4qmVeMQHS8kYmEKXSIitYzRaCCjUQjN6wexdMNBvl62h/lr9tO3TV2SEwIw6GBtEYtQ6BIRqaUc7I10Sa1D68YhzF9TyEeztzE3t4B72sZQP8rX2uWJ1DgKXSIitZyLkz29W9elfbNwvl25j7/9eyP1In24p20MkcEe1i5PpMbQBL6IiADg5ebIQ13iGftYGh4uDrz82Vr+NWMzR06dt3ZpIjWCQpeIiFwn0NuFx7IaMmpQKhculjHig1w+X7CD0yUXrV2aSLWm0CUiIjcUEeTBH+5rwtMPJFFw+CzPvZfDtOX5nP/psrVLE6mWtKZLRERuKSHChxcHJLN+53GmLc/nu5/3+OrQLAwHe+3xJWIqhS4REbktg8FAckIASXF+rPzxMDNW7GVR3n6yW9WlZWIwRqO2mRC5HYUuERExmZ3RSJsmoaQ1CGLJ+oNMWbKL+WsK6du2Lkmx/trjS+QWFLpEROSOOTrY0a1FBG2ahDA3t5D3ZmwhIsiDe9rFEF/H29rlidgkLaQXERGzuTo70K9tDK89nk54gBtvTtrA36du5MDREmuXJmJzFLpEROSu+Xg4MbBbPV4Z2gInBztGfbKGD2Zt5XjxBWuXJmIzFLpERKTSBPu68mTvREYMTOH0uYs8//5qJi3cyZnzpdYuTcTqFLpERKTSRYd48vQDTfnf+5qw6+BpnvtXDjNW7OXCRe3xJbWXFtKLiIjFNIzypf4jPqzbcYxpy/JZsv4AmS2jaJsUhoO9/r9faheFLhERsSijwUBqvUCaxvmzYlMRM1buZcHa/fRpXZcWDYMwapsJqSUUukREpErY2xlp1zSM9MRgFuXt54uFO5mTW0CX1DqkNQjWyJfUeApdIiJSpZwc7OiZfnWKcfG6A3z9XT5ff5dP+2bhtG8ahqebo7VLFLEIhS4REbEKdxcHsltF0yMtgtytR1mwdj+zc/aR1iCYLql1CA90t3aJIpVKoUtERKzKwd6OVo1DyGgUzPaCUyzMO8Coj9dQL9KHzql1aBzjp3VfUiModImIiE0wGAzUj/KlfpQvR06eZ2Hefv41YzM+Hs50TgknIzEEJ0c7a5cpYjaFLhERsTlBvq483CWBPm3qsnzjIeasLmDasj20TQqlY3I4vp7O1i5R5I4pdImIiM1yc3age4tIOqfUYf3OYyxYu5/5a/aTUi+Azql1iAn1snaJIiZT6BIREZtnb2ekef0gmtcPIv/gaRas3c9rn68nOtSDLqkRNIv3x86oLSfEtil0iYhItRIT5sWTYV6cOP0Ti9cf4NO52/n3Ejs6JtehTZMQXJ0drF2iyA0pdImISLXk5+XMfe1jycqIYuWPh1mYt58ZK/bSqlEInVLDCfJxtXaJItdR6BIRkWrN2dGejsnhtG8WxqbdJ1iYt58X3ltNk1h/OqfWoV6ENwZtOSE2QKFLRERqBKPBQFKcP0lx/hQeOcvCvP28/e8fCPVzo3NqHZrXD9JRQ2JVCl0iIlLjRAR5MLhnA+5pG8PSDQf599LdTP0unw5Nw2ino4bEShS6RESkxvJyd6J367r0TI9k9ZYjLMzbz7c5BaQ1DKJLio4akqql0CUiIjWeg70drZuE0qpxCNsKTrFg7X5GfbyG+lE+dE6pQyMdNSRVQKFLRERqDYPBQIMoXxpE+XL456OG/jljM74/HzXUUkcNiQUpdImISK0U7OvKgC4J9Gl99aihb3MKmLZ8D22SQunYLBxvb205IZVLoUtERGo1dxcHeqRF0iW1Dut2/HzUUO5+MhqH0LFZGBFBHtYuUWoIhS4RERGuHjXUokEQzesHkn/oDIvXH2T0J2tJrOtHj7QI4utovy+5OwpdIiIiv2AwGIgN8yKlYQjb8o8xN7eQv07+gagQD3qkRdIk1l+L7sUsCl0iIiI3EeLnxm961Kd3q2gWrN3P+zO34uflTPcWEbRoEIS9nTZbFdMpdImIiNyGr6czD3SMo1fLKJasO8CUJbuZ/v0eujaPoHWTUJwcdMej3J5Cl4iIiIncXRzIahVN1+YRLN90iHlrCpm5ch+dUsLpmByOm7ODtUsUG6bQJSIicoecHO3onFKH9k3DyN16hDmrC5ibW0i7pFC6pEbg4+Fk7RLFBlksdB0/fpxhw4Zhb29PWVkZo0ePZv78+Xz//fcAdO7cmaFDh1rq5UVERCzO3s5IRqMQ0hOD2bjrOLNXF7AobxUtE4Pp1iKCED83a5coNsRQXl5ebokLl5WVYTAYMBqN5ObmMnXqVIYNG0ZUVBTl5eX079+fv//97wQHB5t0vWPHzlqizAre3q4UF5+36GvUROqbedQ386hv5lHfzGNO38rLy9lRWMyc1QVs2XeSZvEB9EiLJDrE00JV2h593iAg4MZ7u1lspMvO7j+LCs+ePUu9evWIiooCrt6Oa29vj9Fo+l0flt4Z2M7OqN2HzaC+mUd9M4/6Zh71zTzm9i3Nx420JmHsOXiab5blM3ZCHokx/vRtF0ujGL8av9eXPm83Z9E1Xbt372bEiBEUFRUxbty4isfnzZtHnTp1CAwMNPlalk7NSubmUd/Mo76ZR30zj/pmnrvtm6+bA4N71KNXegTzcgt55ZNc6gS60yMtkqbxATV2ry993m4+0mWx6cVf2r59OyNHjmTq1KmsW7eO//u//+P999/H2dnZ5GtoetE2qW/mUd/Mo76ZR30zT2X3rbjkIgvX7mfphoN4uzvRvUUE6YnBNW6vL33erDC9WFpaiqOjIwAeHh44Ozuza9cu3njjDf71r3/dUeASERGp7rzdnbi3fSw90yNZsv4gXy3LZ/qKvXRJrUPbpFCcHbWhQE1nsZGuDRs28NZbb1XMXQ8fPpw333yTI0eOEBAQAMCIESNISEgw6Xoa6bJN6pt51DfzqG/mUd/MY+m+lV4qY8WPRczLLeTCxct0TL6615eHq6PFXrMq6PNm5enFyqDQZZvUN/Oob+ZR38yjvpmnqvpWduUKa7cdZc7qAo6eukCbJqF0bR6Bn1f1nBHS580K04siIiJye3ZGI2kNg2nRIIhN+SeYs7qA4e/lkNYgiG5pkYT5a6+vmkKhS0RExAYYDAaaxPrTJNafnfuv7vX10oe5JMX50yMtkpgwL2uXKHdJoUtERMTGxNfxJr6ON/uPljA3t4DXvlhPXLgXPdIjSYz2rfF7fdVUCl0iIiI2qk6gO49lNqRP67rMX1PI+Gk/EuLrSve0SJITAmrcdhM1nUKXiIiIjQvwduHhLglkZUSzaN1+Pp+/g8lLdtEuKYy2SaF4u+uA7epAoUtERKSa8HRzpG+bGHqmR5G79QhL1h3g21X7SE4IoEOzcOLCvTT1aMMUukRERKoZJwc72jQJpXXjEHYfPM2S9Qf5y5cbCPV3o0OzMNIaBOPkaHf7C0mVUugSERGppgwGA3Hh3sSFe/NAh1iWbTzEjBV7mbo0n1aNQ2jfLIwgHx0+bSsUukRERGoAL3cnsjKi6ZEWyYZdx1my7gAvvLeaxLp+dGgWRqMYvxp7yHZ1odAlIiJSg9jbGUmtF0hqvUAOHC1hyYaD/HPGZrzcHGnfNJxWjUNwd3Gwdpm1kkKXiIhIDRUe6M7Argnc07YuK388zJL1B/jm+z2kNQiiQ7NwIoNvfFyNWIZCl4iISA3n6uxA59Q6dEwJZ+u+kyxZd5CXP11LTJgXHZLDSEkI1J5fVUChS0REpJYwGgwkRvuRGO3HseILfLfhIBMX7GTy4t20bRJKu6Zh+Hhozy9LUegSERGphQK8Xbi3fSzZraJZs+0oi9cfYHZOAc3i/emYHE58HW/t+VXJFLpERERqMUcHO1o1DiGjUTB7is6wZN0B3pryA0E+rnRIDie9YRDOjooLlUFdFBEREQwGAzGhXsSEenF/hziWbzzEt6v28dV3u8lIvLrnV4ifm7XLrNYUukREROQ6nm6O9GoZRfe0CH7YdYIl6w8w4oNcGkT50CE5nCYx/hiNmnq8UwpdIiIickN2RiPJCQEkJwRw8Pg5lq4/wPuztuLu7ED7ZmG0bhyCh6ujtcusNhS6RERE5LbC/N14uEsC/drGsGrz1T2/pn+/lxb1A+mQHE50iKe1S7R5Cl0iIiJiMhcnezomh9OhWRjbCk6xZP1BXpmQR3SIJx2ahdE5LcraJdoshS4RERG5YwaDgQZRvjSI8uXE6Z/47oeDTF68mylLdpNaL5D0hsHUDfXUthO/oNAlIiIid8XPy5l+bWPIyohix8EzLF5TyOsT1+Pn6UxawyDSGwYT5Otq7TKtTqFLM3/wbQAACdtJREFUREREKoWDvR2tmoSRGOnDmfOlrN12lNVbDjNz5T7qhnqS3jCY1PqBeNbSxfcKXSIiIlLpPF0d6ZgcTsfkcI6cOs/qLUdYmLefLxftIrGuL+kNg0mK88fJwc7apVYZhS4RERGxqCAfV7JbRZOVEcWeojOs3nyEiQt38um87aTEB5CWGEz9CJ8av/eXQpeIiIhUiet2ve8Yy5a9J8nZcph3vtqEq7M9aQ2urv+qE+heIxfgK3SJiIhIlbO3M/L/7d1/TNd1Asfx1+fL7x+CKALZnViSnJjRvoChrkPcjBquDU39mlA3xeZcg7lVq+273eZsa90frWJef3T9cbtBMWQ0T0P64ax5gBACiaXSsc4gEAVCQ/ELvO+POq7uWtNqn8/3A8/Hf/DXk8/88fq+Px/9ZKYlKjMtUdfGJ9R2blCNXf1qONmihYkxyl2erNyMFM2Pj3Q69VfD6AIAAI6KigjVmhW3ac2K2zR8ZVwnPx1QY1e/Dh7/p9J/O1er7k5RdvoCRUeGOZ36izC6AABA0EiYE6GClYtUsHKRegevqunMgA6d6NHfGs4qMy1Rq5anaMWd8xUW6nE69ZYxugAAQFC6fUGsNuXFquj3d+r8hRE1dg3oL4c/lceScn6XpNzlKUr7Tbw8Lnn+i9EFAACCmseylL4oQemLErR9/V3q/PyyGrsG9KeqU0qYEzH9/NfCxBinU38SowsAALhGWGiIstKTlJWepKvXAmo9e1FNp/v19398odSUOVqVkayVGcmaGxvhdOr/YXQBAABXio0K09p7b9fae2/XpZFrajozoOMdfXrrWLcyFs/TquXJ8i5doMjw4Jg7wVEBAADwCyTOjdKG1YtVuCpV/xq4qsauflUf+1x/PXpW3rsWKHd5ipbfkaAQj3MP4DO6AADAjGFZllJT5ig1ZY425y/Rp18Mq/H0gP5cd1op86P1xz/kONbG6AIAADNSiMeju++Yr7vvmK/xG5MaunLd0R73/ScXAAAAtygiPES3zXf2XzcyugAAAGzA6AIAALABowsAAMAGjC4AAAAbMLoAAABswOgCAACwAaMLAADABowuAAAAGzC6AAAAbMDoAgAAsAGjCwAAwAaMLgAAABswugAAAGxgGWOM0xEAAAAzHSddAAAANmB0AQAA2IDRBQAAYANGFwAAgA0YXQAAADZgdAEAANiA0QUAAGADRhcAAIANGF2Sqqur5fP5VFJSogsXLjid4wqnTp3S1q1bVVxcrCeeeEKjo6NOJ7lKa2ur0tPTNTQ05HSKa3R2dmrHjh0qKSnR66+/7nSOa+zbt08+n09btmxRc3Oz0zlBLRAIyOfzKTs7W/X19ZKkoaEhlZaWatu2bXr11VcdLgxOP3bd/H6/tm7dqs2bN6uurs7hwiBiZrnh4WHzyCOPmEAgYDo6OkxZWZnTSa7Q399vxsbGjDHGVFZWmgMHDjhc5C5PPvmk2bhxo7l8+bLTKa4wPj5udu3aNf1rDjenp6fHPPbYY8YYY/r6+syjjz7qcFFwm5qaMgMDA+aVV14x77zzjjHGmBdeeMEcOXLEGGPMrl27zPnz551MDEo/dt16enqMMd/+3i0oKDCBQMDBwuAx60+6Ojs7tXLlSoWGhuqee+5RT0+P00mukJycrKioKElSWFiYQkJCHC5yj2PHjikrK0vR0dFOp7hGe3u7IiMjVVZWph07duizzz5zOskVEhMTFRkZqYmJCY2OjmrevHlOJwU1y7KUlJT0g++1tbUpPz9fkrR27Vq1tLQ4kRbUfuy6LV68WNJ//36wLMuBsuAT6nSA077++mvFx8dPf214FeUtGR4eVmVlJbd7btLU1JQqKytVUVGh999/3+kc17h48aK6u7tVU1Ojr776Sn6/X1VVVU5nBb2YmBgtXLhQDz74oK5fv66Kigqnk1xnbGxMkZGRkqS4uDh9+eWXDhe5yxtvvKGHHnqID+bfmfUnXXFxcT94HsnjmfWX5KZdu3ZN5eXl8vv9fIK+SYcOHdK6desUERHhdIqrxMXFyev1Kjo6WkuWLNHVq1edTnKFEydOaGRkRA0NDaqtrdW+ffucTnKdqKgojY+PS5KuXLnygw/p+Gn19fXq6OjQnj17nE4JGrN+YWRmZqqlpUWTk5Pq6upSamqq00muMDExob1796qkpERer9fpHNc4d+6cjh49qp07d+rs2bN66qmnnE5yhczMTPX09GhqakqDg4MKDw93OskVpqamFB8fL4/Ho9jYWI2NjTmd5DpZWVk6fvy4JOnDDz9Udna2w0Xu0NzcrMrKSr344oscZnyPZbifpqqqKr399tsKDQ3V888/z/C6CXV1ddq/f7+WLVsmScrLy1NpaanDVe5SUlKil19+mVPCm1RTU6Pa2lpNTEzo6aefVk5OjtNJQW9yclLPPvusent7NT4+rscff1wPP/yw01lBrby8XKdPn1Z0dLTuv/9+lZaW6plnntE333yj3NxclZeXO50YlP73ujU0NCgmJkZxcXGSxJ9132F0AQAA2IAzPwAAABswugAAAGzA6AIAALABowsAAMAGjC4AAAAbMLoA4CesWbPG6QQAMwSjCwAAwAaz/t2LAGaGmpoaVVVVKRAI6IEHHlBOTo5ee+01SVJfX58KCwtVVlYmSTpw4ICOHDkiy7K0d+9erVu3TpJUUVGh+vp6WZal7du3y+fzSZL279+vxsZGLVq0SBUVFbxHDsDPwugC4Hrd3d366KOPVF1dLcuytGfPHkVFRam9vV319fVKSEhQcXGx8vPzZYzRBx98oNraWo2MjMjn8+m+++7TyZMn1dbWptraWoWHh2tkZESSdOnSJRUUFMjv92v37t1qamriliOAn4XRBcD1Ghsb1d7ero0bN0qSxsbGtGHDBmVnZys5OVmStH79en388ceSpIKCAoWHhyspKUkZGRnq7u5WU1OTNm3aNP1ex7lz50r69mXb/3nl0LJly9Tb22v3jwdghmB0AXA9Y4y2bdum3bt3T3+vublZlmVNf21ZlizL0q2++ez7L9f2eDyanJz85cEAZiUepAfgerm5uTp8+LBGR0clSf39/RoZGVFra6sGBgYUCAT07rvvyuv1yuv16r333lMgENDg4KDOnDmjtLQ0rV69WgcPHtSNGzckafr2IgD8WjjpAuB6S5cu1c6dO1VcXCxjjGJiYuTz+ZSZmannnntOvb29Kiws1IoVKyRJeXl5KioqkmVZ8vv9iomJUV5enj755BMVFRUpJCRExcXF2rJli8M/GYCZxDK3etYOAC7Q3NysN998Uy+99JLTKQAgiduLAAAAtuCkCwAAwAacdAEAANiA0QUAAGADRhcAAIANGF0AAAA2YHQBAADY4N84Azj4w1bowQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Privacy budget epsilon = 3 ==> full 10% lower --> 7% lower\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "ax1.plot(p_test_df.index.values, p_test_df['accuracy'], 'g-',label='accuracy')\n",
        "ax1.plot(p_test_df.index.values, p_test_df['curr_privacy'], 'b', label='accuracy & privacy')\n",
        "ax1.set_xlabel('batch_number')\n",
        "# plt.ylabel(label='accuracy & privacy')\n",
        "ax2.set_ylabel('accuracy', color='g')\n",
        "ax2.set_ylabel('privacy', color='b')\n",
        "ax1.set_ylim([80, 100])\n",
        "ax2.set_ylim([0,3])\n",
        "plt.title('MNIST test accuracy/privacy by epoch')\n",
        "# plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "B3rskXpl881i",
        "outputId": "f01215a0-d601-401a-a91f-9e67d8137f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'MNIST test accuracy/privacy by epoch')"
            ]
          },
          "metadata": {},
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "p_test_df['curr_privacy']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joD6h4hX-Dl0",
        "outputId": "06076a0f-e6de-4194-daeb-9634415a13e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     2.200\n",
              "1     2.314\n",
              "2     2.403\n",
              "3     2.473\n",
              "4     2.542\n",
              "5     2.600\n",
              "6     2.657\n",
              "7     2.713\n",
              "8     2.763\n",
              "9     2.813\n",
              "10    2.862\n",
              "11    2.911\n",
              "12    2.955\n",
              "13    2.999\n",
              "Name: curr_privacy, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Privacy budget epsilon = 3 ==> full 10% lower --> 7% lower\n",
        "plt.plot(p_test_df['loss'], p_test_df['curr_privacy'])\n",
        "# plt.plot(p_train_df.index.values, p_train_df['curr_privacy'])\n",
        "plt.xlabel('loss')\n",
        "plt.ylabel('privacy')\n",
        "plt.title('MNIST test accuracy - non-DP vs DP')"
      ],
      "metadata": {
        "id": "xtus3aG3477N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Privacy budget of epsilon = 3, delta = 10^-5 --> performance loss of 7-10%, with a similar run-time (2 mins for 60000 samples - 50k train, 10k test, 14 epochs)"
      ],
      "metadata": {
        "id": "mXQoauKdOqKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "sns.set_context(\"paper\")\n",
        "plt.style.context('seaborn-poster')\n",
        "\n",
        "# Privacy budget epsilon = 3 ==> full 10% lower --> 7% lower\n",
        "plt.plot(test_stats.index.values, test_stats['accuracy'])\n",
        "plt.plot(p_test_stats.index.values, p_test_stats['accuracy'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('MNIST test accuracy - non-DP vs DP')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "-S5YsYeuL-DH",
        "outputId": "60e8dc93-aa04-4166-8371-9d880e9243aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'MNIST test accuracy - non-DP vs DP')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "base = '/content/gdrive/My Drive/Colab Notebooks/data/' \n",
        "outpth = os.path.join(base, 'first_file.csv')\n",
        "\n",
        "df = pd.DataFrame([('rohun', 'dad'), ('diana', 'mom'), ('sadie', 'baby')], columns=['name', 'title'])\n",
        "df.to_csv(outpth, index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtdVWM7hCDUb",
        "outputId": "6fa61fd6-f859-4678-ea03-a39fd0b3bebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-PPNrh316GNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test class\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import pandas as pd\n",
        "\n",
        "class RoNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n"
      ],
      "metadata": {
        "id": "UoOWoKbs6GRC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}